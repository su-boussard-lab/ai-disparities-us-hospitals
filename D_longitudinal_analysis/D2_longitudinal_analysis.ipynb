{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D2. longitudinal health outcome analysis \n",
    "\n",
    "**Description**  \n",
    "This section investigate whether AI implementation level affects the change in hospital care quality overtime. This was done because we could not specify the exact timing of the AI implementation using the AHA dataset. \n",
    "\n",
    "**Purpose**  \n",
    "To investigate whether AI implementation level affects the change in hospital care quality overtime\n",
    "\n",
    "**Disclaimer**  \n",
    "- This codebase was partially cleaned and annotated using OpenAI’s ChatGPT-4o. Please review and validate before using for critical purposes.  \n",
    "- AHA data is subscription-based and not publicly shareable. All reported results are aggregated at the state or census division level.\n",
    "- All publicly available data should also be independently downlowded from the source.  \n",
    "\n",
    "**Notebook Workflow**  \n",
    "\n",
    "0. Load necessary libraries, functions, and pre-processed data \n",
    "1. Feature engineering for hospital characteristics \n",
    "2. Assess missingness of the care quality metric \n",
    "3. conduct ML \n",
    "4. conduct feature importance analysis\n",
    "5. conduct longitudinal analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2_0 load necessary libraries, functions, and preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from libpysal.weights import KNN, DistanceBand\n",
    "from esda.moran import Moran\n",
    "from spreg import ML_Error, OLS\n",
    "from shapely.geometry import Point\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed dataframe \n",
    "AHA_master = pd.read_csv('./data/AHA_master_external_data.csv', low_memory=False)\n",
    "AHA_IT = AHA_master[~AHA_master['id_it'].isnull()]\n",
    "AHA_IT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and if needed, reload the module\n",
    "import calculate_ai_scores\n",
    "AHA_master2 = calculate_ai_scores.apply_ai_scores_to_dataframe(AHA_IT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2_1 Feature Engineering \n",
    "\n",
    "These hospital characteristics were selected based on investigator consensus, and we used LASSO regression analysis to explore and identify additional variables that predict AI/ML implementation and reflect hospital resource levels.\n",
    "\n",
    "- **rural_urban_type** : collected from AHA survey. categorized into {1: rural, 2: micro, 3: metro} based on the location of the hospital ('CBSATYPE')\n",
    "- **system member** : hospital belonging to a corporate body that owns or manage health provider facilities or health-related subsidiaries. ('MHSMEMB')\n",
    "- **delivery_system** : delivery system identified using existing theory and AHA Annual Survey data {1: Centralized Health System, 2: Centralized Physician/Insurance Health System, 3: Moderately Centralized Health System, 4: Decentralized Health System, 5: Independent Hospital System, 6/Missing: Insufficient data to determine} ('CLUSTER')\n",
    "- **community_hospital** : all nonfederal, short-term general, and special hospitals whose facilities and services are available to the public {0: No, 1: Yes}('CHC')\n",
    "- **subsidary_hospital** : Hospital itself operates subsidiary corporation {0: No, 1: Yes} ('SUBS')\n",
    "- **frontline_hospital** : Frontline facility {0: No, 1: Yes} ('FRTLN')\n",
    "- **joint_commission_accreditaion** : Accreditation by joint commision {0: No, 1: Yes} ('MAPP1')\n",
    "- **center_quality** : Center for Improvement in Healthcare Quality Accreditation {0: No, 1: Yes} ('MAPP22')\n",
    "- **teaching_hospital** : major teaching hospital ('MAPP8'), minor teaching hospital ('MAPP3' or 'MAPP5')\n",
    "- **critical_access** critical access hospital {0: No, 1: Yes} ('MAPP18')\n",
    "- **rural_referral** : rural referral center {0: No, 1: Yes} ('MAPP19')\n",
    "- **ownership_type** : type of organization responsible for establishing policy concerning overall operation {government_federal, government_nonfederal, nonprofit, forprofit, other} ('CNTRL')\n",
    "- **bedsize** : bed-size category, ordinal variable ('BSC')\n",
    "- **medicare_ipd_percentage** : medicare inpatient days / total inpatient days. Proxy variable to reflect the proportion of medicare patient \n",
    "- **medicaid_ipd_percentage** : medicaid inpatient days / total inpatient days. Proxy variable to reflect the proportion of medicaid patients \n",
    "- **core_index** : summary measure to track the interoperability of US hospitals (https://doi.org/10.1093/jamia/ocae289)\n",
    "- **friction_index** : summary measures to track the barrier or difficulty in interoperability between hospitals (https://doi.org/10.1093/jamia/ocae289)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add numpy import at the top if not already imported\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## rural_urban_type\n",
    "# Continue with CBSA type and other variables\n",
    "AHA_master2['rural_urban_type'] = AHA_master2['cbsatype_as'].map({\n",
    "    'Rural': 1,      # Rural = 1 (lowest)\n",
    "    'Micro': 2,      # Micropolitan = 2 (middle)\n",
    "    'Metro': 3       # Metropolitan = 3 (highest)\n",
    "})\n",
    "\n",
    "## system_member\n",
    "# Create new column 'system_member' based on the conditions\n",
    "AHA_master2['system_member'] = AHA_master2['mhsmemb_as'].copy()\n",
    "# Set to 1 where sysid_as is not null and mhsmemb_as is null\n",
    "AHA_master2.loc[(AHA_master2['sysid_as'].notna()) & (AHA_master2['mhsmemb_as'].isna()), 'system_member'] = 1\n",
    "# Convert all remaining null values to 0\n",
    "AHA_master2['system_member'] = AHA_master2['system_member'].fillna(0).astype(int)\n",
    "\n",
    "## AHA System Cluster Code - delivery_system\n",
    "# Handle NaN values before converting to int\n",
    "AHA_master2['delivery_system'] = AHA_master2['cluster_as'].fillna(0).astype(int)\n",
    "\n",
    "## community_hospital\n",
    "# Handle NaN values before converting to int\n",
    "AHA_master2['community_hospital'] = AHA_master2['chc_as'].fillna(0).replace(2, 0).astype(int)\n",
    "\n",
    "## subsidary_hospital\n",
    "# Handle NaN values before converting to int\n",
    "AHA_master2['subsidary_hospital'] = AHA_master2['subs_as'].fillna(0).astype(int)\n",
    "\n",
    "## frontline_hospital\n",
    "# Handle both '.' and NaN values before converting to int\n",
    "AHA_master2['frontline_hospital'] = AHA_master2['frtln_as'].replace('.', 0).fillna(0).astype(int)\n",
    "\n",
    "## joint_commission_accreditation\n",
    "# Handle NaN values before converting to int\n",
    "AHA_master2['joint_commission_accreditation'] = AHA_master2['mapp1_as'].fillna(0).replace(2, 0).astype(int)\n",
    "\n",
    "## center_quality\n",
    "# Handle NaN values before converting to int\n",
    "AHA_master2['center_quality'] = AHA_master2['mapp22_as'].fillna(0).replace(2, 0).astype(int)\n",
    "\n",
    "# teaching hospitals - Handle NaN values in the underlying columns\n",
    "AHA_master2['teaching_hospital'] = ((AHA_master2['mapp5_as'].fillna(0) == 1) | \n",
    "                                   (AHA_master2['mapp3_as'].fillna(0) == 1) | \n",
    "                                   (AHA_master2['mapp8_as'].fillna(0) == 1)).astype(int)\n",
    "\n",
    "AHA_master2['major_teaching_hospital'] = (AHA_master2['mapp8_as'].fillna(0) == 1).astype(int)\n",
    "\n",
    "AHA_master2['minor_teaching_hospital'] = (((AHA_master2['mapp5_as'].fillna(0) == 1) | \n",
    "                                          (AHA_master2['mapp3_as'].fillna(0) == 1)) & \n",
    "                                         ~(AHA_master2['mapp8_as'].fillna(0) == 1)).astype(int)\n",
    "\n",
    "# critical access hospital\n",
    "AHA_master2['critical_access'] = (AHA_master2['mapp18_as'].fillna(0) == 1).astype(int)\n",
    "\n",
    "# rural referral center \n",
    "AHA_master2['rural_referral'] = (AHA_master2['mapp19_as'].fillna(0) == 1).astype(int)\n",
    "\n",
    "# medicare medicaid percentage - Handle division by zero and NaN values\n",
    "# Replace inf and NaN with 0 for percentages\n",
    "AHA_master2['medicare_ipd_percentage'] = (AHA_master2['mcripd_as'].fillna(0) / \n",
    "                                         AHA_master2['ipdtot_as'].replace(0, 1).fillna(1) * 100)\n",
    "AHA_master2['medicare_ipd_percentage'] = AHA_master2['medicare_ipd_percentage'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "AHA_master2['medicaid_ipd_percentage'] = (AHA_master2['mcdipd_as'].fillna(0) / \n",
    "                                         AHA_master2['ipdtot_as'].replace(0, 1).fillna(1) * 100)\n",
    "AHA_master2['medicaid_ipd_percentage'] = AHA_master2['medicaid_ipd_percentage'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "# bed size - Handle NaN values\n",
    "AHA_master2['bedsize'] = AHA_master2['bsc_as'].fillna(0).astype(int)\n",
    "\n",
    "# hospital ownership type - Handle NaN values before comparison\n",
    "AHA_master2['nonfederal_government'] = ((AHA_master2['cntrl_as'].fillna(0) == 12) | \n",
    "                                       (AHA_master2['cntrl_as'].fillna(0) == 13) |\n",
    "                                       (AHA_master2['cntrl_as'].fillna(0) == 14) | \n",
    "                                       (AHA_master2['cntrl_as'].fillna(0) == 15) | \n",
    "                                       (AHA_master2['cntrl_as'].fillna(0) == 16)).astype(int)\n",
    "\n",
    "AHA_master2['non_profit_nongovernment'] = ((AHA_master2['cntrl_as'].fillna(0) == 21) | \n",
    "                                          (AHA_master2['cntrl_as'].fillna(0) == 23)).astype(int)\n",
    "\n",
    "AHA_master2['for_profit'] = ((AHA_master2['cntrl_as'].fillna(0) == 31) | \n",
    "                            (AHA_master2['cntrl_as'].fillna(0) == 32) | \n",
    "                            (AHA_master2['cntrl_as'].fillna(0) == 33)).astype(int)\n",
    "\n",
    "AHA_master2['federal_government'] = ((AHA_master2['cntrl_as'].fillna(0) == 40) | \n",
    "                                    (AHA_master2['cntrl_as'].fillna(0) == 44) | \n",
    "                                    (AHA_master2['cntrl_as'].fillna(0) == 45) | \n",
    "                                    (AHA_master2['cntrl_as'].fillna(0) == 46) | \n",
    "                                    (AHA_master2['cntrl_as'].fillna(0) == 47) | \n",
    "                                    (AHA_master2['cntrl_as'].fillna(0) == 48)).astype(int)\n",
    "\n",
    "# Create a categorical column for hospital ownership types\n",
    "def create_ownership_category(row):\n",
    "    cntrl_val = row['cntrl_as'] if pd.notna(row['cntrl_as']) else 0\n",
    "    if cntrl_val in [12, 13, 14, 15, 16]:\n",
    "        return 'nonfederal_government'\n",
    "    elif cntrl_val in [21, 23]:\n",
    "        return 'non_profit_nongovernment'\n",
    "    elif cntrl_val in [31, 32, 33]:\n",
    "        return 'for_profit'\n",
    "    elif cntrl_val in [40, 44, 45, 46, 47, 48]:\n",
    "        return 'federal_government'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Create the categorical column\n",
    "AHA_master2['ownership_type'] = AHA_master2.apply(create_ownership_category, axis=1)\n",
    "\n",
    "# Optional: Print some diagnostics to check your data\n",
    "print(\"Data processing completed!\")\n",
    "print(f\"Data shape: {AHA_master2.shape}\")\n",
    "print(f\"Ownership type distribution:\")\n",
    "print(AHA_master2['ownership_type'].value_counts())\n",
    "print(f\"Rural/Urban distribution:\")\n",
    "print(AHA_master2['rural_urban_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_exposures = ['ai_base_score', 'ai_base_breadth_score', 'ai_base_dev_score', 'ai_base_eval_score']\n",
    "AHA_imputed = AHA_master2.copy()\n",
    "# For a simple approach using a fixed value (e.g., 0)\n",
    "AHA_imputed[ai_exposures] = AHA_imputed[ai_exposures].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_features = [\"teaching_hospital\",\n",
    "\"nonfederal_government\",\n",
    "\"non_profit_nongovernment\",\n",
    "\"for_profit\",\n",
    "\"federal_government\",\n",
    "\"critical_access\",\n",
    "\"rural_referral\",\n",
    "\"medicare_ipd_percentage\",\n",
    "\"medicaid_ipd_percentage\",\n",
    "\"bedsize\",\n",
    "\"delivery_system\",\n",
    "\"community_hospital\",\n",
    "\"subsidary_hospital\",\n",
    "\"frontline_hospital\",\n",
    "\"joint_commission_accreditation\",\n",
    "\"system_member\"]\n",
    "coordinates_features = [\"latitude_address\",\n",
    "\"longitude_address\"]\n",
    "geo_features = [\"rural_urban_type\",\n",
    "\"national_adi_median\",\n",
    "\"svi_themes_median\",\n",
    "\"svi_theme1_median\",\n",
    "\"svi_theme2_median\",\n",
    "\"svi_theme3_median\",\n",
    "\"svi_theme4_median\",\n",
    "\"Device_Percent\",\n",
    "\"Broadband_Percent\",\n",
    "\"Internet_Percent\",\n",
    "\"mean_primary_hpss\",\n",
    "\"mean_dental_hpss\",\n",
    "\"mean_mental_hpss\",\n",
    "\"mean_mua_score\",\n",
    "\"mean_mua_elders_score\",\n",
    "\"mean_mua_infant_score\"]\n",
    "interoperability_features = ['core_index', \"friction_index\"]\n",
    "all_covariates = hospital_features + geo_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_quality_outcomes = [\"COMP_HIP_KNEE\",\n",
    "\"MORT_30_AMI\",\n",
    "\"MORT_30_CABG\",\n",
    "\"MORT_30_COPD\",\n",
    "\"MORT_30_HF\",\n",
    "\"MORT_30_PN\",\n",
    "\"MORT_30_STK\",\n",
    "\"PSI_03\",\n",
    "\"PSI_04\",\n",
    "\"PSI_06\",\n",
    "\"PSI_08\",\n",
    "\"PSI_09\",\n",
    "\"PSI_10\",\n",
    "\"PSI_11\",\n",
    "\"PSI_12\",\n",
    "\"PSI_13\",\n",
    "\"PSI_14\",\n",
    "\"PSI_15\",\n",
    "\"PSI_90\",\n",
    "\"Total HAC Score\",\n",
    "\"READM-30-AMI-HRRP\",\n",
    "\"READM-30-CABG-HRRP\",\n",
    "\"READM-30-COPD-HRRP\",\n",
    "\"READM-30-HF-HRRP\",\n",
    "\"READM-30-HIP-KNEE-HRRP\",\n",
    "\"READM-30-PN-HRRP\",\n",
    "\"MSPB-1\",\n",
    "\"EDV\",\n",
    "\"ED_2_Strata_1\",\n",
    "\"ED_2_Strata_2\",\n",
    "\"HCP_COVID_19\",\n",
    "\"HH_01\",\n",
    "\"HH_02\",\n",
    "\"IMM_3\",\n",
    "\"OP_18b\",\n",
    "\"OP_18c\",\n",
    "\"OP_22\",\n",
    "\"OP_23\",\n",
    "\"OP_29\",\n",
    "\"OP_31\",\n",
    "\"OP_40\",\n",
    "\"SAFE_USE_OF_OPIOIDS\",\n",
    "\"SEP_1\",\n",
    "\"SEP_SH_3HR\",\n",
    "\"SEP_SH_6HR\",\n",
    "\"SEV_SEP_3HR\",\n",
    "\"SEV_SEP_6HR\",\n",
    "\"STK_02\",\n",
    "\"STK_03\",\n",
    "\"STK_05\",\n",
    "\"STK_06\",\n",
    "\"VTE_1\",\n",
    "\"VTE_2\",\n",
    "\"EDAC_30_AMI\",\n",
    "\"EDAC_30_HF\",\n",
    "\"EDAC_30_PN\",\n",
    "\"OP_32\",\n",
    "\"OP_35_ADM\",\n",
    "\"OP_35_ED\",\n",
    "\"OP_36\",\n",
    "\"READM_30_AMI\",\n",
    "\"READM_30_CABG\",\n",
    "\"READM_30_COPD\",\n",
    "\"READM_30_HF\",\n",
    "\"READM_30_HIP_KNEE\",\n",
    "\"READM_30_HOSP_WIDE\",\n",
    "\"READM_30_PN\"]\n",
    "# List of columns to process\n",
    "outcome_columns = hospital_quality_outcomes.copy()  # Make a copy to safely modify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Your setup\n",
    "aha_df = AHA_imputed.copy()\n",
    "aha_df['mcrnum_as'] = aha_df['mcrnum_as'].astype(str).str.replace('.0', '')\n",
    "full_hospital_list = aha_df['mcrnum_as'].unique()\n",
    "print(f\"Total number of hospitals in AHA: {len(full_hospital_list)}\")\n",
    "print(f\"Sample AHA hospital IDs: {full_hospital_list[:5]}\")\n",
    "\n",
    "data_order = ['01_2022', '04_2022', '07_2022', '10_2022', \n",
    "              '01_2023', '04_2023', '07_2023', '10_2023', \n",
    "              '01_2024', '04_2024', '07_2024', '10_2024', \n",
    "              '02_2025', '04_2025']\n",
    "\n",
    "outcome_files = {\n",
    "    'General Hospital Info': \"data/outcomes/merged_general_hospital_info.csv\",\n",
    "    'Death Complication': \"data/outcomes/merged_death_complication.csv\",\n",
    "    'HAC Reduction': \"data/outcomes/merged_HAC_reduction.csv\",\n",
    "    'Readmission': \"data/outcomes/merged_readmission.csv\",\n",
    "    'Medicare Spending': \"data/outcomes/merged_Medicare_Hospital_Spending_Per_Patient.csv\",\n",
    "    'Timely Care': \"data/outcomes/merged_Timely_and_Effective_Care.csv\",\n",
    "    'Unplanned Visits': \"data/outcomes/merged_Unplanned_Hospital_Visits.csv\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2_2 Assess missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('merged_df_april_2025.csv')\n",
    "print(f\"merged_df shape: {merged_df.shape}\")\n",
    "\n",
    "# Find outcome columns (columns with prefixes from merging)\n",
    "outcome_cols = [col for col in merged_df.columns \n",
    "               if any(prefix in col for prefix in ['Timely_Care_', 'Death_Complication_', 'HAC_Reduction_', \n",
    "                                                  'Readmission_', 'Medicare_Spending_', 'Unplanned_Visits_', \n",
    "                                                  'General_Hospital_Info_'])]\n",
    "\n",
    "print(f\"Found {len(outcome_cols)} outcome columns\")\n",
    "\n",
    "# Calculate missingness for each outcome column\n",
    "results = []\n",
    "for col in outcome_cols:\n",
    "    missing_pct = (merged_df[col].isnull().sum() / len(merged_df)) * 100\n",
    "    results.append({'column': col, 'missing_pct': missing_pct})\n",
    "\n",
    "# Sort by missingness\n",
    "results_df = pd.DataFrame(results).sort_values('missing_pct')\n",
    "\n",
    "# Show results\n",
    "print(f\"\\nColumns with < 50% missing (good for LASSO):\")\n",
    "good_cols = results_df[results_df['missing_pct'] < 50]\n",
    "for _, row in good_cols.iterrows():\n",
    "    print(f\"  {row['column']}: {row['missing_pct']:.1f}% missing\")\n",
    "\n",
    "print(f\"\\nFound {len(good_cols)} usable columns for LASSO analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LASSO analysis on your April 2025 merged_df\n",
    "\n",
    "# Find outcome columns (from your merged data)\n",
    "outcome_cols = [col for col in merged_df.columns \n",
    "               if any(prefix in col for prefix in ['Timely_Care_', 'Death_Complication_', 'HAC_Reduction_', \n",
    "                                                  'Readmission_', 'Medicare_Spending_', 'Unplanned_Visits_'])]\n",
    "\n",
    "print(f\"Found {len(outcome_cols)} outcome columns\")\n",
    "\n",
    "# Filter to columns with < 50% missing data\n",
    "good_outcomes = []\n",
    "for col in outcome_cols:\n",
    "    missing_pct = (merged_df[col].isnull().sum() / len(merged_df)) * 100\n",
    "    if missing_pct < 50:\n",
    "        good_outcomes.append(col)\n",
    "\n",
    "print(f\"Using {len(good_outcomes)} outcomes with < 50% missing data\")\n",
    "\n",
    "predictor_cols = all_covariates \n",
    "\n",
    "# Filter predictors that exist and have < 75% missing\n",
    "good_predictors = []\n",
    "for col in predictor_cols:\n",
    "    if col in merged_df.columns:\n",
    "        missing_pct = (merged_df[col].isnull().sum() / len(merged_df)) * 100\n",
    "        if missing_pct < 75:\n",
    "            good_predictors.append(col)\n",
    "\n",
    "# Run LASSO analysis\n",
    "if good_outcomes and good_predictors:\n",
    "    print(\"\\nRunning LASSO...\")\n",
    "    try:\n",
    "        table, feature_importance_dict = lasso_covariate_table(\n",
    "            merged_df, \n",
    "            good_outcomes, \n",
    "            predictor_columns=good_predictors\n",
    "        )\n",
    "        \n",
    "        print(\"✓ LASSO completed successfully!\")\n",
    "        print(f\"Results: {len(table)} outcomes analyzed\")\n",
    "        \n",
    "        # Show summary results\n",
    "        if len(table) > 0:\n",
    "            print(f\"\\nSummary:\")\n",
    "            print(f\"  Average features selected: {table['num_selected'].mean():.1f}\")\n",
    "            print(f\"  Range: {table['num_selected'].min()}-{table['num_selected'].max()} features\")\n",
    "            \n",
    "            # Show top results\n",
    "            print(f\"\\nTop 5 outcomes by number of features selected:\")\n",
    "            top_outcomes = table.nlargest(5, 'num_selected')\n",
    "            for _, row in top_outcomes.iterrows():\n",
    "                print(f\"  {row['outcome']}: {row['num_selected']} features\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LASSO failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No suitable outcomes or predictors found\")\n",
    "    print(f\"Outcomes: {len(good_outcomes)}, Predictors: {len(good_predictors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LASSO analysis on your April 2025 merged_df\n",
    "print(\"Running LASSO analysis on April 2025 data...\")\n",
    "print(f\"merged_df shape: {merged_df.shape}\")\n",
    "\n",
    "# Find outcome columns (from your merged data)\n",
    "outcome_cols = [col for col in merged_df.columns \n",
    "               if any(prefix in col for prefix in ['Timely_Care_', 'Death_Complication_', 'HAC_Reduction_', \n",
    "                                                  'Readmission_', 'Medicare_Spending_', 'Unplanned_Visits_'])]\n",
    "\n",
    "print(f\"Found {len(outcome_cols)} outcome columns\")\n",
    "\n",
    "# Filter to columns with < 50% missing data\n",
    "good_outcomes = []\n",
    "for col in outcome_cols:\n",
    "    missing_pct = (merged_df[col].isnull().sum() / len(merged_df)) * 100\n",
    "    if missing_pct < 50:\n",
    "        good_outcomes.append(col)\n",
    "\n",
    "print(f\"Using {len(good_outcomes)} outcomes with < 50% missing data\")\n",
    "\n",
    "# Use your predefined covariates (from your notebook)\n",
    "predictor_cols = all_covariates  # Your hospital + geographic features\n",
    "\n",
    "# Filter predictors that exist and have < 75% missing\n",
    "good_predictors = []\n",
    "for col in predictor_cols:\n",
    "    if col in merged_df.columns:\n",
    "        missing_pct = (merged_df[col].isnull().sum() / len(merged_df)) * 100\n",
    "        if missing_pct < 75:\n",
    "            good_predictors.append(col)\n",
    "\n",
    "print(f\"Using {len(good_predictors)} predictors with < 75% missing data\")\n",
    "\n",
    "# Run LASSO analysis\n",
    "if good_outcomes and good_predictors:\n",
    "    print(\"\\nRunning LASSO...\")\n",
    "    try:\n",
    "        table, feature_importance_dict = lasso_covariate_table(\n",
    "            merged_df, \n",
    "            good_outcomes, \n",
    "            predictor_columns=good_predictors\n",
    "        )\n",
    "        \n",
    "        print(\"✓ LASSO completed successfully!\")\n",
    "        print(f\"Results: {len(table)} outcomes analyzed\")\n",
    "        \n",
    "        # Show 5 most important features for each outcome in table format\n",
    "        if len(table) > 0:\n",
    "            print(f\"\\nTop 5 most important features for each outcome:\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Create a summary table\n",
    "            summary_results = []\n",
    "            \n",
    "            for outcome, importance_df in feature_importance_dict.items():\n",
    "                # Get top 5 features by absolute coefficient value\n",
    "                top_5_features = importance_df.head(5)  # Already sorted by Abs_Coefficient\n",
    "                \n",
    "                for i, (_, row) in enumerate(top_5_features.iterrows(), 1):\n",
    "                    summary_results.append({\n",
    "                        'Outcome': outcome,\n",
    "                        'Rank': i,\n",
    "                        'Feature': row['Feature'],\n",
    "                        'Coefficient': row['Coefficient'],\n",
    "                        'Abs_Coefficient': row['Abs_Coefficient']\n",
    "                    })\n",
    "            \n",
    "            # Convert to DataFrame and display\n",
    "            summary_df = pd.DataFrame(summary_results)\n",
    "            \n",
    "            # Display table for each outcome\n",
    "            for outcome in summary_df['Outcome'].unique():\n",
    "                outcome_data = summary_df[summary_df['Outcome'] == outcome]\n",
    "                print(f\"\\n{outcome}:\")\n",
    "                print(outcome_data[['Rank', 'Feature', 'Coefficient']].to_string(index=False))\n",
    "            \n",
    "            # Save the complete results table\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #summary_df.to_csv('lasso_top5_features_april2025.csv', index=False)\n",
    "            print(f\"\\n✓ Saved complete results to 'lasso_top5_features_april2025.csv'\")\n",
    "            \n",
    "            # Overall summary\n",
    "            print(f\"\\n\" + \"=\"*60)\n",
    "            print(f\"OVERALL SUMMARY\")\n",
    "            print(f\"=\"*60)\n",
    "            print(f\"Total outcomes analyzed: {len(feature_importance_dict)}\")\n",
    "            print(f\"Average features selected per outcome: {table['num_selected'].mean():.1f}\")\n",
    "            \n",
    "            # Most frequently selected features across all outcomes\n",
    "            all_features = summary_df['Feature'].value_counts()\n",
    "            if len(all_features) > 0:\n",
    "                print(f\"\\nMost frequently selected features (top 10):\")\n",
    "                for feature, count in all_features.head(10).items():\n",
    "                    print(f\"  {feature}: selected in {count} outcomes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LASSO failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No suitable outcomes or predictors found\")\n",
    "    print(f\"Outcomes: {len(good_outcomes)}, Predictors: {len(good_predictors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def run_lasso_feature_selection(df, outcome_columns, predictor_columns=None, cv=5):\n",
    "    \"\"\"\n",
    "    Simple LASSO feature selection for multiple outcomes.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with outcome and predictor columns\n",
    "        outcome_columns: List of outcome column names\n",
    "        predictor_columns: List of predictor column names (optional)\n",
    "        cv: Number of cross-validation folds\n",
    "    \n",
    "    Returns:\n",
    "        results_df: DataFrame with outcomes and their selected features\n",
    "        feature_importance_dict: Dictionary with feature importance for each outcome\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set default predictors if not provided\n",
    "    if predictor_columns is None:\n",
    "        exclude_cols = outcome_columns + ['time_point', 'time_point2', 'ai_base_score', 'id_it']\n",
    "        predictor_columns = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Filter to existing columns\n",
    "    predictor_columns = [col for col in predictor_columns if col in df.columns]\n",
    "    print(f\"Using {len(predictor_columns)} predictor columns\")\n",
    "    \n",
    "    results = []\n",
    "    feature_importance_dict = {}\n",
    "    \n",
    "    for outcome in outcome_columns:\n",
    "        print(f\"\\nProcessing: {outcome}\")\n",
    "        \n",
    "        if outcome not in df.columns:\n",
    "            print(f\"  Skipping - not found in dataframe\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare data\n",
    "        data = df.dropna(subset=[outcome]).copy()\n",
    "        \n",
    "        if len(data) < 50:\n",
    "            print(f\"  Skipping - only {len(data)} observations\")\n",
    "            continue\n",
    "        \n",
    "        # Convert outcome to numeric\n",
    "        try:\n",
    "            data[outcome] = pd.to_numeric(data[outcome], errors='coerce')\n",
    "            data = data.dropna(subset=[outcome])\n",
    "        except:\n",
    "            print(f\"  Skipping - cannot convert to numeric\")\n",
    "            continue\n",
    "        \n",
    "        if len(data) < 50:\n",
    "            print(f\"  Skipping - only {len(data)} observations after conversion\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = data[predictor_columns]\n",
    "        y = data[outcome]\n",
    "        \n",
    "        # Remove constant columns\n",
    "        constant_cols = X.columns[X.nunique() <= 1]\n",
    "        if len(constant_cols) > 0:\n",
    "            X = X.drop(columns=constant_cols)\n",
    "            print(f\"  Removed {len(constant_cols)} constant columns\")\n",
    "        \n",
    "        # Remove completely empty columns\n",
    "        empty_cols = X.columns[X.isnull().all()]\n",
    "        if len(empty_cols) > 0:\n",
    "            X = X.drop(columns=empty_cols)\n",
    "            print(f\"  Removed {len(empty_cols)} empty columns\")\n",
    "        \n",
    "        if X.shape[1] == 0:\n",
    "            print(f\"  Skipping - no valid predictors\")\n",
    "            continue\n",
    "        \n",
    "        # Create simple pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('lasso', LassoCV(cv=cv, random_state=0, max_iter=2000))\n",
    "        ])\n",
    "        \n",
    "        # Fit model\n",
    "        try:\n",
    "            pipeline.fit(X, y)\n",
    "            \n",
    "            # Get selected features\n",
    "            coefs = pipeline.named_steps['lasso'].coef_\n",
    "            selected_features = X.columns[abs(coefs) > 1e-10].tolist()\n",
    "            \n",
    "            # Create feature importance DataFrame\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': X.columns,\n",
    "                'Coefficient': coefs,\n",
    "                'Abs_Coefficient': abs(coefs)\n",
    "            }).sort_values('Abs_Coefficient', ascending=False)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'outcome': outcome,\n",
    "                'selected_features': ', '.join(selected_features),\n",
    "                'num_selected': len(selected_features),\n",
    "                'observations': len(data),\n",
    "                'best_alpha': pipeline.named_steps['lasso'].alpha_\n",
    "            })\n",
    "            \n",
    "            feature_importance_dict[outcome] = feature_importance\n",
    "            \n",
    "            print(f\"  ✓ Selected {len(selected_features)} features from {len(data)} observations\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"\\n✓ Successfully processed {len(results)} outcomes\")\n",
    "    \n",
    "    return results_df, feature_importance_dict\n",
    "\n",
    "def analyze_results(results_df, feature_importance_dict, top_n=10):\n",
    "    \"\"\"\n",
    "    Simple analysis of LASSO results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"LASSO RESULTS SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if len(results_df) == 0:\n",
    "        print(\"No outcomes were successfully processed\")\n",
    "        return\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"Total outcomes processed: {len(results_df)}\")\n",
    "    print(f\"Average features selected: {results_df['num_selected'].mean():.1f}\")\n",
    "    print(f\"Range of features selected: {results_df['num_selected'].min()}-{results_df['num_selected'].max()}\")\n",
    "    \n",
    "    # Most frequently selected features\n",
    "    all_features = []\n",
    "    for outcome, importance_df in feature_importance_dict.items():\n",
    "        selected = importance_df[importance_df['Abs_Coefficient'] > 1e-10]['Feature'].tolist()\n",
    "        all_features.extend(selected)\n",
    "    \n",
    "    if all_features:\n",
    "        feature_counts = pd.Series(all_features).value_counts()\n",
    "        print(f\"\\nTop {top_n} most frequently selected features:\")\n",
    "        for feature, count in feature_counts.head(top_n).items():\n",
    "            print(f\"  {feature}: selected in {count} outcomes\")\n",
    "    \n",
    "    # Show outcomes with most features selected\n",
    "    print(f\"\\nOutcomes with most features selected:\")\n",
    "    top_outcomes = results_df.nlargest(5, 'num_selected')\n",
    "    for _, row in top_outcomes.iterrows():\n",
    "        print(f\"  {row['outcome']}: {row['num_selected']} features\")\n",
    "    \n",
    "    return feature_counts if all_features else pd.Series()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LASSO analysis with table output\n",
    "print(\"Running LASSO analysis...\")\n",
    "print(f\"merged_df shape: {merged_df.shape}\")\n",
    "\n",
    "# Find outcome columns\n",
    "outcome_cols = [col for col in merged_df.columns \n",
    "               if any(prefix in col for prefix in ['Timely_Care_', 'Death_Complication_', 'HAC_Reduction_', \n",
    "                                                  'Readmission_', 'Medicare_Spending_', 'Unplanned_Visits_'])]\n",
    "\n",
    "# Filter to good outcomes (< 50% missing)\n",
    "good_outcomes = [col for col in outcome_cols \n",
    "                if (merged_df[col].isnull().sum() / len(merged_df)) * 100 < 50]\n",
    "\n",
    "# Filter to good predictors (< 75% missing)  \n",
    "good_predictors = [col for col in all_covariates \n",
    "                  if col in merged_df.columns and \n",
    "                  (merged_df[col].isnull().sum() / len(merged_df)) * 100 < 75]\n",
    "\n",
    "print(f\"Using {len(good_outcomes)} outcomes and {len(good_predictors)} predictors\")\n",
    "\n",
    "# Run LASSO\n",
    "if good_outcomes and good_predictors:\n",
    "    table, feature_importance_dict = lasso_covariate_table(\n",
    "        merged_df, good_outcomes, predictor_columns=good_predictors\n",
    "    )\n",
    "    \n",
    "    # Create simple dataframe with top 5 features as columns\n",
    "    summary_data = []\n",
    "    for outcome, importance_df in feature_importance_dict.items():\n",
    "        top_5 = importance_df.head(5)['Feature'].tolist()\n",
    "        \n",
    "        # Pad with empty strings if less than 5 features\n",
    "        while len(top_5) < 5:\n",
    "            top_5.append('')\n",
    "            \n",
    "        summary_data.append({\n",
    "            'Outcome': outcome,\n",
    "            'Feature_1': top_5[0],\n",
    "            'Feature_2': top_5[1], \n",
    "            'Feature_3': top_5[2],\n",
    "            'Feature_4': top_5[3],\n",
    "            'Feature_5': top_5[4]\n",
    "        })\n",
    "    \n",
    "    # Create and display dataframe\n",
    "    results_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\nTop 5 Features for Each Outcome:\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Save results\n",
    "    #results_df.to_csv('lasso_top5_simple.csv', index=False)\n",
    "    print(f\"\\n✓ Saved to lasso_top5_simple.csv\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No suitable data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_base_ai_implementation_row(row):\n",
    "    \"\"\"\n",
    "    Calculate base AI implementation score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: Base AI implementation score\n",
    "    \"\"\"\n",
    "    # Base AI implementation score (continuous)\n",
    "    # Return None if the input value is null\n",
    "    if pd.isna(row['aipred_it']):\n",
    "        return None\n",
    "    elif row['aipred_it'] == 1:  # Machine Learning\n",
    "        return 2\n",
    "    elif row['aipred_it'] == 2:  # Other Non-Machine Learning Predictive Models\n",
    "        return 1\n",
    "    else:  # Neither (3) or Do not know (4)\n",
    "        return 0\n",
    "\n",
    "def calculate_ai_implementation_breadth_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI implementation breadth score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI implementation breadth score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        breadth_score = base_score\n",
    "        # Implementation Breadth Score - count use cases\n",
    "        use_case_cols = ['aitraj_it', 'airfol_it', 'aimhea_it', 'airect_it', \n",
    "                     'aibill_it', 'aische_it', 'aipoth_it', 'aicloth_it']\n",
    "        for col in use_case_cols:\n",
    "            if row[col] is None:\n",
    "                breadth_score += 0\n",
    "            else:\n",
    "                breadth_score += row[col] * 0.25  # 0.25 points per use case\n",
    "        return breadth_score\n",
    "\n",
    "def calculate_ai_development_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI development score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI development score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0 \n",
    "    else:\n",
    "        dev_score = base_score\n",
    "        if 'mlsed_it' in row and pd.notna(row['mlsed_it']):\n",
    "            dev_score += row['mlsed_it'] * 2  # Self-developed\n",
    "        if 'mldev_it' in row and pd.notna(row['mldev_it']):\n",
    "            dev_score += row['mldev_it']  # EHR developer\n",
    "        if 'mlthd_it' in row and pd.notna(row['mlthd_it']):\n",
    "            dev_score += row['mlthd_it']  # Third-party\n",
    "        if 'mlpubd_it' in row and pd.notna(row['mlpubd_it']):\n",
    "            dev_score += row['mlpubd_it'] * 0.5  # Public domain\n",
    "        return dev_score\n",
    "\n",
    "def calculate_ai_evaluation_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI evaluation score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI evaluation score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        eval_score = base_score\n",
    "        # For model accuracy (MLACCU)\n",
    "        if row['mlaccu_it'] is None:\n",
    "            eval_score += 0\n",
    "        elif row['mlaccu_it'] == 1:  # All models\n",
    "            eval_score += 1\n",
    "        elif row['mlaccu_it'] == 2:  # Most models\n",
    "            eval_score += 0.75\n",
    "        elif row['mlaccu_it'] == 3:  # Some models\n",
    "            eval_score += 0.5\n",
    "        elif row['mlaccu_it'] == 4:  # Few models\n",
    "            eval_score += 0.25\n",
    "        # For None (5) or Do not know (6), no points added\n",
    "    \n",
    "    # For model bias (MLBIAS)\n",
    "        if row['mlbias_it'] is None:\n",
    "            eval_score += 0\n",
    "        elif row['mlbias_it'] == 1:  # All models\n",
    "            eval_score += 1\n",
    "        elif row['mlbias_it'] == 2:  # Most models\n",
    "            eval_score += 0.75\n",
    "        elif row['mlbias_it'] == 3:  # Some models\n",
    "            eval_score += 0.5\n",
    "        elif row['mlbias_it'] == 4:  # Few models\n",
    "            eval_score += 0.25\n",
    "        # For None (5) or Do not know (6), no points added\n",
    "    \n",
    "        return eval_score\n",
    "\n",
    "def calculate_all_ai_scores_row(row):\n",
    "    \"\"\"\n",
    "    Calculate all AI/ML implementation scores as continuous measures for a single row.\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with all calculated scores\n",
    "    \"\"\"\n",
    "    # Calculate all scores\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    breadth_score = calculate_ai_implementation_breadth_row(row)\n",
    "    dev_score = calculate_ai_development_row(row)\n",
    "    eval_score = calculate_ai_evaluation_row(row)\n",
    "    \n",
    "    return {\n",
    "        'ai_base_score': base_score,\n",
    "        'ai_base_breadth_score': breadth_score,\n",
    "        'ai_base_dev_score': dev_score,\n",
    "        'ai_base_eval_score': eval_score\n",
    "    }\n",
    "\n",
    "def apply_ai_scores_to_dataframe(df):\n",
    "    \"\"\"\n",
    "    Apply all AI score calculations row by row to a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: A pandas DataFrame with hospital data\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with added AI score columns\n",
    "    \"\"\"\n",
    "    # Initialize empty columns for scores\n",
    "    df['ai_base_score'] = float('nan')\n",
    "    df['ai_base_breadth_score'] = float('nan')\n",
    "    df['ai_base_dev_score'] = float('nan')\n",
    "    df['ai_base_eval_score'] = float('nan')\n",
    "    \n",
    "    # Apply row by row calculations\n",
    "    for index, row in df.iterrows():\n",
    "        scores = calculate_all_ai_scores_row(row)\n",
    "        for score_name, score_value in scores.items():\n",
    "            df.at[index, score_name] = score_value\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_master2 = apply_ai_scores_to_dataframe(AHA_IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_master2['delivery_system'] = AHA_master2['cluster_as']\n",
    "# Create new column 'system_member' based on the conditions\n",
    "AHA_master2['system_member'] = AHA_master2['mhsmemb_as'].copy()\n",
    "\n",
    "# Set to 1 where sysid_as is not null and mhsmemb_as is null\n",
    "AHA_master2.loc[(AHA_master2['sysid_as'].notna()) & (AHA_master2['mhsmemb_as'].isna()), 'system_member'] = 1\n",
    "\n",
    "# Convert all remaining null values to 0\n",
    "AHA_master2['system_member'] = AHA_master2['system_member'].fillna(0)\n",
    "# bed size \n",
    "AHA_master2['bedsize'] = AHA_master2['bsc_as'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_ml = ['ai_base_score', 'ai_base_breadth_score', 'ai_base_dev_score', 'ai_base_eval_score']\n",
    "hospital_resource = ['delivery_system', 'system_member', 'bedsize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_hospital = pd.read_csv(\"./data/outcomes/merged_general_hospital_info.csv\", low_memory=False)\n",
    "general_hospital = general_hospital.replace('Not Available', np.nan)\n",
    "AHA_master3 = AHA_master2[ai_ml + all_covariates + ['id_it', 'mcrnum_as']]\n",
    "AHA_master3['mcrnum_as'] = AHA_master3['mcrnum_as'].astype(str).str.zfill(6)\n",
    "for col in ai_ml+hospital_resource:\n",
    "    AHA_master3[col] = AHA_master3[col].astype(float).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rural_urban_type\n",
    "# Continue with CBSA type and other variables\n",
    "AHA_master2['rural_urban_type'] = AHA_master2['cbsatype_as'].map({\n",
    "    'Rural': 1,      # Rural = 1 (lowest)\n",
    "    'Micro': 2,      # Micropolitan = 2 (middle)\n",
    "    'Metro': 3       # Metropolitan = 3 (highest)\n",
    "})\n",
    "\n",
    "## system_member\n",
    "# Create new column 'system_member' based on the conditions\n",
    "AHA_master2['system_member'] = AHA_master2['mhsmemb_as'].copy()\n",
    "# Set to 1 where sysid_as is not null and mhsmemb_as is null\n",
    "AHA_master2.loc[(AHA_master2['sysid_as'].notna()) & (AHA_master2['mhsmemb_as'].isna()), 'system_member'] = 1\n",
    "# Convert all remaining null values to 0\n",
    "AHA_master2['system_member'] = AHA_master2['system_member'].fillna(0)\n",
    "\n",
    "## AHA System Cluster Code - delivery_system\n",
    "AHA_master2['delivery_system'] = AHA_master2['cluster_as']\n",
    "\n",
    "## community_hospital\n",
    "AHA_master2['community_hospital'] = AHA_master2['chc_as'].replace(2, 0)\n",
    "\n",
    "## subsidary_hospital\n",
    "AHA_master2['subsidary_hospital'] = AHA_master2['subs_as']\n",
    "\n",
    "## frontline_hospital\n",
    "AHA_master2['frontline_hospital'] = AHA_master2['frtln_as'].replace('.', 0)\n",
    "\n",
    "## joint_commission_accreditation\n",
    "AHA_master2['joint_commission_accreditation'] = AHA_master2['mapp1_as'].replace(2,0)\n",
    "\n",
    "## center_quality\n",
    "AHA_master2['center_quality'] = AHA_master2['mapp22_as'].replace(2,0)\n",
    "\n",
    "# teaching hospitals \n",
    "AHA_master2['teaching_hospital'] = ((AHA_master2['mapp5_as'] == 1) | (AHA_master2['mapp3_as'] == 1) | (AHA_master2['mapp8_as'] == 1)).astype(int)\n",
    "AHA_master2['major_teaching_hospital'] = ((AHA_master2['mapp8_as'] == 1)).astype(int)\n",
    "AHA_master2['minor_teaching_hospital'] = (((AHA_master2['mapp5_as'] == 1) | (AHA_master2['mapp3_as'] == 1))&~(AHA_master2['mapp8_as'] == 1)).astype(int)\n",
    "\n",
    "# critical access hospital\n",
    "AHA_master2['critical_access'] = (AHA_master2['mapp18_as'] == 1).astype(int)\n",
    "\n",
    "\n",
    "# rural referral center \n",
    "AHA_master2['rural_referral'] = (AHA_master2['mapp19_as'] == 1).astype(int)\n",
    "\n",
    "# medicare medicaid percentage\n",
    "AHA_master2['medicare_ipd_percentage'] = AHA_master2['mcripd_as'] / AHA_master2['ipdtot_as'] * 100\n",
    "AHA_master2['medicaid_ipd_percentage'] = AHA_master2['mcdipd_as'] / AHA_master2['ipdtot_as'] * 100\n",
    "\n",
    "# bed size \n",
    "AHA_master2['bedsize'] = AHA_master2['bsc_as'].astype(int)\n",
    "\n",
    "# hospital ownership type \n",
    "\n",
    "AHA_master2['nonfederal_governement'] = ((AHA_master2['cntrl_as'] == 12) | (AHA_master2['cntrl_as'] == 13)|(AHA_master['cntrl_as'] == 14) | (AHA_master['cntrl_as'] == 15)| (AHA_master['cntrl_as'] == 16)).astype(int)\n",
    "AHA_master2['non_profit_nongovernment'] = ((AHA_master2['cntrl_as'] == 21) | (AHA_master2['cntrl_as'] == 23)).astype(int)\n",
    "AHA_master2['for_profit'] = ((AHA_master2['cntrl_as'] == 31) | (AHA_master2['cntrl_as'] == 32) | (AHA_master['cntrl_as'] == 33)).astype(int)\n",
    "AHA_master2['federal_government'] = ((AHA_master2['cntrl_as'] == 40) | (AHA_master2['cntrl_as'] == 44) | (AHA_master2['cntrl_as'] == 45) | (AHA_master2['cntrl_as'] == 46) | (AHA_master['cntrl_as'] == 47) | (AHA_master['cntrl_as'] == 48)).astype(int)\n",
    "# Create a categorical column for hospital ownership types\n",
    "def create_ownership_category(row):\n",
    "    if row['cntrl_as'] in [12, 13, 14, 15, 16]:\n",
    "        return 'nonfederal_government'\n",
    "    elif row['cntrl_as'] in [21, 23]:\n",
    "        return 'non_profit_nongovernment'\n",
    "    elif row['cntrl_as'] in [31, 32, 33]:\n",
    "        return 'for_profit'\n",
    "    elif row['cntrl_as'] in [40, 44, 45, 46, 47, 48]:\n",
    "        return 'federal_government'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Create the categorical column\n",
    "AHA_master2['ownership_type'] = AHA_master2.apply(create_ownership_category, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_master3 = AHA_master2[ai_ml + all_covariates + ['id_it', 'mcrnum_as']]\n",
    "AHA_master3['mcrnum_as'] = AHA_master3['mcrnum_as'].astype(str).str.zfill(6)\n",
    "for col in ai_ml+hospital_resource:\n",
    "    AHA_master3[col] = AHA_master3[col].astype(float).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see what we're dealing with\n",
    "print(\"Sample of mcrnum_as values:\")\n",
    "print(AHA_master3['mcrnum_as'].head(10))\n",
    "\n",
    "# Then try this step-by-step conversion\n",
    "# 1. First replace any 'nan' strings with actual NaN\n",
    "AHA_master3['mcrnum_as'] = AHA_master3['mcrnum_as'].replace('nan', np.nan)\n",
    "\n",
    "# 2. Convert to numeric, coercing errors to NaN\n",
    "AHA_master3['mcrnum_as'] = pd.to_numeric(AHA_master3['mcrnum_as'], errors='coerce')\n",
    "\n",
    "# 3. Convert to integer (this will handle the decimal points)\n",
    "AHA_master3['mcrnum_as'] = AHA_master3['mcrnum_as'].fillna(-1).astype(int)\n",
    "\n",
    "# 4. Convert to string and pad with zeros\n",
    "AHA_master3['mcrnum_as'] = AHA_master3['mcrnum_as'].apply(lambda x: str(x).zfill(6) if x != -1 else np.nan)\n",
    "\n",
    "# Check the result\n",
    "print(\"\\nResult after conversion:\")\n",
    "print(AHA_master3['mcrnum_as'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_complication = pd.read_csv(\"./data/outcomes/merged_death_complication.csv\", low_memory=False)\n",
    "death_complication = death_complication.replace('Not Available', np.nan)\n",
    "death_complication_AHA = AHA_master3.merge(death_complication, left_on = 'mcrnum_as', right_on = 'Facility ID', how = 'left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the get_sort_index function to properly return a value\n",
    "def get_sort_index(tp):\n",
    "    try:\n",
    "        if pd.isna(tp) or tp == 'nan':\n",
    "            return np.nan\n",
    "        month, year = tp.split('_')\n",
    "        return f\"{year}_{month.zfill(2)}\"  # Returns \"2022_01\" format\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_hospital_quality_over_time(df, outcome_column):\n",
    "    # Set publication-quality settings\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'Helvetica',\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 12,\n",
    "        'axes.titlesize': 14,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10,\n",
    "        'legend.fontsize': 10,\n",
    "        'figure.dpi': 300,\n",
    "        'savefig.dpi': 300,\n",
    "        'pdf.fonttype': 42,\n",
    "        'ps.fonttype': 42,\n",
    "        'axes.linewidth': 1,\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.3,\n",
    "        'grid.linestyle': '--',\n",
    "        'grid.linewidth': 0.5\n",
    "    })\n",
    "\n",
    "    # Data preparation\n",
    "    df[outcome_column] = pd.to_numeric(df[outcome_column], errors='coerce')\n",
    "    df['time_point'] = df['time_point'].astype(str).replace('nan', np.nan)\n",
    "    \n",
    "    # Apply the function to transform date format within this function\n",
    "    df['time_point2'] = df['time_point'].apply(get_sort_index)\n",
    "    model_df = df[[outcome_column, 'time_point2', 'ai_base_score', 'id_it']].dropna()\n",
    "    model_df['ai_cat'] = model_df['ai_base_score'].astype('category')\n",
    "    \n",
    "    # Fit model\n",
    "    model = smf.mixedlm(f'{outcome_column} ~ time_point2 + ai_cat + time_point2:ai_cat',\n",
    "                    model_df, groups='id_it').fit(reml=False)\n",
    "    \n",
    "    # Define ALL quarters with numeric indices for proper ordering\n",
    "    quarter_data = {\n",
    "        '2022_01': {'idx': 0, 'label': 'Q1 2022'},\n",
    "        '2022_04': {'idx': 1, 'label': 'Q2 2022'},\n",
    "        '2022_07': {'idx': 2, 'label': 'Q3 2022'}, \n",
    "        '2022_10': {'idx': 3, 'label': 'Q4 2022'},\n",
    "        '2023_01': {'idx': 4, 'label': 'Q1 2023'},\n",
    "        '2023_04': {'idx': 5, 'label': 'Q2 2023'},\n",
    "        '2023_07': {'idx': 6, 'label': 'Q3 2023'},\n",
    "        '2023_10': {'idx': 7, 'label': 'Q4 2023'},\n",
    "        '2024_01': {'idx': 8, 'label': 'Q1 2024'},\n",
    "        '2024_04': {'idx': 9, 'label': 'Q2 2024'}, \n",
    "        '2024_07': {'idx': 10, 'label': 'Q3 2024'}, \n",
    "        '2024_10': {'idx': 11, 'label': 'Q4 2024'}, \n",
    "        '2025_02': {'idx': 12, 'label': 'Q1 2025'},\n",
    "        '2025_04': {'idx': 13, 'label': 'Q2 2025'},\n",
    "    }\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    summary = (model_df\n",
    "           .groupby(['time_point2', 'ai_cat'])[outcome_column]\n",
    "           .agg(['mean', 'sem'])\n",
    "           .reset_index())\n",
    "    \n",
    "    # Add numeric indices for proper sorting\n",
    "    summary['x_idx'] = summary['time_point2'].map(lambda x: quarter_data.get(x, {}).get('idx', -1))\n",
    "    \n",
    "    # Create figure with specific size and DPI\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=300)\n",
    "    \n",
    "    # Define custom color palette\n",
    "    colors = {\n",
    "        0: '#808080',  # Gray for AI 0\n",
    "        1: '#2ca02c',  # Green for AI 1\n",
    "        2: '#1f77b4'   # Blue for AI 2\n",
    "    }\n",
    "    \n",
    "    # Plot for each AI level using numeric indices for x-axis\n",
    "    for ai in sorted(summary['ai_cat'].unique()):\n",
    "        subset = summary[summary['ai_cat'] == ai].sort_values('x_idx')\n",
    "        \n",
    "        # Plot lines connecting available data points\n",
    "        ax.plot(subset['x_idx'], subset['mean'], \n",
    "                label=f'AI {int(ai)}', \n",
    "                marker='o',\n",
    "                color=colors[int(ai)],\n",
    "                linewidth=2,\n",
    "                markersize=8,\n",
    "                markeredgecolor='white',\n",
    "                markeredgewidth=1)\n",
    "        \n",
    "        # Add error bands\n",
    "        ax.fill_between(subset['x_idx'],\n",
    "                     subset['mean'] - subset['sem'],\n",
    "                     subset['mean'] + subset['sem'],\n",
    "                     alpha=0.2,\n",
    "                     color=colors[int(ai)])\n",
    "\n",
    "    # Set x-ticks at all quarter positions\n",
    "    x_ticks = [v['idx'] for v in quarter_data.values()]\n",
    "    x_labels = [v['label'] for v in quarter_data.values()]\n",
    "    \n",
    "    # Ensure x-axis shows ALL quarters in proper order\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "    \n",
    "    # Set x-axis limits to ensure we start at Q1 2022\n",
    "    ax.set_xlim(-0.5, max(x_ticks) + 0.5)\n",
    "    \n",
    "    # Calculate appropriate y-axis limits\n",
    "    y_min = summary['mean'].min() - 2 * summary['sem'].max()\n",
    "    y_max = summary['mean'].max() + 2 * summary['sem'].max()\n",
    "    \n",
    "    # Add some padding\n",
    "    y_range = y_max - y_min\n",
    "    y_min = max(0, y_min - 0.05 * y_range)  # Start at 0 if data allows\n",
    "    y_max = y_max + 0.05 * y_range\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel(\"Quarter\", fontweight='bold', labelpad=10)\n",
    "    ax.set_ylabel(\"Quality Metric (Mean ± SEM)\", fontweight='bold', labelpad=10)\n",
    "    ax.set_title(f\"{outcome_column} Over Time by AI Level\", \n",
    "                 fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add legend with custom formatting\n",
    "    legend = ax.legend(title=\"AI Level\", \n",
    "                      title_fontsize=12,\n",
    "                      frameon=True,\n",
    "                      framealpha=0.95,\n",
    "                      edgecolor='black',\n",
    "                      loc='best')\n",
    "    \n",
    "    # Add grid with custom styling\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print data point statistics \n",
    "    print(\"Data points by time and AI level:\")\n",
    "    crosstab = pd.crosstab(summary['time_point2'], summary['ai_cat'])\n",
    "    print(crosstab)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # Print model summary statistics\n",
    "    print(\"\\nModel Summary Statistics:\")\n",
    "    print(f\"Number of observations: {len(model_df)}\")\n",
    "    print(f\"Number of hospitals: {model_df['id_it'].nunique()}\")\n",
    "    print(\"\\nFixed Effects:\")\n",
    "    print(model.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy\n",
    "\n",
    "def visualize_adjusted_hospital_quality_over_time(\n",
    "    df, outcome_column, additional_covariates=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot adjusted trends of a quality metric over time by AI level,\n",
    "    adjusting for optional selected covariates.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with columns:\n",
    "        * outcome_column (continuous)\n",
    "        * time_point (str, e.g., '2022_01')\n",
    "        * ai_base_score (numeric or categorical)\n",
    "        * id_it (hospital identifier)\n",
    "        * any additional covariates (columns in df) if provided\n",
    "    - outcome_column: str name of the quality metric column\n",
    "    - additional_covariates: list of column names to include as fixed effects\n",
    "    \"\"\"\n",
    "    # Publication-style settings\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'Helvetica',\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 12,\n",
    "        'axes.titlesize': 14,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10,\n",
    "        'legend.fontsize': 10,\n",
    "        'figure.dpi': 300,\n",
    "        'axes.linewidth': 1,\n",
    "    })\n",
    "    \n",
    "    # Copy & core prep\n",
    "    df = df.copy()\n",
    "    df[outcome_column] = pd.to_numeric(df[outcome_column], errors='coerce')\n",
    "    df['time_point2'] = df['time_point'].apply(get_sort_index)\n",
    "    df['ai_cat'] = df['ai_base_score'].astype('category')\n",
    "    \n",
    "    # Build model DataFrame\n",
    "    cols = [outcome_column, 'time_point2', 'ai_cat', 'id_it']\n",
    "    if additional_covariates:\n",
    "        cols += additional_covariates\n",
    "    model_df = df[cols].dropna()\n",
    "    model_df['time_num'] = model_df['time_point2'].apply(convert_time_to_numeric)\n",
    "    \n",
    "    # Construct formula\n",
    "    base_terms = [\"time_num * ai_cat\"]\n",
    "    cov_terms = additional_covariates if additional_covariates else []\n",
    "    formula = f\"{outcome_column} ~ \" + \" + \".join(base_terms + cov_terms)\n",
    "    \n",
    "    # Fit mixed-effects model\n",
    "    model = smf.mixedlm(formula, model_df, groups='id_it', re_formula=\"~time_num\")\n",
    "    result = model.fit(reml=False)\n",
    "    \n",
    "    # Quarter mapping\n",
    "    quarter_data = {\n",
    "        '2022_01': (0, 'Q1 2022'), '2022_04': (1, 'Q2 2022'),\n",
    "        '2022_07': (2, 'Q3 2022'), '2022_10': (3, 'Q4 2022'),\n",
    "        '2023_01': (4, 'Q1 2023'), '2023_04': (5, 'Q2 2023'),\n",
    "        '2023_07': (6, 'Q3 2023'), '2023_10': (7, 'Q4 2023'),\n",
    "        '2024_01': (8, 'Q1 2024'), '2024_04': (9, 'Q2 2024'),\n",
    "        '2024_07': (10,'Q3 2024'), '2024_10': (11,'Q4 2024'),\n",
    "        '2025_02': (12,'Q1 2025'), '2025_04': (13,'Q2 2025')\n",
    "    }\n",
    "    \n",
    "    # Get typical values for covariates\n",
    "    typicals = {}\n",
    "    if additional_covariates:\n",
    "        for cov in additional_covariates:\n",
    "            if pd.api.types.is_numeric_dtype(model_df[cov]):\n",
    "                typicals[cov] = model_df[cov].median()\n",
    "            else:\n",
    "                typicals[cov] = model_df[cov].mode()[0]\n",
    "                \n",
    "    # Build prediction grid\n",
    "    times = sorted(model_df['time_point2'].unique(), key=lambda x: quarter_data[x][0])\n",
    "    ai_levels = sorted(model_df['ai_cat'].cat.categories)\n",
    "    pred_rows = []\n",
    "    for t in times:\n",
    "        for a in ai_levels:\n",
    "            row = {\n",
    "                'time_point2': t,\n",
    "                'time_num': convert_time_to_numeric(t),\n",
    "                'ai_cat': a\n",
    "            }\n",
    "            for cov, val in typicals.items():\n",
    "                row[cov] = val\n",
    "            pred_rows.append(row)\n",
    "    pred_df = pd.DataFrame(pred_rows)\n",
    "    \n",
    "    # Predict\n",
    "    design = patsy.dmatrix(formula, pred_df, return_type='dataframe')\n",
    "    pred_df['predicted'] = result.predict(design)\n",
    "    pred_df['x_idx'] = pred_df['time_point2'].map(lambda x: quarter_data[x][0])\n",
    "    pred_df['label'] = pred_df['time_point2'].map(lambda x: quarter_data[x][1])\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=300)\n",
    "    colors = {0:'#808080',1:'#2ca02c',2:'#1f77b4'}\n",
    "    for a in ai_levels:\n",
    "        sub = pred_df[pred_df['ai_cat']==a].sort_values('x_idx')\n",
    "        ax.plot(sub['x_idx'], sub['predicted'], marker='o', color=colors[int(a)], label=f\"AI {a}\")\n",
    "    ax.set_xticks([v[0] for v in quarter_data.values()])\n",
    "    ax.set_xticklabels([v[1] for v in quarter_data.values()], rotation=45, ha='right')\n",
    "    ax.set_xlim(-0.5, max(v[0] for v in quarter_data.values())+0.5)\n",
    "    ax.set_xlabel(\"Quarter\", fontweight='bold', labelpad=10)\n",
    "    ax.set_ylabel(f\"Adjusted {outcome_column}\", fontweight='bold', labelpad=10)\n",
    "    title = f\"{outcome_column} Over Time by AI Level (Adjusted)\"\n",
    "    if additional_covariates:\n",
    "        title += \"\\n(adjusted for \" + \", \".join(additional_covariates) + \")\"\n",
    "    ax.set_title(title, fontweight='bold', pad=20)\n",
    "    ax.legend(title=\"AI Level\", frameon=True, edgecolor='black')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"MixedLM Results:\\n\", result.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's convert the time points to a numeric format\n",
    "# Assuming time_point2 is in format 'YYYY_MM'\n",
    "def convert_time_to_numeric(time_str):\n",
    "    year, month = time_str.split('_')\n",
    "    return float(year) + (float(month) - 1) / 12  # This will give us years as decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_slopes_by_ai_level(df, outcome_column):\n",
    "    df[outcome_column] = pd.to_numeric(df[outcome_column], errors='coerce')\n",
    "    df['time_point2'] = df['time_point'].apply(get_sort_index)\n",
    "    model_df = df[[outcome_column, 'time_point2', 'ai_base_score', 'id_it']].dropna()\n",
    "    model_df['time_point2_numeric'] = model_df['time_point2'].apply(convert_time_to_numeric)\n",
    "    model_df['ai_cat'] = model_df['ai_base_score'].astype('category')\n",
    "    # Now fit the model with the numeric time variable\n",
    "    model = smf.mixedlm(f'{outcome_column} ~ time_point2_numeric + ai_cat + time_point2_numeric:ai_cat',\n",
    "                    model_df, groups='id_it')\n",
    "    result = model.fit(reml=False)\n",
    "    # Extract coefficients\n",
    "    coefs = result.params\n",
    "\n",
    "    # Calculate slopes per group\n",
    "    slopes = {\n",
    "    \"AI 0 (reference)\": coefs[\"time_point2_numeric\"],\n",
    "    \"AI 1\": coefs[\"time_point2_numeric\"] + coefs.get(\"time_point2_numeric:ai_cat[T.1.0]\", 0),\n",
    "    \"AI 2\": coefs[\"time_point2_numeric\"] + coefs.get(\"time_point2_numeric:ai_cat[T.2.0]\", 0)\n",
    "    }\n",
    "\n",
    "    # Format as a DataFrame\n",
    "    slope_df = pd.DataFrame.from_dict(slopes, orient='index', columns=['Estimated Slope (per year)'])\n",
    "    slope_df.index.name = \"AI Level\"\n",
    "\n",
    "    # Display the results\n",
    "    print(\"\\nEstimated Slopes by AI Level:\")\n",
    "    print(slope_df)\n",
    "\n",
    "    # Show the full model summary\n",
    "    print(\"\\nModel Summary:\")\n",
    "    print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adjusted_slopes_by_ai_level(df, outcome_column, additional_covariates=None):\n",
    "    \"\"\"\n",
    "    Fit a mixed-effects model for outcome over time by AI level,\n",
    "    adjusting for any selected covariates, and compute slopes per AI group.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame with required columns\n",
    "    - outcome_column: name of the outcome variable\n",
    "    - additional_covariates: list of column names to include as fixed effects\n",
    "\n",
    "    Returns:\n",
    "    - result: fitted mixedlm result object\n",
    "    - slope_df: pandas.DataFrame with estimated slopes per AI level\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[outcome_column] = pd.to_numeric(df[outcome_column], errors='coerce')\n",
    "    df['time_point2'] = df['time_point'].apply(get_sort_index)\n",
    "    cols = [outcome_column, 'time_point2', 'ai_base_score', 'id_it']\n",
    "    if additional_covariates:\n",
    "        cols += additional_covariates\n",
    "    model_df = df[cols].dropna()\n",
    "    \n",
    "    # Numeric time and categorical AI\n",
    "    model_df['time_point2_numeric'] = model_df['time_point2'].apply(convert_time_to_numeric)\n",
    "    model_df['ai_cat'] = model_df['ai_base_score'].astype('category')\n",
    "    \n",
    "    # Build formula string\n",
    "    base_terms = ['time_point2_numeric', 'ai_cat', 'time_point2_numeric:ai_cat']\n",
    "    cov_terms = additional_covariates or []\n",
    "    formula = f\"{outcome_column} ~ \" + \" + \".join(base_terms + cov_terms)\n",
    "    \n",
    "    # Fit mixed-effects model\n",
    "    model = smf.mixedlm(formula, model_df, groups='id_it')\n",
    "    result = model.fit(reml=False)\n",
    "    \n",
    "   # Extract slopes for each AI category\n",
    "    coefs = result.params\n",
    "    slopes = {\n",
    "        \"AI 0 (reference)\": coefs[\"time_point2_numeric\"],\n",
    "        \"AI 1\": coefs[\"time_point2_numeric\"] + coefs.get(\"time_point2_numeric:ai_cat[T.1.0]\", 0),\n",
    "        \"AI 2\": coefs[\"time_point2_numeric\"] + coefs.get(\"time_point2_numeric:ai_cat[T.2.0]\", 0)\n",
    "    }\n",
    "    \n",
    "    # Create detailed results table\n",
    "    results_table = pd.DataFrame({\n",
    "        'Coefficient': result.params,\n",
    "        'Std Error': result.bse,\n",
    "        'P-value': result.pvalues,\n",
    "        'Lower CI': result.conf_int()[0],\n",
    "        'Upper CI': result.conf_int()[1]\n",
    "    }).round(5)\n",
    "    \n",
    "    # Create slope dataframe with more information\n",
    "    slope_df = pd.DataFrame.from_dict(slopes, orient='index', columns=['Estimated Slope'])\n",
    "    slope_df.index.name = \"AI Level\"\n",
    "    slope_df = slope_df.round(5)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nSelected covariates:\", additional_covariates)\n",
    "    print(\"\\nEstimated Slopes by AI Level:\")\n",
    "    print(slope_df)\n",
    "    \n",
    "    print(\"\\nDetailed Model Results:\")\n",
    "    print(results_table)\n",
    "    \n",
    "    print(\"\\nModel Summary:\")\n",
    "    print(result.summary())\n",
    "    \n",
    "    # Reset display options\n",
    "    pd.reset_option('display.float_format')\n",
    "    \n",
    "    return result, slope_df, results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_covariate(result, outcome):\n",
    "    \"\"\"\n",
    "    Extract the top 5 features for a specific outcome from lasso results.\n",
    "    Allows partial matching of outcome names.\n",
    "    \n",
    "    Args:\n",
    "        result (pd.DataFrame): DataFrame containing lasso results\n",
    "        outcome (str): The outcome name to filter for (can be partial)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of top 5 features for the specified outcome\n",
    "    \"\"\"\n",
    "    # Find all outcomes that contain the search string\n",
    "    matching_outcomes = result[result['Outcome'].str.contains(outcome, case=False)]\n",
    "    \n",
    "    if matching_outcomes.empty:\n",
    "        available_outcomes = result['Outcome'].tolist()\n",
    "        raise ValueError(f\"No outcomes found containing '{outcome}'. Available outcomes are:\\n\" + \n",
    "                       \"\\n\".join(available_outcomes))\n",
    "    \n",
    "    if len(matching_outcomes) > 1:\n",
    "        print(f\"Warning: Multiple outcomes found containing '{outcome}':\")\n",
    "        for idx, row in matching_outcomes.iterrows():\n",
    "            print(f\"- {row['Outcome']}\")\n",
    "        print(\"Using the first match.\")\n",
    "    \n",
    "    result_row = matching_outcomes.iloc[[0]]  # Take the first match\n",
    "    \n",
    "    features = []\n",
    "    for i in range(5):\n",
    "        num = i + 1\n",
    "        string = 'Feature_' + str(num)\n",
    "        features.append(result_row[string].iloc[0])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_adjusted_hospital_quality_over_time(df, outcome_column, additional_covariates):\n",
    "    # Set publication-quality settings\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'Helvetica',\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 12,\n",
    "        'axes.titlesize': 14,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10,\n",
    "        'legend.fontsize': 10,\n",
    "        'figure.dpi': 300,\n",
    "        'savefig.dpi': 300,\n",
    "        'pdf.fonttype': 42,\n",
    "        'ps.fonttype': 42,\n",
    "        'axes.linewidth': 1,\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.3,\n",
    "        'grid.linestyle': '--',\n",
    "        'grid.linewidth': 0.5\n",
    "    })\n",
    "\n",
    "    # Data preparation\n",
    "    print(\"Data preparation and validation:\")\n",
    "    print(f\"Total rows in dataset: {len(df)}\")\n",
    "    \n",
    "    # Convert outcome to numeric and handle missing values\n",
    "    df[outcome_column] = pd.to_numeric(df[outcome_column], errors='coerce')\n",
    "    df['time_point'] = df['time_point'].astype(str).replace('nan', np.nan)\n",
    "    \n",
    "    # Apply the function to transform date format\n",
    "    df['time_point2'] = df['time_point'].apply(get_sort_index)\n",
    "    \n",
    "    # Create model dataframe and handle missing values\n",
    "    model_df = df[[outcome_column, 'time_point2', 'ai_base_score', 'id_it'] + additional_covariates].dropna()\n",
    "    print(f\"Rows after dropping missing values: {len(model_df)}\")\n",
    "    \n",
    "    # Convert ai_base_score to category\n",
    "    model_df['ai_cat'] = model_df['ai_base_score'].astype('category')\n",
    "    \n",
    "    # Create the formula string with all covariates\n",
    "    formula = f\"Q('{outcome_column}') ~ time_point2 + ai_cat + time_point2:ai_cat + \" + \" + \".join(additional_covariates)\n",
    "    \n",
    "    # Fit model\n",
    "    model = smf.mixedlm(formula, model_df, groups='id_it').fit(reml=False)\n",
    "    \n",
    "    # Get unique time points from the data\n",
    "    unique_time_points = sorted(model_df['time_point2'].unique())\n",
    "    \n",
    "    # Create prediction dataframe using only time points that exist in the data\n",
    "    pred_rows = []\n",
    "    for time_point in unique_time_points:\n",
    "        for ai in model_df['ai_cat'].unique():\n",
    "            # Use median values for additional covariates\n",
    "            row = {'time_point2': time_point, 'ai_cat': ai}\n",
    "            for cov in additional_covariates:\n",
    "                row[cov] = model_df[cov].median()\n",
    "            pred_rows.append(row)\n",
    "    \n",
    "    pred_df = pd.DataFrame(pred_rows)\n",
    "    \n",
    "    # Add the outcome column (required by statsmodels)\n",
    "    pred_df[outcome_column] = 0  # This value doesn't matter for prediction\n",
    "    \n",
    "    # Get predictions\n",
    "    pred_df['predicted'] = model.predict(pred_df)\n",
    "    \n",
    "    # Create mapping for x-axis labels\n",
    "    quarter_labels = {\n",
    "        '2022_01': 'Q1 2022',\n",
    "        '2022_04': 'Q2 2022',\n",
    "        '2022_07': 'Q3 2022',\n",
    "        '2022_10': 'Q4 2022',\n",
    "        '2023_01': 'Q1 2023',\n",
    "        '2023_04': 'Q2 2023',\n",
    "        '2023_07': 'Q3 2023',\n",
    "        '2023_10': 'Q4 2023',\n",
    "        '2024_01': 'Q1 2024',\n",
    "        '2024_04': 'Q2 2024',\n",
    "        '2024_07': 'Q3 2024',\n",
    "        '2024_10': 'Q4 2024',\n",
    "        '2025_02': 'Q1 2025',\n",
    "        '2025_04': 'Q2 2025'\n",
    "    }\n",
    "    \n",
    "    # Add numeric indices for proper sorting\n",
    "    pred_df['x_idx'] = pred_df['time_point2'].map(lambda x: list(unique_time_points).index(x))\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=300)\n",
    "    \n",
    "    # Define custom color palette\n",
    "    colors = {\n",
    "        0: '#808080',  # Gray for AI 0\n",
    "        1: '#2ca02c',  # Green for AI 1\n",
    "        2: '#1f77b4'   # Blue for AI 2\n",
    "    }\n",
    "    \n",
    "    # Plot for each AI level\n",
    "    for ai in sorted(pred_df['ai_cat'].unique()):\n",
    "        subset = pred_df[pred_df['ai_cat'] == ai].sort_values('x_idx')\n",
    "        \n",
    "        # Plot lines connecting available data points\n",
    "        ax.plot(subset['x_idx'], subset['predicted'], \n",
    "                label=f'AI {int(ai)}', \n",
    "                marker='o',\n",
    "                color=colors[int(ai)],\n",
    "                linewidth=2,\n",
    "                markersize=8,\n",
    "                markeredgecolor='white',\n",
    "                markeredgewidth=1)\n",
    "    \n",
    "    # Set x-ticks\n",
    "    x_ticks = range(len(unique_time_points))\n",
    "    x_labels = [quarter_labels.get(tp, tp) for tp in unique_time_points]\n",
    "    \n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "    \n",
    "    # Set axis limits\n",
    "    ax.set_xlim(-0.5, len(unique_time_points) - 0.5)\n",
    "    \n",
    "    # Calculate appropriate y-axis limits\n",
    "    y_min = pred_df['predicted'].min() - 0.05 * (pred_df['predicted'].max() - pred_df['predicted'].min())\n",
    "    y_max = pred_df['predicted'].max() + 0.05 * (pred_df['predicted'].max() - pred_df['predicted'].min())\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel(\"Quarter\", fontweight='bold', labelpad=10)\n",
    "    ax.set_ylabel(\"Adjusted Quality Metric\", fontweight='bold', labelpad=10)\n",
    "    \n",
    "    # Add title and subtitle\n",
    "    ax.set_title(f\"Adjusted {outcome_column} Over Time by AI Level\", \n",
    "                 fontweight='bold', pad=20)\n",
    "\n",
    "    \n",
    "    # Add legend\n",
    "    legend = ax.legend(title=\"AI Level\", \n",
    "                      title_fontsize=12,\n",
    "                      frameon=True,\n",
    "                      framealpha=0.95,\n",
    "                      edgecolor='black',\n",
    "                      loc='best')\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print model summary and coefficient values\n",
    "    print(\"\\nModel Summary Statistics:\")\n",
    "    print(f\"Number of observations: {len(model_df)}\")\n",
    "    print(f\"Number of hospitals: {model_df['id_it'].nunique()}\")\n",
    "    print(\"\\nFixed Effects:\")\n",
    "    print(model.summary().tables[1])\n",
    "    \n",
    "    # Print the actual coefficient values for additional covariates\n",
    "    print(\"\\nCoefficients for additional covariates:\")\n",
    "    for cov in additional_covariates:\n",
    "        if cov in model.params.index:\n",
    "            print(f\"{cov}: {model.params[cov]:.4f}\")\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
