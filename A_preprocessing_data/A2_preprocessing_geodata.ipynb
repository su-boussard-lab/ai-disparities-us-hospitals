{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Data Preprocessing – Geo Data\n",
    "\n",
    "**Description**  \n",
    "This section preprocesses geospatial and community data from multiple sources.  \n",
    "It includes data loading, filtering, recoding, and cleaning steps to prepare datasets for geospatial analysis.\n",
    "\n",
    "**Data Sources**  \n",
    "1. Hospital Service Areas – Dartmouth Atlas: https://data.dartmouthatlas.org/supplemental/  \n",
    "2. Area Deprivation Index – Neighborhood Atlas: https://www.neighborhoodatlas.medicine.wisc.edu/  \n",
    "3. Social Vulnerability Index – CDC/ATSDR: https://www.atsdr.cdc.gov/place-health/php/svi/index.html  \n",
    "4. Health Professional Shortage Areas (HPSA) and Medically Underserved Areas (MUA) – HRSA: https://data.hrsa.gov/data/download  \n",
    "5. Internet access, device usage, broadband access – U.S. Census API\n",
    "\n",
    "**Purpose**  \n",
    "To prepare a merged geospatial dataset from multiple sources for downstream hospital- and region-level analysis.\n",
    "\n",
    "**Input**\n",
    "- AHA 2023 and 2024 master dataframe created from A1 (AHA20232024_master.csv)\n",
    "- Zipcode & Hospital Service Area Crosswalk Data (ZipHsaHrr19.csv)\n",
    "- individual state ADI files (\\*_ADI_\\*.csv)\n",
    "- SVI file (SVI_2022_US_ZCTA.csv)\n",
    "- HPSA primary health file (HPSA_PCP.shp)\n",
    "- HPSA dental health file (HPSA_DH.shp)\n",
    "- HPSA mental health file (HPSA_MH.shp)\n",
    "- MUA file (MUA.shp\")\n",
    "\n",
    "**Output**\n",
    "- combined_adi_national_by_zip5.csv\n",
    "- hospital_adi_df.csv\n",
    "- hospital_county_internet_df.csv\n",
    "- hospital_svi_df.csv\n",
    "- hospital_HPSA_MUA_score_df.csv\n",
    "- AHA_master_external_data.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. load necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load libraries \n",
    "# Import standard Python libraries\n",
    "import getpass  \n",
    "import re \n",
    "import json \n",
    "import sys  \n",
    "\n",
    "# Import data analysis and visualization libraries\n",
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt  \n",
    "import geopandas as gpd\n",
    "# Import datetime utilities\n",
    "from datetime import datetime, timedelta  \n",
    "\n",
    "\n",
    "# Import operating system utilities\n",
    "import os  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 load AHA master and hospital service area crosswalk data \n",
    "1. load AHA master data \n",
    "2. load hospital service area crosswalk data \n",
    "3. link AHA master and hospital service area crosswalk data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2_1 load AHA master data created in A1_preprocessing_hospital_data.ipynb \n",
    "AHA_master = pd.read_csv('AHA20232024_master.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2_2 load hospital service area crosswalk data from Dartmouth Atlas \n",
    "hsa_crosswalk = pd.read_csv('ZipHsaHrr19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2_3 link AHA master and hospital service area crosswalk data \n",
    "\n",
    "# This section links each hospital in the AHA master dataset to its corresponding Hospital Service Area (HSA) using a ZIP code-level crosswalk. The steps include:\n",
    "# - Padding ZIP codes to standard 5-digit format\n",
    "# - Aggregating ZIP codes by HSA\n",
    "# - Merging AHA hospital records with HSA \n",
    "\n",
    "\n",
    "# Function to standardize ZIP codes to 5-digit format\n",
    "def pad_zipcode(zipcode):\n",
    "    \"\"\"Pad ZIP code with leading zeros to ensure 5-digit format.\"\"\"\n",
    "    if pd.isna(zipcode):\n",
    "        return zipcode\n",
    "    zipcode = str(zipcode).strip()\n",
    "    zipcode = ''.join(filter(str.isdigit, zipcode))  # Keep only numeric characters\n",
    "    return zipcode.zfill(5)\n",
    "\n",
    "# Standardize ZIP code in the HSA crosswalk\n",
    "hsa_crosswalk['zipcode19'] = hsa_crosswalk['zipcode19'].astype(str)\n",
    "hsa_crosswalk['zipcode19'] = hsa_crosswalk['zipcode19'].str[:5].apply(pad_zipcode)\n",
    "\n",
    "# Aggregate ZIP codes by HSA code, city, and state\n",
    "hsa_aggregated = hsa_crosswalk.groupby(['hsanum', 'hsacity', 'hsastate']).agg({\n",
    "    'zipcode19': lambda x: list(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Ensure consistent data types for merging\n",
    "hsa_crosswalk['hsanum'] = hsa_crosswalk['hsanum'].astype(int)\n",
    "AHA_master['hsacode_as'] = AHA_master['hsacode_as'].fillna(0).astype(int)\n",
    "\n",
    "# Merge AHA data with HSA metadata using HSA code\n",
    "merged_data = pd.merge(\n",
    "    AHA_master,\n",
    "    hsa_aggregated,\n",
    "    how='left',\n",
    "    left_on='hsacode_as',\n",
    "    right_on='hsanum'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. load geodata \n",
    "1. Area Deprivation Index - zipcode level \n",
    "2. Social Vulnerability Index - zipcode level \n",
    "3. Healthcare Professional Shortage Area - boundary with geo-dataframe \n",
    "4. Internet Access, Broadband Access, Device Usage - county level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3_1 Load and Aggregate Area Deprivation Index (ADI) Data\n",
    "\n",
    "# This section processes multiple ADI files from the Neighborhood Atlas, each corresponding to a different state.  \n",
    "# For each file:\n",
    "# - It identifies the ZIP code column\n",
    "# - Extracts 5-digit ZIP codes\n",
    "# - Aggregates national and state ADI ranks at the ZIP level using median values of each hospital's service area ZIP codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Function to process a single ADI file and return ZIP-level aggregations\n",
    "def process_adi_file(file_path):\n",
    "    print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "    \n",
    "    # Read the file\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"  Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check for ADI columns\n",
    "    has_natrank = 'ADI_NATRANK' in df.columns\n",
    "    has_staterank = 'ADI_STATERANK' in df.columns\n",
    "    \n",
    "    # Identify the ZIP column\n",
    "    zip_column = next((col for col in df.columns if 'zip' in col.lower()), None)\n",
    "\n",
    "    # Standardize ZIP to 5 digits\n",
    "    df[zip_column] = df[zip_column].astype(str)\n",
    "    df['zipcode5'] = df[zip_column].str[:5]\n",
    "\n",
    "    # Extract state code from filename\n",
    "    state_code = os.path.basename(file_path).split('_')[0]\n",
    "    df['state'] = state_code\n",
    "\n",
    "    # Convert ADI columns to numeric\n",
    "    if has_natrank:\n",
    "        df['ADI_NATRANK'] = pd.to_numeric(df['ADI_NATRANK'], errors='coerce')\n",
    "    if has_staterank:\n",
    "        df['ADI_STATERANK'] = pd.to_numeric(df['ADI_STATERANK'], errors='coerce')\n",
    "\n",
    "    # Aggregate by 5-digit ZIP\n",
    "    nat_agg = None\n",
    "    state_agg = None\n",
    "\n",
    "    if has_natrank:\n",
    "        nat_agg = df.groupby(['state', 'zipcode5'])['ADI_NATRANK'].median().reset_index()\n",
    "        nat_agg.rename(columns={'ADI_NATRANK': 'adi_national_median'}, inplace=True)\n",
    "\n",
    "    if has_staterank:\n",
    "        state_agg = df.groupby(['state', 'zipcode5'])['ADI_STATERANK'].median().reset_index()\n",
    "        state_agg.rename(columns={'ADI_STATERANK': 'adi_state_median'}, inplace=True)\n",
    "\n",
    "\n",
    "    return nat_agg, state_agg\n",
    "\n",
    "# Get all ADI CSV files in the folder\n",
    "adi_files = glob.glob('../data/ADI/*_ADI_*.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3_2 Process and Merge Area Deprivation Index (ADI) Files\n",
    "\n",
    "# **Description**  \n",
    "# This section processes multiple ADI files from the Neighborhood Atlas (one per state), standardizes ZIP codes, and aggregates national and state ADI rankings at the ZIP code level. It includes:\n",
    "\n",
    "# - Padding and standardizing ZIP codes\n",
    "# - Converting ADI rank columns to numeric\n",
    "# - Aggregating median ADI scores at the ZIP level per state\n",
    "# - Merging all state files into combined national and state-level ADI datasets\n",
    "# - Saving the outputs for downstream geospatial analysis\n",
    "\n",
    "# The final output includes three files:\n",
    "# - `combined_adi_national_by_zip5.csv`: Median national ADI ranks per ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to pad ZIP codes\n",
    "def pad_zipcode(zipcode,digits):\n",
    "    \"\"\"Pad ZIP code with leading zeros to make it 5 digits\"\"\"\n",
    "    if pd.isna(zipcode):\n",
    "        return zipcode\n",
    "    # Convert to string and remove any non-numeric characters\n",
    "    zipcode = str(zipcode).strip()\n",
    "    zipcode = ''.join(filter(str.isdigit, zipcode))\n",
    "    # Pad with leading zeros to make it 5 digits\n",
    "    return zipcode.zfill(digits)\n",
    "\n",
    "# Get all ADI files\n",
    "adi_files = glob.glob('./ADI/*_ADI_*.csv')\n",
    "\n",
    "# Containers for results\n",
    "nat_aggregated_dfs = []\n",
    "state_aggregated_dfs = []\n",
    "\n",
    "# Process each file\n",
    "for file_path in adi_files:\n",
    "    state_code = os.path.basename(file_path).split('_')[0]\n",
    "    print(f\"Processing {state_code}...\")\n",
    "    \n",
    "    # Read the ADI file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Identify the ZIP code column\n",
    "    zip_column = None\n",
    "    for col in df.columns:\n",
    "        if 'zip' in col.lower():\n",
    "            zip_column = col\n",
    "            break\n",
    "    \n",
    "    if not zip_column:\n",
    "        print(f\"  Warning: No ZIP column found in {state_code}\")\n",
    "        continue\n",
    "    \n",
    "    # Check for ADI columns\n",
    "    has_natrank = 'ADI_NATRANK' in df.columns\n",
    "    \n",
    "    if not (has_natrank):\n",
    "        print(f\"  Warning: No ADI columns found in {state_code}\")\n",
    "        continue\n",
    "    \n",
    "    # Convert ZIP to string and extract first 5 digits, then pad with zeros\n",
    "    df[zip_column] = df[zip_column].astype(str)\n",
    "    df[zip_column] = df[zip_column].apply(pad_zipcode, digits = 9)\n",
    "    df['zipcode5'] = df[zip_column].str[:5].apply(pad_zipcode, digits = 5)\n",
    "    df['state'] = state_code\n",
    "    \n",
    "    # Process national rank data\n",
    "    if has_natrank:\n",
    "        # Convert to numeric\n",
    "        df['ADI_NATRANK'] = pd.to_numeric(df['ADI_NATRANK'], errors='coerce')\n",
    "        \n",
    "        # Drop rows with null ADI values\n",
    "        nat_df = df.dropna(subset=['ADI_NATRANK'])\n",
    "        print(f\"  National rank: {len(df) - len(nat_df)} null values dropped\")\n",
    "        \n",
    "        # Aggregate to ZIP code level\n",
    "        nat_agg = nat_df.groupby(['state', 'zipcode5'])['ADI_NATRANK'].median().reset_index()\n",
    "        nat_agg.rename(columns={'ADI_NATRANK': 'adi_national_median'}, inplace=True)\n",
    "        \n",
    "        nat_aggregated_dfs.append(nat_agg)\n",
    "        print(f\"  Aggregated national rank: {nat_agg.shape[0]} ZIP codes\")\n",
    "    \n",
    "\n",
    "# Combine all files\n",
    "combined_nat_adi = pd.concat(nat_aggregated_dfs, ignore_index=True) if nat_aggregated_dfs else None\n",
    "\n",
    "# Save files\n",
    "if combined_nat_adi is not None:\n",
    "    combined_nat_adi = combined_nat_adi.drop_duplicates(subset=['zipcode5'], keep='first')\n",
    "    combined_nat_adi.to_csv('combined_adi_national_by_zip5.csv', index=False)\n",
    "    print(f\"\\nSaved national ADI data: {combined_nat_adi.shape[0]} total ZIP codes from {combined_nat_adi['state'].nunique()} states\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nNational ADI Statistics:\")\n",
    "    print(combined_nat_adi['adi_national_median'].describe())\n",
    "    \n",
    "    # Count by state\n",
    "    print(\"\\nZIP codes per state (National Rank):\")\n",
    "    nat_state_counts = combined_nat_adi['state'].value_counts().sort_index()\n",
    "    for state, count in nat_state_counts.items():\n",
    "        print(f\"  {state}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_nat_adi = pd.read_csv('combined_adi_national_by_zip5.csv')\n",
    "combined_nat_adi['zipcode5'] = combined_nat_adi['zipcode5'].astype(str).str.zfill(5)\n",
    "\n",
    "def calculate_hospital_adi_metrics(hospital_row, zip_adi_data):\n",
    "    \"\"\"\n",
    "    Calculate ADI metrics for a hospital based on its service area ZIP codes\n",
    "    \"\"\"\n",
    "    # Get ZIP codes for this hospital's service area\n",
    "    service_area_zips = hospital_row['zipcode19']\n",
    "    \n",
    "    # Check for None, empty list, or list containing only NaN values\n",
    "    if service_area_zips is None or len(service_area_zips) == 0:\n",
    "        return {\n",
    "            'national_adi_median': np.nan\n",
    "        }\n",
    "    \n",
    "    # Filter out any None or NaN values if it's a list\n",
    "    if isinstance(service_area_zips, list):\n",
    "        service_area_zips = [zip for zip in service_area_zips if pd.notna(zip)]\n",
    "        if len(service_area_zips) == 0:\n",
    "            return {\n",
    "                'national_adi_median': np.nan\n",
    "            }\n",
    "    \n",
    "    # Find matching ZIP codes in ADI data\n",
    "    service_area_data = zip_adi_data[zip_adi_data['zipcode5'].isin(service_area_zips)]\n",
    "    \n",
    "    # If no matching ZIP codes, return NaN values\n",
    "    if len(service_area_data) == 0:\n",
    "        return {\n",
    "            'national_adi_median': np.nan\n",
    "        }\n",
    "    \n",
    "    # Calculate median of national and state ADI ranks\n",
    "    national_adi_median = service_area_data['adi_national_median'].median()\n",
    "\n",
    "    return {\n",
    "        'national_adi_median': national_adi_median\n",
    "    }\n",
    "\n",
    "# Apply function to each hospital\n",
    "adi_values = []\n",
    "for idx, row in merged_data.iterrows():\n",
    "    try:\n",
    "        values = calculate_hospital_adi_metrics(row, combined_nat_adi)\n",
    "        values['id_as'] = row['id_as']\n",
    "        adi_values.append(values)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {idx}: {e}\")\n",
    "        adi_values.append({\n",
    "            'id_as': row['id_as'],\n",
    "            'national_adi_median': np.nan,\n",
    "            'state_adi_median': np.nan\n",
    "        })\n",
    "\n",
    "# Create DataFrame with ADI values\n",
    "hospital_adi_df = pd.DataFrame(adi_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_adi_df.to_csv('hospital_adi_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3_3 Load and Aggregate Social Vulnerability Index (ADI) Data\n",
    "\n",
    "# This section processes SVI index data from the CDC and aggregates the SVI scores at the ZIP code level for each hospital's service area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = pd.read_csv('SVI_2022_US_ZCTA.csv')\n",
    "svi['zipcode5'] = svi['FIPS'].apply(pad_zipcode, digits = 5)\n",
    "svi_columns = ['zipcode5','RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4','RPL_THEMES']\n",
    "svi_concise = svi[svi_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes Social Vulnerability Index (SVI) metrics at the hospital level based on ZIP codes in their service areas.  \n",
    "# For each hospital, the code:\n",
    "# - Extracts its ZIP codes\n",
    "# - Matches them with SVI data\n",
    "# - Calculates the median values for each SVI theme across the ZIPs\n",
    "\n",
    "# The result is a summary of social vulnerability for the hospital’s service region.\n",
    "\n",
    "def calculate_hospital_svi_metrics(hospital_row, zip_svi_data):\n",
    "    \"\"\"\n",
    "    Calculate ADI metrics for a hospital based on its service area ZIP codes\n",
    "    \"\"\"\n",
    "    # Get ZIP codes for this hospital's service area\n",
    "    service_area_zips = hospital_row['zipcode19']\n",
    "    \n",
    "    # Check for None, empty list, or list containing only NaN values\n",
    "    if service_area_zips is None or len(service_area_zips) == 0:\n",
    "        return {\n",
    "            'svi_themes_median': np.nan,\n",
    "            'svi_theme1_median': np.nan,\n",
    "            'svi_theme2_median': np.nan,\n",
    "            'svi_theme3_median': np.nan,\n",
    "            'svi_theme4_median': np.nan\n",
    "        }\n",
    "    \n",
    "    # Filter out any None or NaN values if it's a list\n",
    "    if isinstance(service_area_zips, list):\n",
    "        service_area_zips = [zip for zip in service_area_zips if pd.notna(zip)]\n",
    "        if len(service_area_zips) == 0:\n",
    "            return {\n",
    "                'svi_themes_median': np.nan,\n",
    "                'svi_theme1_median': np.nan,\n",
    "                'svi_theme2_median': np.nan,\n",
    "                'svi_theme3_median': np.nan,\n",
    "                'svi_theme4_median': np.nan\n",
    "            }\n",
    "    \n",
    "    # Find matching ZIP codes in SVI data\n",
    "    service_area_data = zip_svi_data[zip_svi_data['zipcode5'].isin(service_area_zips)]\n",
    "    \n",
    "    # If no matching ZIP codes, return NaN values\n",
    "    if len(service_area_data) == 0:\n",
    "        return {\n",
    "            'svi_themes_median': np.nan,\n",
    "            'svi_theme1_median': np.nan,\n",
    "            'svi_theme2_median': np.nan,\n",
    "            'svi_theme3_median': np.nan,\n",
    "            'svi_theme4_median': np.nan\n",
    "        }\n",
    "    \n",
    "    # Calculate median of SVI scores for the hospital's service area\n",
    "    svi_themes_median = service_area_data['RPL_THEMES'].median()\n",
    "    svi_theme1_median = service_area_data['RPL_THEME1'].median()\n",
    "    svi_theme2_median = service_area_data['RPL_THEME2'].median()\n",
    "    svi_theme3_median = service_area_data['RPL_THEME3'].median()\n",
    "    svi_theme4_median = service_area_data['RPL_THEME4'].median()\n",
    "\n",
    "    return {\n",
    "        'svi_themes_median': svi_themes_median,\n",
    "        'svi_theme1_median': svi_theme1_median,\n",
    "        'svi_theme2_median': svi_theme2_median,\n",
    "        'svi_theme3_median': svi_theme3_median,\n",
    "        'svi_theme4_median': svi_theme4_median\n",
    "    }\n",
    "\n",
    "# Apply function to each hospital\n",
    "svi_values = []\n",
    "for idx, row in merged_data.iterrows():\n",
    "    try:\n",
    "        values = calculate_hospital_svi_metrics(row, svi)\n",
    "        values['id_as'] = row['id_as']\n",
    "        svi_values.append(values)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {idx}: {e}\")\n",
    "        svi_values.append({\n",
    "            'id_as': row['id_as'],\n",
    "            'svi_themes_median': np.nan,\n",
    "            'svi_theme1_median': np.nan,\n",
    "            'svi_theme2_median': np.nan,\n",
    "            'svi_theme3_median': np.nan,\n",
    "            'svi_theme4_median': np.nan\n",
    "        })\n",
    "\n",
    "# Create DataFrame with ADI values\n",
    "hospital_svi_df = pd.DataFrame(svi_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_svi_df.to_csv('hospital_svi_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3_4 Retrieve and Process Census Data on Internet and Device Access\n",
    "\n",
    "# This section uses the U.S. Census Bureau's API to retrieve ACS 5-year estimates on household technology access by county.  \n",
    "# It calculates metrics such as broadband access, internet access, and device availability, which can be linked to hospital county.\n",
    "# This process repeats A0_data_collection.ipynb to show how the data was processed but please refer to the A0_data_collection.ipynb for more details.\n",
    "\n",
    "\n",
    "# **Variables used:**\n",
    "# - `B28001_001E`: Total households (computer/device access)\n",
    "# - `B28001_002E`: Households with at least one device\n",
    "# - `B28001_008E`: Households with no device\n",
    "# - `B28002_001E`: Total households (internet subscription)\n",
    "# - `B28002_008E`: Households with broadband\n",
    "# - `B28002_013E`: Households with no internet access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Set up\n",
    "params = {\n",
    "    \"get\": \"NAME,B28001_001E,B28001_002E,B28001_008E,B28002_001E,B28002_008E,B28002_004E,B28002_002E,B28002_013E\",\n",
    "    \"for\": \"county:*\",\n",
    "    \"in\": \"state:*\",\n",
    "    \"key\": API_KEY\n",
    "}\n",
    "\n",
    "# Request data\n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# Convert to DataFrame\n",
    "internet_access = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "# Rename columns for clarity\n",
    "internet_access = internet_access.rename(columns={\n",
    "    \"NAME\": \"County\",\n",
    "    \"B28001_001E\": \"Total_Computer_Households\",\n",
    "    \"B28001_002E\": \"at_least_1_device\",\n",
    "    \"B28001_008E\": \"no_device\",\n",
    "    \"B28002_001E\": \"Total_Internet_Households\",\n",
    "    \"B28002_004E\": \"With_Broadband\",\n",
    "    \"B28002_002E\": \"Internet_Access\",\n",
    "    \"B28002_013E\" : \"No_Internet\",\n",
    "\n",
    "    \"state\": \"State_Code\",\n",
    "    \"county\": \"County_Code\"\n",
    "})\n",
    "\n",
    "# Convert numeric columns\n",
    "for col in [\"Total_Computer_Households\", \"at_least_1_device\", \"no_device\", \"Total_Internet_Households\", \"With_Broadband\", \"No_Internet\"]:\n",
    "    internet_access[col] = pd.to_numeric(internet_access[col])\n",
    "\n",
    "# Calculate percent\n",
    "internet_access[\"Device_Percent\"] = (internet_access[\"at_least_1_device\"] / internet_access[\"Total_Computer_Households\"] * 100).round(2)\n",
    "internet_access[\"Broadband_Percent\"] = (internet_access[\"With_Broadband\"] / internet_access[\"Total_Internet_Households\"] * 100).round(2)\n",
    "internet_access[\"Internet_Percent\"] = (\n",
    "    (1 - internet_access[\"No_Internet\"] / internet_access[\"Total_Internet_Households\"]) * 100\n",
    ").round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure both columns are strings and properly formatted\n",
    "internet_access['State_Code'] = internet_access['State_Code'].astype(str).str.zfill(2)\n",
    "internet_access['County_Code'] = internet_access['County_Code'].astype(str).str.zfill(3)\n",
    "\n",
    "# Create the combined FIPS column\n",
    "internet_access['fstct'] = internet_access['State_Code'] + internet_access['County_Code']\n",
    "internet_access_concise = internet_access[['fstct','County','Device_Percent','Broadband_Percent','Internet_Percent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['fcounty_as'] = merged_data['fcounty_as'].astype(str).str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internet_merged = merged_data.merge(internet_access_concise, left_on = 'fcounty_as', right_on = 'fstct', how = 'left')[['id_as','Device_Percent','Broadband_Percent','Internet_Percent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internet_merged.to_csv('hospital_county_internet_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3_5 Load HPSA and MUA data \n",
    "# This section uses geospatial joins to link each hospital with its corresponding:\n",
    "# - Primary Care Health Professional Shortage Area (HPSA)\n",
    "# - Dental HPSA\n",
    "# - Mental Health HPSA\n",
    "# - Medically Underserved Area (MUA)\n",
    "\n",
    "# Scores from each layer are assigned to hospitals based on whether their geocoded hospital location falls within these designated regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# --- Load HPSA and MUA shapefiles ---\n",
    "hpsa_primary = gpd.read_file(\"HPSA_PCP.shp\")\n",
    "hpsa_dental = gpd.read_file(\"HPSA_DH.shp\")\n",
    "hpsa_mental = gpd.read_file(\"HPSA_MH.shp\")\n",
    "mua = gpd.read_file(\"MUA.shp\")\n",
    "\n",
    "# --- Convert AHA hospitals to GeoDataFrame using address coordinates ---\n",
    "AHA_master['geometry'] = AHA_master.apply(lambda row: Point(row.long_as, row.lat_as), axis=1)\n",
    "hospitals_gdf = gpd.GeoDataFrame(AHA_master, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "# --- Ensure all layers share the same CRS ---\n",
    "hpsa_primary = hpsa_primary.to_crs(\"EPSG:4326\")\n",
    "hpsa_dental = hpsa_dental.to_crs(\"EPSG:4326\")\n",
    "hpsa_mental = hpsa_mental.to_crs(\"EPSG:4326\")\n",
    "mua = mua.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# --- Rename score columns for clarity ---\n",
    "hpsa_primary['primary_hpss'] = hpsa_primary['HpsScore']\n",
    "hpsa_dental['dental_hpss'] = hpsa_dental['HpsScore']\n",
    "hpsa_mental['mental_hpss'] = hpsa_mental['HpsScore']\n",
    "mua['mua_score'] = mua['MuaSCORE']  # IMU: 0=worst, 100=best\n",
    "mua['mua_elders_score'] = mua['PpAge6PIu']  # Elder component: 0=worst, 26=best\n",
    "mua['mua_infant_score'] = mua['InfMorRtIu']  # Infant component: 0=worst, 26=best\n",
    "\n",
    "# --- Spatial joins: assign scores to hospitals within each polygon layer ---\n",
    "primary_joined = gpd.sjoin(hospitals_gdf, hpsa_primary[['primary_hpss', 'geometry']], how=\"left\", predicate=\"intersects\")\n",
    "dental_joined = gpd.sjoin(hospitals_gdf, hpsa_dental[['dental_hpss', 'geometry']], how=\"left\", predicate=\"intersects\")\n",
    "mental_joined = gpd.sjoin(hospitals_gdf, hpsa_mental[['mental_hpss', 'geometry']], how=\"left\", predicate=\"intersects\")\n",
    "mua_joined = gpd.sjoin(hospitals_gdf, mua[['mua_score', 'mua_elders_score', 'mua_infant_score', 'geometry']], how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "# --- Replace missing scores (hospitals not within HPSA/MUA) with 0 ---\n",
    "primary_joined['primary_hpss'] = primary_joined['primary_hpss'].fillna(0)\n",
    "dental_joined['dental_hpss'] = dental_joined['dental_hpss'].fillna(0)\n",
    "mental_joined['mental_hpss'] = mental_joined['mental_hpss'].fillna(0)\n",
    "mua_joined['mua_score'] = mua_joined['mua_score'].fillna(100)\n",
    "mua_joined['mua_elders_score'] = mua_joined['mua_elders_score'].fillna(20.2)\n",
    "mua_joined['mua_infant_score'] = mua_joined['mua_infant_score'].fillna(26)\n",
    "mua_joined['mua_shortage'] = 100 - mua_joined['mua_score']\n",
    "mua_joined['mua_elders_shortage'] = 20.2 - mua_joined['mua_elders_score']\n",
    "mua_joined['mua_infant_shortage'] = 26 - mua_joined['mua_infant_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mua_columns = ['mua_score', 'mua_elders_score', 'mua_infant_score', 'mua_shortage', 'mua_elders_shortage', 'mua_infant_shortage']\n",
    "mua_aggregated = mua_joined.groupby('id_as')[mua_columns].mean().reset_index()\n",
    "mua_aggregated.rename(columns={\n",
    "    'mua_score': 'mean_mua_score',\n",
    "    'mua_elders_score': 'mean_mua_elders_score',\n",
    "    'mua_infant_score': 'mean_mua_infant_score', \n",
    "    'mua_shortage': 'mean_mua_shortage',\n",
    "    'mua_elders_shortage': 'mean_mua_elders_shortage',\n",
    "    'mua_infant_shortage': 'mean_mua_infant_shortage'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merge all HPSA and MUA summaries into one hospital-level DataFrame ---\n",
    "all_hpsa_mua = mua_aggregated\n",
    "\n",
    "# --- Preview the combined data ---\n",
    "print(all_hpsa_mua.head())\n",
    "\n",
    "# --- Show summary statistics ---\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(all_hpsa_mua.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hpsa_mua.to_csv('hospital_HPSA_MUA_score_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 link all data to AHA_master\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link all data to AHA_master\n",
    "AHA_master2 = AHA_master.copy()\n",
    "AHA_master2 = AHA_master2.merge(hospital_adi_df, left_on = 'id_as', right_on = 'id_as', how = 'left')\n",
    "AHA_master2 = AHA_master2.merge(hospital_svi_df, left_on = 'id_as', right_on = 'id_as', how = 'left')\n",
    "AHA_master2 = AHA_master2.merge(internet_merged, left_on = 'id_as', right_on = 'id_as', how = 'left')\n",
    "AHA_master2 = AHA_master2.merge(all_hpsa_mua, left_on = 'id_as', right_on = 'id_as', how = 'left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_master2.to_csv('AHA_master_external_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
