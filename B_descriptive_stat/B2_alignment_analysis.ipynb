{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Alignment Analysis â€“ Need vs AI Implementation\n",
    "\n",
    "**Description**  \n",
    "This section evaluates the alignment between healthcare need and AI implementation levels across hospitals. Tertiles are computed for both variables to assess patterns of alignment. \n",
    "\n",
    "**Purpose**  \n",
    "To examine whether AI implementation correspond with areas of greatest need. \n",
    "\n",
    "**Method Summary**  \n",
    "- Rank-based tertiles were created for HPSA, MUA, ADI, SVI scores.  \n",
    "- AI implementation scores were already categorized into three ctegories (Low, Medium, High).  \n",
    "- Cross-tabulations were generated and visualized using heatmaps.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1 Load necessary libraries, functions, and pre-processed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load necessary libraries \n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_exposures = [\"ai_base_score\",\n",
    "\"ai_base_breadth_score\",\n",
    "\"ai_base_dev_score\",\n",
    "\"ai_base_eval_score_2023\",\n",
    "\"ai_base_eval_score_2024\",\n",
    "\"llm_readiness_score\", \n",
    "\"ai_base_score_imputed\",\n",
    "\"ai_base_breadth_score_imputed\",\n",
    "\"ai_base_dev_score_imputed\",\n",
    "\"ai_base_eval_score_2023_imputed\",\n",
    "\"ai_base_eval_score_2024_imputed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_master = pd.read_csv(\"../../data/AHA_master_external_data.csv\", low_memory=False)\n",
    "AHA_IT = AHA_master[AHA_master.id_it.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Data engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all functions but only use what you need\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from calculate_scores import (\n",
    "    create_union_aipred_row, \n",
    "    calculate_base_ai_implementation_row_imputed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_IT['aipred_it_union'] = AHA_IT.apply(calculate_scores.create_union_aipred_row, axis=1)\n",
    "AHA_IT = calculate_scores.apply_ai_scores_to_dataframe(AHA_IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_division = {\n",
    "    # Division 1: New England\n",
    "    'ME': 'New England', 'NH': 'New England', 'VT': 'New England', \n",
    "    'MA': 'New England', 'RI': 'New England', 'CT': 'New England',\n",
    "    \n",
    "    # Division 2: Mid Atlantic\n",
    "    'NY': 'Mid Atlantic', 'NJ': 'Mid Atlantic', 'PA': 'Mid Atlantic',\n",
    "    \n",
    "    # Division 3: South Atlantic\n",
    "    'DE': 'South Atlantic', 'MD': 'South Atlantic', 'DC': 'South Atlantic',\n",
    "    'VA': 'South Atlantic', 'WV': 'South Atlantic', 'NC': 'South Atlantic',\n",
    "    'SC': 'South Atlantic', 'GA': 'South Atlantic', 'FL': 'South Atlantic',\n",
    "    \n",
    "    # Division 4: East North Central\n",
    "    'OH': 'East North Central', 'IN': 'East North Central', 'IL': 'East North Central',\n",
    "    'MI': 'East North Central', 'WI': 'East North Central',\n",
    "    \n",
    "    # Division 5: East South Central\n",
    "    'KY': 'East South Central', 'TN': 'East South Central', \n",
    "    'AL': 'East South Central', 'MS': 'East South Central',\n",
    "    \n",
    "    # Division 6: West North Central\n",
    "    'MN': 'West North Central', 'IA': 'West North Central', 'MO': 'West North Central',\n",
    "    'ND': 'West North Central', 'SD': 'West North Central', 'NE': 'West North Central',\n",
    "    'KS': 'West North Central',\n",
    "    \n",
    "    # Division 7: West South Central\n",
    "    'AR': 'West South Central', 'LA': 'West South Central', \n",
    "    'OK': 'West South Central', 'TX': 'West South Central',\n",
    "    \n",
    "    # Division 8: Mountain\n",
    "    'MT': 'Mountain', 'ID': 'Mountain', 'WY': 'Mountain', 'CO': 'Mountain',\n",
    "    'NM': 'Mountain', 'AZ': 'Mountain', 'UT': 'Mountain', 'NV': 'Mountain',\n",
    "    \n",
    "    # Division 9: Pacific\n",
    "    'WA': 'Pacific', 'OR': 'Pacific', 'CA': 'Pacific', \n",
    "    'AK': 'Pacific', 'HI': 'Pacific',\n",
    "    \n",
    "    # Territories\n",
    "    'PR': 'Territories', 'GU': 'Territories', 'VI': 'Territories', \n",
    "    'AS': 'Territories', 'MP': 'Territories'\n",
    "}\n",
    "division_to_region = {\n",
    "    'New England' : 'Northeast',\n",
    "    'Mid Atlantic' : 'Northeast', \n",
    "    'East North Central' : 'Midwest', \n",
    "    'West North Central' : 'Midwest', \n",
    "    'South Atlantic' : 'South', \n",
    "    'East South Central' : 'South', \n",
    "    'West South Central' : 'South', \n",
    "    'Mountain' : 'West', \n",
    "    'Pacific' : 'West'\n",
    " }\n",
    "# Add census division column to the dataframe\n",
    "AHA_IT['division'] = AHA_IT['mstate_it'].map(state_to_division)\n",
    "AHA_IT['region'] = AHA_IT['division'].map(division_to_region)\n",
    "AHA_IT_US = AHA_IT[AHA_IT2['division']!='Territories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model_type mapping\n",
    "AHA_IT_US['model_type'] = AHA_IT_US['ai_base_score_imputed'].map({\n",
    "    0: 'No Models',\n",
    "    1: 'Non-AI Predictive Models', \n",
    "    2: 'AI Predictive Models'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Alignment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rank_based_tertiles(df, column_name, labels=['Low Need', 'Medium Need', 'High Need']):\n",
    "        return pd.qcut(df[column_name].rank(method='first'), 3, labels=labels)\n",
    "\n",
    "def create_standard_tertiles(df, column_name, labels=['Low Need', 'Medium Need', 'High Need']):\n",
    "        return pd.qcut(df[column_name], 3, labels=labels)\n",
    "\n",
    "def create_designation_binary(df, column_name):\n",
    "        return (df[column_name] > 0).astype(int)\n",
    "    \n",
    "def rr_from_joint_table(tbl, ai_label=\"AI Predictive Models\", hi_label=None, lo_label=None):\n",
    "    if hi_label is None or lo_label is None:\n",
    "        idx = list(tbl.index)\n",
    "        hi_label = next((lab for lab in idx if \"high\" in str(lab).lower()), idx[-1])\n",
    "        lo_label = next((lab for lab in idx if \"low\" in str(lab).lower()), idx[0])\n",
    "    num_H = float(tbl.loc[hi_label, ai_label])\n",
    "    den_H = float(tbl.loc[hi_label, :].sum())\n",
    "    num_L = float(tbl.loc[lo_label, ai_label])\n",
    "    den_L = float(tbl.loc[lo_label, :].sum())\n",
    "    pH = num_H / den_H if den_H > 0 else np.nan\n",
    "    pL = num_L / den_L if den_L > 0 else np.nan\n",
    "    RR = np.nan if (pL == 0 or np.isnan(pL)) else pH / pL\n",
    "    rel = 100 * (RR - 1) if np.isfinite(RR) else np.nan\n",
    "    return pH, pL, RR, rel, hi_label, lo_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AI implementation categories\n",
    "AHA_IT_US['model_type'] = AHA_IT_US['ai_base_score_imputed'].map({\n",
    "    0: 'No Models',\n",
    "    1: 'Non-AI Predictive Models', \n",
    "    2: 'AI Predictive Models'\n",
    "})\n",
    "\n",
    "AHA_IT_US['model_type'] = pd.Categorical(\n",
    "    AHA_IT_US['model_type'],\n",
    "    categories=['No Models', 'Non-AI Predictive Models', 'AI Predictive Models'],\n",
    "    ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPSA/MUA measures - RANK-BASED tertiles\n",
    "hpsa_mua_measures = {\n",
    "    'primary_hpss_tertile': 'mean_primary_hpss',\n",
    "    'mental_hpss_tertile': 'mean_mental_hpss',\n",
    "    'dental_hpss_tertile': 'mean_dental_hpss',\n",
    "    'mua_score_tertile': 'mean_mua_score',\n",
    "    'mua_elder_tertile': 'mean_mua_elders_score',\n",
    "    'mua_infant_tertile': 'mean_mua_infant_score'\n",
    "}\n",
    "for tertile_col, score_col in hpsa_mua_measures.items():\n",
    "    AHA_IT_US[tertile_col] = create_rank_based_tertiles(AHA_IT_US, score_col)\n",
    "\n",
    "# Socioeconomic measures - STANDARD tertiles\n",
    "socio_measures = {\n",
    "    'adi_tertile': 'national_adi_median',\n",
    "    'svi_tertile': 'svi_themes_median'\n",
    "}\n",
    "    \n",
    "for tertile_col, score_col in socio_measures.items():\n",
    "    AHA_IT_US[tertile_col] = create_standard_tertiles(AHA_IT_US, score_col)\n",
    "    \n",
    "# Create designation flags\n",
    "designation_measures = {\n",
    "    \"primary_hpsa_desig\": \"mean_primary_hpss\",\n",
    "    \"mental_hpsa_desig\": \"mean_mental_hpss\",\n",
    "    \"dental_hpsa_desig\": \"mean_dental_hpss\",\n",
    "    \"mua_overall_desig\": \"mean_mua_score\",\n",
    "    \"mua_elder_desig\": \"mean_mua_elders_score\",\n",
    "    \"mua_infant_desig\": \"mean_mua_infant_score\"\n",
    "}\n",
    "    \n",
    "for desig_col, score_col in designation_measures.items():\n",
    "    AHA_IT_US[desig_col] = create_designation_binary(AHA_IT_US, score_col)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create socioeconomic cross-tabulations\n",
    "socioeconomic_measures = {\n",
    "    'Area Deprivation Index': 'adi_tertile',\n",
    "    'Social Vulnerability Index': 'svi_tertile'\n",
    "}\n",
    "\n",
    "socioeconomic_tables = {}\n",
    "socioeconomic_stats = {}\n",
    "\n",
    "for name, column in socioeconomic_measures.items():\n",
    "    # Create cross-tabulation\n",
    "    cross_tab = pd.crosstab(AHA_IT_US[column], AHA_IT_US['model_type'], normalize=True) * 100\n",
    "    \n",
    "    # Reorder to put High Need at TOP, Low Need at BOTTOM\n",
    "    cross_tab = cross_tab.reindex(['High Need', 'Medium Need', 'Low Need'])\n",
    "    \n",
    "    # Ensure all expected columns exist, fill missing with 0\n",
    "    for col in MODEL_LABELS:\n",
    "        if col not in cross_tab.columns:\n",
    "            cross_tab[col] = 0\n",
    "    \n",
    "    # Reorder columns\n",
    "    cross_tab = cross_tab[MODEL_LABELS]\n",
    "    \n",
    "    socioeconomic_tables[name] = cross_tab\n",
    "    \n",
    "    # Calculate statistics\n",
    "    pH, pL, RR, rel, hi_label, lo_label = rr_from_joint_table(cross_tab, ai_label=AI_LABEL)\n",
    "    socioeconomic_stats[name] = {'pH': pH, 'pL': pL, 'RR': RR, 'rel': rel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create designation tables\n",
    "provider_names = [\"Primary HPSA\", \"Mental HPSA\", \"Dental HPSA\", \"MUA Overall\", \"MUA Elder\", \"MUA Infant\"]\n",
    "flag_col = {\n",
    "    \"Primary HPSA\": \"primary_hpsa_desig\",\n",
    "    \"Mental HPSA\": \"mental_hpsa_desig\",\n",
    "    \"Dental HPSA\": \"dental_hpsa_desig\",\n",
    "    \"MUA Overall\": \"mua_overall_desig\",\n",
    "    \"MUA Elder\": \"mua_elder_desig\",\n",
    "    \"MUA Infant\": \"mua_infant_desig\",\n",
    "}\n",
    "\n",
    "designation_tables = {}\n",
    "designation_stats = {}\n",
    "\n",
    "for name in provider_names:\n",
    "    d = AHA_IT_US[AHA_IT_US[\"_mt3\"].notna()].copy()\n",
    "    desig_mask = d[flag_col[name]] == 1\n",
    "    \n",
    "    # Count by designation status and model type\n",
    "    cnt_desig = d.loc[desig_mask, \"_mt3\"].value_counts().reindex(MODEL_LABELS, fill_value=0)\n",
    "    cnt_not_desig = d.loc[~desig_mask, \"_mt3\"].value_counts().reindex(MODEL_LABELS, fill_value=0)\n",
    "    \n",
    "    # Convert to percentages of total\n",
    "    pct_desig = (cnt_desig / len(d)) * 100.0\n",
    "    pct_not_desig = (cnt_not_desig / len(d)) * 100.0\n",
    "    \n",
    "    # Create table\n",
    "    table = pd.DataFrame([pct_desig.values, pct_not_desig.values],\n",
    "                        index=[\"Designated\", \"Not Designated\"], columns=MODEL_LABELS)\n",
    "    \n",
    "    designation_tables[name] = table\n",
    "    \n",
    "    # Calculate statistics\n",
    "    n_desig = int(desig_mask.sum())\n",
    "    n_not_desig = int((~desig_mask).sum())\n",
    "    ai_desig = int((d.loc[desig_mask, \"_mt3\"] == AI_LABEL).sum())\n",
    "    ai_not_desig = int((d.loc[~desig_mask, \"_mt3\"] == AI_LABEL).sum())\n",
    "    \n",
    "    p_desig = ai_desig / n_desig if n_desig > 0 else np.nan\n",
    "    p_not_desig = ai_not_desig / n_not_desig if n_not_desig > 0 else np.nan\n",
    "    RR = p_desig / p_not_desig if p_not_desig > 0 else np.nan\n",
    "    rel_diff = 100 * (RR - 1) if np.isfinite(RR) else np.nan\n",
    "    \n",
    "    designation_stats[name] = {\n",
    "        'n_desig': n_desig, 'n_not_desig': n_not_desig,\n",
    "        'p_desig': p_desig, 'p_not_desig': p_not_desig,\n",
    "        'RR': RR, 'rel_diff': rel_diff\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display socioeconomic results\n",
    "print(\"Socioeconomic Measures (Tertiles)\")\n",
    "print(\"=\"*60)\n",
    "for name, table in socioeconomic_tables.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(table.round(1))\n",
    "    stats = socioeconomic_stats[name]\n",
    "    print(f\"High vs Low AI: {100*stats['pH']:.1f}% vs {100*stats['pL']:.1f}%\")\n",
    "    print(f\"RR = {stats['RR']:.2f} ({stats['rel']:+.0f}% relative)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display designation results\n",
    "print(\"\\nHRSA Designations (Binary)\")\n",
    "print(\"=\"*60)\n",
    "for name, table in designation_tables.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(table.round(1))\n",
    "    stats = designation_stats[name]\n",
    "    print(f\"Designated: {stats['n_desig']} hospitals ({100*stats['p_desig']:.1f}% AI)\")\n",
    "    print(f\"Not Designated: {stats['n_not_desig']} hospitals ({100*stats['p_not_desig']:.1f}% AI)\")\n",
    "    print(f\"Risk Ratio: {stats['RR']:.2f} ({stats['rel_diff']:+.0f}% relative)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
