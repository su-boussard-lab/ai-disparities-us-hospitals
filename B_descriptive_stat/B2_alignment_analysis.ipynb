{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Alignment Analysis – Need vs AI Implementation\n",
    "\n",
    "**Description**  \n",
    "This section evaluates the alignment between healthcare need and AI implementation levels across hospitals. Tertiles are computed for both variables to assess patterns of alignment. \n",
    "\n",
    "**Purpose**  \n",
    "To examine whether AI implementation correspond with areas of greatest need. \n",
    "\n",
    "**Method Summary**  \n",
    "- Rank-based tertiles were created for HPSA, MUA, ADI, SVI scores.  \n",
    "- AI implementation scores were already categorized into three ctegories (Low, Medium, High).  \n",
    "- Cross-tabulations were generated and visualized using heatmaps.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1 Load necessary libraries, functions, and pre-processed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load necessary libraries \n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_master = pd.read_csv(\"../../data/AHA_master_external_data.csv\", low_memory=False)\n",
    "AHA_IT = AHA_master[AHA_master.id_it.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Data engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all functions but only use what you need\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from calculate_scores import (\n",
    "    create_union_aipred_row, \n",
    "    calculate_base_ai_implementation_row_imputed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_IT['aipred_it_union'] = AHA_IT.apply(calculate_scores.create_union_aipred_row, axis=1)\n",
    "AHA_IT = calculate_scores.apply_ai_scores_to_dataframe(AHA_IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_division = {\n",
    "    # Division 1: New England\n",
    "    'ME': 'New England', 'NH': 'New England', 'VT': 'New England', \n",
    "    'MA': 'New England', 'RI': 'New England', 'CT': 'New England',\n",
    "    \n",
    "    # Division 2: Mid Atlantic\n",
    "    'NY': 'Mid Atlantic', 'NJ': 'Mid Atlantic', 'PA': 'Mid Atlantic',\n",
    "    \n",
    "    # Division 3: South Atlantic\n",
    "    'DE': 'South Atlantic', 'MD': 'South Atlantic', 'DC': 'South Atlantic',\n",
    "    'VA': 'South Atlantic', 'WV': 'South Atlantic', 'NC': 'South Atlantic',\n",
    "    'SC': 'South Atlantic', 'GA': 'South Atlantic', 'FL': 'South Atlantic',\n",
    "    \n",
    "    # Division 4: East North Central\n",
    "    'OH': 'East North Central', 'IN': 'East North Central', 'IL': 'East North Central',\n",
    "    'MI': 'East North Central', 'WI': 'East North Central',\n",
    "    \n",
    "    # Division 5: East South Central\n",
    "    'KY': 'East South Central', 'TN': 'East South Central', \n",
    "    'AL': 'East South Central', 'MS': 'East South Central',\n",
    "    \n",
    "    # Division 6: West North Central\n",
    "    'MN': 'West North Central', 'IA': 'West North Central', 'MO': 'West North Central',\n",
    "    'ND': 'West North Central', 'SD': 'West North Central', 'NE': 'West North Central',\n",
    "    'KS': 'West North Central',\n",
    "    \n",
    "    # Division 7: West South Central\n",
    "    'AR': 'West South Central', 'LA': 'West South Central', \n",
    "    'OK': 'West South Central', 'TX': 'West South Central',\n",
    "    \n",
    "    # Division 8: Mountain\n",
    "    'MT': 'Mountain', 'ID': 'Mountain', 'WY': 'Mountain', 'CO': 'Mountain',\n",
    "    'NM': 'Mountain', 'AZ': 'Mountain', 'UT': 'Mountain', 'NV': 'Mountain',\n",
    "    \n",
    "    # Division 9: Pacific\n",
    "    'WA': 'Pacific', 'OR': 'Pacific', 'CA': 'Pacific', \n",
    "    'AK': 'Pacific', 'HI': 'Pacific',\n",
    "    \n",
    "    # Territories\n",
    "    'PR': 'Territories', 'GU': 'Territories', 'VI': 'Territories', \n",
    "    'AS': 'Territories', 'MP': 'Territories'\n",
    "}\n",
    "division_to_region = {\n",
    "    'New England' : 'Northeast',\n",
    "    'Mid Atlantic' : 'Northeast', \n",
    "    'East North Central' : 'Midwest', \n",
    "    'West North Central' : 'Midwest', \n",
    "    'South Atlantic' : 'South', \n",
    "    'East South Central' : 'South', \n",
    "    'West South Central' : 'South', \n",
    "    'Mountain' : 'West', \n",
    "    'Pacific' : 'West'\n",
    " }\n",
    "# Add census division column to the dataframe\n",
    "AHA_IT['division'] = AHA_IT['mstate_it'].map(state_to_division)\n",
    "AHA_IT['region'] = AHA_IT['division'].map(division_to_region)\n",
    "AHA_IT_US = AHA_IT[AHA_IT['division']!='Territories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model_type mapping\n",
    "AHA_IT_US['model_type'] = AHA_IT_US['ai_base_score_imputed'].map({\n",
    "    0: 'No Models',\n",
    "    1: 'Non-AI Predictive Models', \n",
    "    2: 'AI Predictive Models'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Alignment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rank_based_tertiles(df, column_name, labels=['Low Need', 'Medium Need', 'High Need']):\n",
    "        return pd.qcut(df[column_name].rank(method='first'), 3, labels=labels)\n",
    "\n",
    "def create_standard_tertiles(df, column_name, labels=['Low Need', 'Medium Need', 'High Need']):\n",
    "        return pd.qcut(df[column_name], 3, labels=labels)\n",
    "\n",
    "def create_designation_binary(df, column_name):\n",
    "        return (df[column_name] > 0).astype(int)\n",
    "\n",
    "def create_designation_MUA(df, column_name):\n",
    "        return (df[column_name] <= 62).astype(int)\n",
    "    \n",
    "def rr_from_joint_table(tbl, ai_label=\"AI Predictive Models\", hi_label=None, lo_label=None):\n",
    "    if hi_label is None or lo_label is None:\n",
    "        idx = list(tbl.index)\n",
    "        hi_label = next((lab for lab in idx if \"high\" in str(lab).lower()), idx[-1])\n",
    "        lo_label = next((lab for lab in idx if \"low\" in str(lab).lower()), idx[0])\n",
    "    num_H = float(tbl.loc[hi_label, ai_label])\n",
    "    den_H = float(tbl.loc[hi_label, :].sum())\n",
    "    num_L = float(tbl.loc[lo_label, ai_label])\n",
    "    den_L = float(tbl.loc[lo_label, :].sum())\n",
    "    pH = num_H / den_H if den_H > 0 else np.nan\n",
    "    pL = num_L / den_L if den_L > 0 else np.nan\n",
    "    RR = np.nan if (pL == 0 or np.isnan(pL)) else pH / pL\n",
    "    rel = 100 * (RR - 1) if np.isfinite(RR) else np.nan\n",
    "    return pH, pL, RR, rel, hi_label, lo_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AI implementation categories\n",
    "AHA_IT_US['model_type'] = AHA_IT_US['ai_base_score_imputed'].map({\n",
    "    0: 'No Models',\n",
    "    1: 'Non-AI Predictive Models', \n",
    "    2: 'AI Predictive Models'\n",
    "})\n",
    "\n",
    "AHA_IT_US['model_type'] = pd.Categorical(\n",
    "    AHA_IT_US['model_type'],\n",
    "    categories=['No Models', 'Non-AI Predictive Models', 'AI Predictive Models'],\n",
    "    ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPSA/MUA measures - RANK-BASED tertiles\n",
    "hpsa_mua_measures = {\n",
    "    'primary_hpss_tertile': 'mean_primary_hpss',\n",
    "    'mental_hpss_tertile': 'mean_mental_hpss',\n",
    "    'dental_hpss_tertile': 'mean_dental_hpss',\n",
    "    'mua_score_tertile': 'mean_mua_shortage',\n",
    "    'mua_elder_tertile': 'mean_mua_elders_shortage',\n",
    "    'mua_infant_tertile': 'mean_mua_infant_shortage'\n",
    "}\n",
    "for tertile_col, score_col in hpsa_mua_measures.items():\n",
    "    AHA_IT_US[tertile_col] = create_rank_based_tertiles(AHA_IT_US, score_col)\n",
    "\n",
    "# Socioeconomic measures - STANDARD tertiles\n",
    "socio_measures = {\n",
    "    'adi_tertile': 'national_adi_median',\n",
    "    'svi_tertile': 'svi_themes_median'\n",
    "}\n",
    "    \n",
    "for tertile_col, score_col in socio_measures.items():\n",
    "    AHA_IT_US[tertile_col] = create_standard_tertiles(AHA_IT_US, score_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model type labels\n",
    "MODEL_LABELS = ['No Models', 'Non-AI Predictive Models', 'AI Predictive Models']\n",
    "AI_LABEL = 'AI Predictive Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create socioeconomic cross-tabulations\n",
    "socioeconomic_measures = {\n",
    "    'Area Deprivation Index': 'adi_tertile',\n",
    "    'Social Vulnerability Index': 'svi_tertile'\n",
    "}\n",
    "\n",
    "socioeconomic_tables = {}\n",
    "socioeconomic_stats = {}\n",
    "\n",
    "for name, column in socioeconomic_measures.items():\n",
    "    print(f\"\\nProcessing {name}...\")\n",
    "    \n",
    "    # Filter to valid model types\n",
    "    valid_data = AHA_IT_US[AHA_IT_US['model_type'].notna()].copy()\n",
    "    total_all = len(valid_data)\n",
    "    \n",
    "    # ========================================\n",
    "    # Method 1: From raw counts (for verification)\n",
    "    # ========================================\n",
    "    # Get counts for High and Low Need\n",
    "    high_mask = valid_data[column] == 'High Need'\n",
    "    low_mask = valid_data[column] == 'Low Need'\n",
    "    \n",
    "    nH = int(high_mask.sum())\n",
    "    nL = int(low_mask.sum())\n",
    "    \n",
    "    # Count AI in each group\n",
    "    aH = int((valid_data.loc[high_mask, 'model_type'] == AI_LABEL).sum())\n",
    "    aL = int((valid_data.loc[low_mask, 'model_type'] == AI_LABEL).sum())\n",
    "    \n",
    "    # Calculate proportions\n",
    "    pH_counts = aH / nH if nH > 0 else np.nan\n",
    "    pL_counts = aL / nL if nL > 0 else np.nan\n",
    "    RR_counts = pH_counts / pL_counts if (pL_counts > 0 and not np.isnan(pL_counts)) else np.nan\n",
    "    rel_counts = 100 * (RR_counts - 1) if np.isfinite(RR_counts) else np.nan\n",
    "    \n",
    "    # ========================================\n",
    "    # Method 2: From cross-tabulation table\n",
    "    # ========================================\n",
    "    # Create cross-tabulation (global %)\n",
    "    cross_tab = pd.crosstab(\n",
    "        valid_data[column], \n",
    "        valid_data['model_type'], \n",
    "        normalize=True\n",
    "    ) * 100\n",
    "    \n",
    "    # Reorder to put High Need at TOP, Low Need at BOTTOM\n",
    "    cross_tab = cross_tab.reindex(['High Need', 'Medium Need', 'Low Need'])\n",
    "    \n",
    "    # Ensure all expected columns exist, fill missing with 0\n",
    "    for col in MODEL_LABELS:\n",
    "        if col not in cross_tab.columns:\n",
    "            cross_tab[col] = 0\n",
    "    \n",
    "    # Reorder columns\n",
    "    cross_tab = cross_tab[MODEL_LABELS]\n",
    "    \n",
    "    socioeconomic_tables[name] = cross_tab\n",
    "    \n",
    "    # Calculate RR from table\n",
    "    pH_table, pL_table, RR_table, rel_table, hi_label, lo_label = rr_from_joint_table(\n",
    "        cross_tab, \n",
    "        ai_label=AI_LABEL\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # ========================================\n",
    "    # Store comprehensive statistics\n",
    "    # ========================================\n",
    "    socioeconomic_stats[name] = {\n",
    "        # Sample sizes\n",
    "        'nH': nH,  # High Need hospitals\n",
    "        'nL': nL,  # Low Need hospitals\n",
    "        'total': total_all,\n",
    "        \n",
    "        # AI counts\n",
    "        'aH': aH,  # AI in High Need\n",
    "        'aL': aL,  # AI in Low Need\n",
    "        \n",
    "        # Proportions (use table-based as primary)\n",
    "        'pH': pH_table,  # P(AI | High Need)\n",
    "        'pL': pL_table,  # P(AI | Low Need)\n",
    "        \n",
    "        # Risk Ratio\n",
    "        'RR': RR_table,\n",
    "        'rel': rel_table,  # Relative change (%)\n",
    "        \n",
    "        # Labels\n",
    "        'hi_label': hi_label,\n",
    "        'lo_label': lo_label,\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"  High Need: n={nH}, AI={aH} ({100*pH_table:.1f}%)\")\n",
    "    print(f\"  Low Need:  n={nL}, AI={aL} ({100*pL_table:.1f}%)\")\n",
    "    print(f\"  RR = {RR_table:.3f} ({rel_table:+.1f}%)\")\n",
    "\n",
    "# ========================================\n",
    "# Print summary table\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: SOCIOECONOMIC MEASURES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "summary_data = []\n",
    "for name, stats in socioeconomic_stats.items():\n",
    "    summary_data.append({\n",
    "        'Measure': name,\n",
    "        'High Need (n)': stats['nH'],\n",
    "        'Low Need (n)': stats['nL'],\n",
    "        'P(AI | High)': f\"{100*stats['pH']:.1f}%\",\n",
    "        'P(AI | Low)': f\"{100*stats['pL']:.1f}%\",\n",
    "        'Risk Ratio': f\"{stats['RR']:.3f}\",\n",
    "        'Change': f\"{stats['rel']:+.1f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*80)\n",
    "for name, stats in socioeconomic_stats.items():\n",
    "    if stats['RR'] < 1:\n",
    "        print(f\"• {name}: AI adoption is {(1-stats['RR'])*100:.1f}% LOWER in high disadvantage areas\")\n",
    "    elif stats['RR'] > 1:\n",
    "        print(f\"• {name}: AI adoption is {(stats['RR']-1)*100:.1f}% HIGHER in high disadvantage areas\")\n",
    "    else:\n",
    "        print(f\"• {name}: AI adoption is EQUAL across disadvantage levels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# HRSA DESIGNATION MEASURES\n",
    "# ========================================\n",
    "\n",
    "# Define designation measures\n",
    "designation_measures = {\n",
    "    'Primary HPSA': 'primary_hpsa_desig',\n",
    "    'Mental HPSA': 'mental_hpsa_desig',\n",
    "    'Dental HPSA': 'dental_hpsa_desig',\n",
    "    'MUA Overall': 'mua_overall_desig'\n",
    "}\n",
    "\n",
    "# Create designation flags if they don't exist\n",
    "print(\"Creating designation flags...\")\n",
    "AHA_IT_US['primary_hpsa_desig'] = (AHA_IT_US['mean_primary_hpss'] > 0).astype(int)\n",
    "AHA_IT_US['mental_hpsa_desig'] = (AHA_IT_US['mean_mental_hpss'] > 0).astype(int)\n",
    "AHA_IT_US['dental_hpsa_desig'] = (AHA_IT_US['mean_dental_hpss'] > 0).astype(int)\n",
    "AHA_IT_US['mua_overall_desig'] = (AHA_IT_US['mean_mua_score'] <= 62).astype(int)\n",
    "\n",
    "designation_tables = {}\n",
    "designation_stats = {}\n",
    "\n",
    "for name, column in designation_measures.items():\n",
    "    print(f\"\\nProcessing {name}...\")\n",
    "    \n",
    "    # Filter to valid model types\n",
    "    valid_data = AHA_IT_US[AHA_IT_US['model_type'].notna()].copy()\n",
    "    total_all = len(valid_data)\n",
    "    \n",
    "    # ========================================\n",
    "    # Method 1: From raw counts (for verification)\n",
    "    # ========================================\n",
    "    # Get counts for Designated and Not Designated\n",
    "    desig_mask = valid_data[column] == 1\n",
    "    not_desig_mask = valid_data[column] == 0\n",
    "    \n",
    "    nD = int(desig_mask.sum())\n",
    "    nN = int(not_desig_mask.sum())\n",
    "    \n",
    "    # Count AI in each group\n",
    "    aD = int((valid_data.loc[desig_mask, 'model_type'] == AI_LABEL).sum())\n",
    "    aN = int((valid_data.loc[not_desig_mask, 'model_type'] == AI_LABEL).sum())\n",
    "    \n",
    "    # Calculate proportions\n",
    "    pD_counts = aD / nD if nD > 0 else np.nan\n",
    "    pN_counts = aN / nN if nN > 0 else np.nan\n",
    "    RR_counts = pD_counts / pN_counts if (pN_counts > 0 and not np.isnan(pN_counts)) else np.nan\n",
    "    rel_counts = 100 * (RR_counts - 1) if np.isfinite(RR_counts) else np.nan\n",
    "    \n",
    "    # ========================================\n",
    "    # Method 2: From cross-tabulation table\n",
    "    # ========================================\n",
    "    # Count by designation status and model type\n",
    "    cnt_D = valid_data.loc[desig_mask, 'model_type'].value_counts().reindex(MODEL_LABELS, fill_value=0)\n",
    "    cnt_N = valid_data.loc[not_desig_mask, 'model_type'].value_counts().reindex(MODEL_LABELS, fill_value=0)\n",
    "    \n",
    "    # Convert to GLOBAL % (all cells sum to 100%)\n",
    "    pct_D = cnt_D / total_all * 100.0\n",
    "    pct_N = cnt_N / total_all * 100.0\n",
    "    \n",
    "    # Create table\n",
    "    cross_tab = pd.DataFrame([pct_D.values, pct_N.values],\n",
    "                             index=[\"Designated\", \"Not Designated\"],\n",
    "                             columns=MODEL_LABELS)\n",
    "    \n",
    "    designation_tables[name] = cross_tab\n",
    "    \n",
    "    # Calculate RR from table using conditional probabilities\n",
    "    # P(AI | Designated) = P(Designated, AI) / P(Designated)\n",
    "    # P(AI | Not Designated) = P(Not Designated, AI) / P(Not Designated)\n",
    "    \n",
    "    ai_pct_D = cross_tab.loc[\"Designated\", AI_LABEL]\n",
    "    ai_pct_N = cross_tab.loc[\"Not Designated\", AI_LABEL]\n",
    "    \n",
    "    all_pct_D = cross_tab.loc[\"Designated\", :].sum()\n",
    "    all_pct_N = cross_tab.loc[\"Not Designated\", :].sum()\n",
    "    \n",
    "    pD_table = ai_pct_D / all_pct_D if all_pct_D > 0 else np.nan\n",
    "    pN_table = ai_pct_N / all_pct_N if all_pct_N > 0 else np.nan\n",
    "    RR_table = pD_table / pN_table if (pN_table > 0 and not np.isnan(pN_table)) else np.nan\n",
    "    rel_table = 100 * (RR_table - 1) if np.isfinite(RR_table) else np.nan\n",
    "    \n",
    "    # ========================================\n",
    "    # Store comprehensive statistics\n",
    "    # ========================================\n",
    "    designation_stats[name] = {\n",
    "        # Sample sizes\n",
    "        'nD': nD,  # Designated hospitals\n",
    "        'nN': nN,  # Not designated hospitals\n",
    "        'total': total_all,\n",
    "        \n",
    "        # AI counts\n",
    "        'aD': aD,  # AI in Designated\n",
    "        'aN': aN,  # AI in Not Designated\n",
    "        \n",
    "        # Proportions (use table-based as primary)\n",
    "        'pD': pD_table,  # P(AI | Designated)\n",
    "        'pN': pN_table,  # P(AI | Not Designated)\n",
    "        \n",
    "        # Risk Ratio\n",
    "        'RR': RR_table,\n",
    "        'rel': rel_table,  # Relative change (%)\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"  Designated:     n={nD}, AI={aD} ({100*pD_table:.1f}%)\")\n",
    "    print(f\"  Not Designated: n={nN}, AI={aN} ({100*pN_table:.1f}%)\")\n",
    "    print(f\"  RR = {RR_table:.3f} ({rel_table:+.1f}%)\")\n",
    "\n",
    "# ========================================\n",
    "# Print summary table for designations\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: HRSA DESIGNATION MEASURES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "summary_data = []\n",
    "for name, stats in designation_stats.items():\n",
    "    summary_data.append({\n",
    "        'Designation': name,\n",
    "        'Designated (n)': stats['nD'],\n",
    "        'Not Designated (n)': stats['nN'],\n",
    "        'P(AI | Designated)': f\"{100*stats['pD']:.1f}%\",\n",
    "        'P(AI | Not Designated)': f\"{100*stats['pN']:.1f}%\",\n",
    "        'Risk Ratio': f\"{stats['RR']:.3f}\",\n",
    "        'Change': f\"{stats['rel']:+.1f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*80)\n",
    "for name, stats in designation_stats.items():\n",
    "    if stats['RR'] < 1:\n",
    "        print(f\"• {name}: AI adoption is {(1-stats['RR'])*100:.1f}% LOWER in designated areas\")\n",
    "    elif stats['RR'] > 1:\n",
    "        print(f\"• {name}: AI adoption is {(stats['RR']-1)*100:.1f}% HIGHER in designated areas\")\n",
    "    else:\n",
    "        print(f\"• {name}: AI adoption is EQUAL in designated vs not designated areas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# DISPLAY ALL RESULTS\n",
    "# ========================================\n",
    "\n",
    "# Display socioeconomic results\n",
    "print(\"Socioeconomic Measures (Tertiles)\")\n",
    "print(\"=\"*60)\n",
    "for name, table in socioeconomic_tables.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(table.round(1))\n",
    "    stats = socioeconomic_stats[name]\n",
    "    print(f\"High vs Low AI: {100*stats['pH']:.1f}% vs {100*stats['pL']:.1f}%\")\n",
    "    print(f\"RR = {stats['RR']:.2f} ({stats['rel']:+.0f}% relative)\")\n",
    "\n",
    "# Display designation results\n",
    "print(\"\\n\\nHRSA Designation Measures (Binary)\")\n",
    "print(\"=\"*60)\n",
    "for name, table in designation_tables.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(table.round(1))\n",
    "    stats = designation_stats[name]\n",
    "    print(f\"Designated vs Not Designated AI: {100*stats['pD']:.1f}% vs {100*stats['pN']:.1f}%\")\n",
    "    print(f\"RR = {stats['RR']:.2f} ({stats['rel']:+.0f}% relative)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
