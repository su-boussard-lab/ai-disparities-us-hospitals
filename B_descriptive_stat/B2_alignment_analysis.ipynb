{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2. Alignment Analysis – Need vs AI Implementation\n",
    "\n",
    "**Description**  \n",
    "This section evaluates the alignment between healthcare need and AI implementation levels across hospitals. Tertiles are computed for both variables to assess patterns of alignment. \n",
    "\n",
    "**Purpose**  \n",
    "To examine whether AI implementation correspond with areas of greatest need. \n",
    "\n",
    "**Method Summary**  \n",
    "- Rank-based tertiles were created for HPSA, MUA, ADI, SVI scores.  \n",
    "- AI implementation scores were already categorized into three ctegories (Low, Medium, High).  \n",
    "- Cross-tabulations were generated and visualized using heatmaps.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1 Load necessary libraries, functions, and pre-processed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load necessary libraries \n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_exposures = [\"ai_base_score\",\n",
    "\"ai_base_breadth_score\",\n",
    "\"ai_base_dev_score\",\n",
    "\"ai_base_eval_score_2023\",\n",
    "\"ai_base_eval_score_2024\",\n",
    "\"llm_readiness_score\", \n",
    "\"ai_base_score_imputed\",\n",
    "\"ai_base_breadth_score_imputed\",\n",
    "\"ai_base_dev_score_imputed\",\n",
    "\"ai_base_eval_score_2023_imputed\",\n",
    "\"ai_base_eval_score_2024_imputed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_master = pd.read_csv(\"./data/AHA_master_external_data.csv\", low_memory=False)\n",
    "AHA_IT = AHA_master[AHA_master.id_it.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Data engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_master2 = calculate_ai_scores.apply_ai_scores_to_dataframe(AHA_IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_IT_US = AHA_master2[AHA_master2['division']!='Territories']\n",
    "AHA_IT_US.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric AI implementation scores to categorical labels\n",
    "AHA_IT_US['AI_implementation_tertile'] = AHA_IT_US['ai_base_score'].map({\n",
    "    0: 'Low',\n",
    "    1: 'Medium',\n",
    "    2: 'High'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Alignment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# TABLE 1: Tertile Ranges for Need Indicators\n",
    "# =========================\n",
    "\n",
    "def get_tertile_ranges(series, measure_name):\n",
    "    \"\"\"Get the actual value ranges for tertiles\"\"\"\n",
    "    try:\n",
    "        # Create tertiles\n",
    "        tertiles = pd.qcut(series, 3, labels=['Low Need', 'Medium Need', 'High Need'])\n",
    "        \n",
    "        # Get the cut points\n",
    "        _, bins = pd.qcut(series, 3, retbins=True)\n",
    "        \n",
    "        # Get actual min/max within each tertile\n",
    "        low_values = series[tertiles == 'Low Need']\n",
    "        med_values = series[tertiles == 'Medium Need'] \n",
    "        high_values = series[tertiles == 'High Need']\n",
    "        \n",
    "        return {\n",
    "            'Measure': measure_name,\n",
    "            'Low Need Range': f\"{low_values.min():.2f} - {low_values.max():.2f}\",\n",
    "            'Medium Need Range': f\"{med_values.min():.2f} - {med_values.max():.2f}\",\n",
    "            'High Need Range': f\"{high_values.min():.2f} - {high_values.max():.2f}\",\n",
    "            'Low Need N': len(low_values),\n",
    "            'Medium Need N': len(med_values),\n",
    "            'High Need N': len(high_values),\n",
    "            'Cutpoint 1': f\"{bins[1]:.2f}\",\n",
    "            'Cutpoint 2': f\"{bins[2]:.2f}\"\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Socioeconomic measures (using standard tertiles)\n",
    "socio_measures = {\n",
    "    'Area Deprivation Index': 'national_adi_median',\n",
    "    'Social Vulnerability Index': 'svi_themes_median',\n",
    "    'SVI Theme 1 (Socioeconomic)': 'svi_theme1_median',\n",
    "    'SVI Theme 2 (Household)': 'svi_theme2_median',\n",
    "    'SVI Theme 3 (Minority)': 'svi_theme3_median',\n",
    "    'SVI Theme 4 (Housing/Transport)': 'svi_theme4_median'\n",
    "}\n",
    "\n",
    "tertile_ranges = []\n",
    "for measure_name, column in socio_measures.items():\n",
    "    if column in AHA_master2.columns:\n",
    "        range_info = get_tertile_ranges(AHA_master2[column], measure_name)\n",
    "        if range_info:\n",
    "            tertile_ranges.append(range_info)\n",
    "\n",
    "tertile_df = pd.DataFrame(tertile_ranges)\n",
    "\n",
    "# =========================\n",
    "# TABLE 2: Designation Distribution for HPSA/MUA Indicators\n",
    "# =========================\n",
    "\n",
    "def get_designation_distribution(series, measure_name):\n",
    "    \"\"\"Get distribution of designation status (score > 0 vs score = 0)\"\"\"\n",
    "    try:\n",
    "        designated = (series > 0).sum()\n",
    "        not_designated = (series == 0).sum()\n",
    "        total = len(series)\n",
    "        \n",
    "        # Get ranges for designated areas\n",
    "        positive_values = series[series > 0]\n",
    "        \n",
    "        return {\n",
    "            'Measure': measure_name,\n",
    "            'Not Designated (Score = 0)': f\"{not_designated:,} ({100*not_designated/total:.1f}%)\",\n",
    "            'Designated (Score > 0)': f\"{designated:,} ({100*designated/total:.1f}%)\",\n",
    "            'Designated Score Range': f\"{positive_values.min():.2f} - {positive_values.max():.2f}\" if len(positive_values) > 0 else \"N/A\",\n",
    "            'Designated Score Median': f\"{positive_values.median():.2f}\" if len(positive_values) > 0 else \"N/A\",\n",
    "            'Total Hospitals': f\"{total:,}\"\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# HPSA/MUA measures (using binary designation)\n",
    "provider_measures = {\n",
    "    'Primary HPSA Score': 'mean_primary_hpss',\n",
    "    'Mental HPSA Score': 'mean_mental_hpss',\n",
    "    'Dental HPSA Score': 'mean_dental_hpss',\n",
    "    'MUA Overall Score': 'mean_mua_score',\n",
    "    'MUA Elder Score': 'mean_mua_elders_score',\n",
    "    'MUA Infant Score': 'mean_mua_infant_score'\n",
    "}\n",
    "\n",
    "designation_distributions = []\n",
    "for measure_name, column in provider_measures.items():\n",
    "    if column in AHA_master2.columns:\n",
    "        dist_info = get_designation_distribution(AHA_master2[column], measure_name)\n",
    "        if dist_info:\n",
    "            designation_distributions.append(dist_info)\n",
    "\n",
    "designation_df = pd.DataFrame(designation_distributions)\n",
    "\n",
    "# =========================\n",
    "# RANK-BASED TERTILE RANGES FOR HPSA/MUA MEASURES\n",
    "# =========================\n",
    "\n",
    "def get_rank_based_tertile_ranges(series, measure_name):\n",
    "    \"\"\"Get the actual value ranges for rank-based tertiles\"\"\"\n",
    "    try:\n",
    "        # Create rank-based tertiles (handles duplicate values)\n",
    "        ranked_series = series.rank(method='first')\n",
    "        tertiles = pd.qcut(ranked_series, 3, labels=['Low Need', 'Medium Need', 'High Need'])\n",
    "        \n",
    "        # Get actual values within each tertile group\n",
    "        low_mask = tertiles == 'Low Need'\n",
    "        med_mask = tertiles == 'Medium Need'\n",
    "        high_mask = tertiles == 'High Need'\n",
    "        \n",
    "        low_values = series[low_mask]\n",
    "        med_values = series[med_mask]\n",
    "        high_values = series[high_mask]\n",
    "        \n",
    "        # Get the rank cutpoints\n",
    "        _, rank_bins = pd.qcut(ranked_series, 3, retbins=True)\n",
    "        \n",
    "        # Find the actual score values at the rank cutpoints\n",
    "        sorted_values = series.sort_values()\n",
    "        cutpoint1_idx = int(rank_bins[1]) - 1  # Convert to 0-based index\n",
    "        cutpoint2_idx = int(rank_bins[2]) - 1\n",
    "        \n",
    "        # Handle edge cases\n",
    "        cutpoint1_value = sorted_values.iloc[min(cutpoint1_idx, len(sorted_values)-1)]\n",
    "        cutpoint2_value = sorted_values.iloc[min(cutpoint2_idx, len(sorted_values)-1)]\n",
    "        \n",
    "        return {\n",
    "            'Measure': measure_name,\n",
    "            'Low Need Range': f\"{low_values.min():.3f} - {low_values.max():.3f}\",\n",
    "            'Medium Need Range': f\"{med_values.min():.3f} - {med_values.max():.3f}\",\n",
    "            'High Need Range': f\"{high_values.min():.3f} - {high_values.max():.3f}\",\n",
    "            'Low Need N': len(low_values),\n",
    "            'Medium Need N': len(med_values),\n",
    "            'High Need N': len(high_values),\n",
    "            'Score Cutpoint 1': f\"{cutpoint1_value:.3f}\",\n",
    "            'Score Cutpoint 2': f\"{cutpoint2_value:.3f}\",\n",
    "            'Rank Cutpoint 1': f\"{rank_bins[1]:.0f}\",\n",
    "            'Rank Cutpoint 2': f\"{rank_bins[2]:.0f}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {measure_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# HPSA/MUA measures using rank-based tertiles\n",
    "hpsa_mua_measures = {\n",
    "    'Primary HPSA Score': 'mean_primary_hpss',\n",
    "    'Mental HPSA Score': 'mean_mental_hpss',\n",
    "    'Dental HPSA Score': 'mean_dental_hpss',\n",
    "    'MUA Overall Score': 'mean_mua_score',\n",
    "    'MUA Elder Score': 'mean_mua_elders_score',\n",
    "    'MUA Infant Score': 'mean_mua_infant_score'\n",
    "}\n",
    "\n",
    "rank_tertiles = []\n",
    "for measure_name, column in hpsa_mua_measures.items():\n",
    "    if column in AHA_master2.columns:\n",
    "        range_info = get_rank_based_tertile_ranges(AHA_master2[column], measure_name)\n",
    "        if range_info:\n",
    "            rank_tertiles.append(range_info)\n",
    "\n",
    "rank_tertiles_df = pd.DataFrame(rank_tertiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model type categorization\n",
    "AHA_master2['model_type'] = AHA_master2['ai_base_score_imputed'].map({\n",
    "    0: 'No Models',\n",
    "    1: 'Non-AI Predictive Models', \n",
    "    2: 'AI Predictive Models'\n",
    "})\n",
    "\n",
    "AHA_master2['model_type'] = pd.Categorical(\n",
    "    AHA_master2['model_type'],\n",
    "    categories=['No Models', 'Non-AI Predictive Models', 'AI Predictive Models'],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# SEPARATE the need measures into two groups as requested by reviewer\n",
    "provider_shortage_measures = {\n",
    "    'Primary HPSA': 'primary_hpss_tertile_new',\n",
    "    'Mental HPSA': 'mental_hpss_tertile_new', \n",
    "    'Dental HPSA': 'dental_hpss_tertile_new',\n",
    "    'MUA Overall': 'mua_score_tertile_new'\n",
    "}\n",
    "\n",
    "socioeconomic_measures = {\n",
    "    'Area Deprivation Index': 'adi_tertile',\n",
    "    'Social Vulnerability Index Overall': 'svi_tertile',\n",
    "    'Social Vulnerability Index Theme 1': 'svi1_tertile',\n",
    "    'Social Vulnerability Index Theme 2': 'svi2_tertile',\n",
    "    'Social Vulnerability Index Theme 3': 'svi3_tertile',\n",
    "    'Social Vulnerability Index Theme 4': 'svi4_tertile'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tables(need_measures_dict):\n",
    "    cross_tabs = {}\n",
    "\n",
    "    for name, column in need_measures_dict.items():\n",
    "        # Create cross-tabulation\n",
    "        cross_tabs[name] = pd.crosstab(\n",
    "            AHA_master2[column], \n",
    "            AHA_master2['model_type'], \n",
    "            normalize=True\n",
    "        ) * 100\n",
    "        \n",
    "        # Reorder to put High Need at TOP, Low Need at BOTTOM\n",
    "        cross_tabs[name] = cross_tabs[name].reindex(['High Need', 'Medium Need', 'Low Need'])\n",
    "    \n",
    "    return cross_tabs\n",
    "\n",
    "# Calculate metrics for both groups\n",
    "provider_tabs = calculate_tables(provider_shortage_measures)\n",
    "socioeconomic_tabs = calculate_tables(socioeconomic_measures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_LABEL = \"AI Predictive Models\"\n",
    "MODEL_LABELS = [\"No Models\", \"Non-AI Predictive Models\", \"AI Predictive Models\"]\n",
    "\n",
    "\n",
    "def pick_high_low_rows(tbl_index):\n",
    "    \"\"\"Return the row labels for High and Low based on text; fallbacks to last/first.\"\"\"\n",
    "    idx = list(tbl_index)\n",
    "    hi = next((lab for lab in idx if \"high\" in str(lab).lower()), idx[-1])\n",
    "    lo = next((lab for lab in idx if \"low\"  in str(lab).lower()), idx[0])\n",
    "    return hi, lo\n",
    "\n",
    "def rr_from_joint_table(tbl, ai_label=AI_LABEL, hi_label=None, lo_label=None):\n",
    "    \"\"\"\n",
    "    tbl entries are global % of total (not row-normalized).\n",
    "    RR = P(AI|High)/P(AI|Low) = [p(High,AI)/p(High,*)] / [p(Low,AI)/p(Low,*)]\n",
    "    \"\"\"\n",
    "    if hi_label is None or lo_label is None:\n",
    "        hi_label, lo_label = pick_high_low_rows(tbl.index)\n",
    "    num_H = float(tbl.loc[hi_label, ai_label])\n",
    "    den_H = float(tbl.loc[hi_label, :].sum())\n",
    "    num_L = float(tbl.loc[lo_label, ai_label])\n",
    "    den_L = float(tbl.loc[lo_label, :].sum())\n",
    "    pH = num_H / den_H if den_H > 0 else np.nan\n",
    "    pL = num_L / den_L if den_L > 0 else np.nan\n",
    "    RR = np.nan if (pL == 0 or np.isnan(pL)) else pH / pL\n",
    "    rel = 100 * (RR - 1) if np.isfinite(RR) else np.nan\n",
    "    return pH, pL, RR, rel, hi_label, lo_label\n",
    "\n",
    "# =========================\n",
    "# Figure 1: Socioeconomic Disadvantage (SVI/ADI) \n",
    "# These tables already represent % of total, so all 9 cells sum to ~100.\n",
    "# RR is derived correctly from those tables (High vs Low).\n",
    "# =========================\n",
    "\n",
    "# Names you already use\n",
    "socioeconomic_names = [\n",
    "    \"Area Deprivation Index\",\n",
    "    \"Social Vulnerability Index Overall\",\n",
    "    \"Social Vulnerability Index Theme 1\",\n",
    "    \"Social Vulnerability Index Theme 2\",\n",
    "    \"Social Vulnerability Index Theme 3\",\n",
    "    \"Social Vulnerability Index Theme 4\",\n",
    "]\n",
    "\n",
    "# Color scale across all socioeconomic tabs\n",
    "all_socio_vals = []\n",
    "for t in socioeconomic_tabs.values():\n",
    "    all_socio_vals.extend(t.values.flatten())\n",
    "vmin_socio = float(pd.Series(all_socio_vals).dropna().min())\n",
    "vmax_socio = float(pd.Series(all_socio_vals).dropna().max())\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "for i, name in enumerate(socioeconomic_names):\n",
    "    r, c = divmod(i, 3)\n",
    "    ax = axes[r, c]\n",
    "    tbl = socioeconomic_tabs[name]  # rows = High/Medium/Low; cols = 3 model buckets; entries are global %\n",
    "\n",
    "    sns.heatmap(tbl, annot=True, fmt=\".1f\", cmap=\"YlOrRd\", ax=ax,\n",
    "                vmin=vmin_socio, vmax=vmax_socio, cbar=False,\n",
    "                annot_kws={\"size\": 12, \"weight\": \"bold\"},\n",
    "                linewidths=1, linecolor=\"white\")\n",
    "\n",
    "    pH, pL, RR, rel, hi_label, lo_label = rr_from_joint_table(tbl, ai_label=AI_LABEL)\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{name}\\nHigh vs Low: {100*pH:.1f}% vs {100*pL:.1f}%\\nRR = {RR:.2f} ({rel:+.0f}% rel.)\",\n",
    "        fontsize=12, fontweight=\"bold\", pad=28\n",
    "    )\n",
    "    ax.set_xlabel(\"\", fontsize=11)\n",
    "    ax.set_ylabel(\"\", fontsize=11)\n",
    "    ax.set_xticklabels([\"none\", \"non-AI\", \"AI\"], fontsize=10)\n",
    "    ax.set_yticklabels([str(x) for x in tbl.index], fontsize=10, rotation=0)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 0.92])\n",
    "cax = fig.add_axes([0.92, 0.18, 0.02, 0.66])\n",
    "sm = plt.cm.ScalarMappable(cmap=\"YlOrRd\"); sm.set_array([vmin_socio, vmax_socio])\n",
    "cb = plt.colorbar(sm, cax=cax)\n",
    "cb.set_label(\"Percentage (%)\", fontsize=12, fontweight=\"bold\", rotation=270, labelpad=18)\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "\n",
    "plt.suptitle(\"Socioeconomic Disadvantage vs AI Implementation (Global %; RR from P(AI|group))\",\n",
    "             fontsize=16, y=0.99)\n",
    "fig.savefig(\"figures/socioeconomic_analysis_global_rr.pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# =========================\n",
    "# FIGURE 2: HRSA Designations (binary) – build GLOBAL-% table so 6 cells sum to 100\n",
    "# and compute crude RR from the same 3-bucket mapping.\n",
    "# =========================\n",
    "\n",
    "# Build designation flags (your variables)\n",
    "AHA_master2[\"primary_hpsa_desig\"] = (AHA_master2[\"mean_primary_hpss\"] > 0).astype(int)\n",
    "AHA_master2[\"mental_hpsa_desig\"]  = (AHA_master2[\"mean_mental_hpss\"]  > 0).astype(int)\n",
    "AHA_master2[\"dental_hpsa_desig\"]  = (AHA_master2[\"mean_dental_hpss\"]  > 0).astype(int)\n",
    "AHA_master2[\"mua_overall_desig\"]  = (AHA_master2[\"mean_mua_score\"]    > 0).astype(int)\n",
    "AHA_master2[\"mua_elder_desig\"]    = (AHA_master2[\"mean_mua_elders_score\"]  > 0).astype(int)\n",
    "AHA_master2[\"mua_infant_desig\"]   = (AHA_master2[\"mean_mua_infant_score\"]  > 0).astype(int)\n",
    "\n",
    "provider_names = [\"Primary HPSA\",\"Mental HPSA\",\"Dental HPSA\",\"MUA Overall\",\"MUA Elder\",\"MUA Infant\"]\n",
    "flag_col = {\n",
    "    \"Primary HPSA\": \"primary_hpsa_desig\",\n",
    "    \"Mental HPSA\":  \"mental_hpsa_desig\",\n",
    "    \"Dental HPSA\":  \"dental_hpsa_desig\",\n",
    "    \"MUA Overall\":  \"mua_overall_desig\",\n",
    "    \"MUA Elder\":    \"mua_elder_desig\",\n",
    "    \"MUA Infant\":   \"mua_infant_desig\",\n",
    "}\n",
    "\n",
    "def hpsa_global_table_and_counts(df, desig_flag):\n",
    "    \"\"\"Return (2x3) table of global % and the n for each group, using the 3-bucket mapping.\"\"\"\n",
    "    d = df[df[\"_mt3\"].notna()].copy()\n",
    "    total_all = len(d)\n",
    "\n",
    "    # counts by group x model\n",
    "    desig_mask = d[desig_flag] == 1\n",
    "    cnt_D = d.loc[desig_mask, \"_mt3\"].value_counts().reindex(MODEL_LABELS, fill_value=0)\n",
    "    cnt_N = d.loc[~desig_mask, \"_mt3\"].value_counts().reindex(MODEL_LABELS, fill_value=0)\n",
    "\n",
    "    # convert to GLOBAL %\n",
    "    pct_D = cnt_D / total_all * 100.0\n",
    "    pct_N = cnt_N / total_all * 100.0\n",
    "\n",
    "    tbl = pd.DataFrame([pct_D.values, pct_N.values],\n",
    "                       index=[\"Designated\",\"Not designated\"],\n",
    "                       columns=MODEL_LABELS)\n",
    "\n",
    "    # n for each group (for RR denominator and for title)\n",
    "    nD = int(desig_mask.sum())\n",
    "    nN = int((~desig_mask).sum())\n",
    "    # AI counts for RR\n",
    "    aD = int((d.loc[desig_mask, \"_mt3\"] == AI_LABEL).sum())\n",
    "    aN = int((d.loc[~desig_mask, \"_mt3\"] == AI_LABEL).sum())\n",
    "\n",
    "    # crude RR from counts\n",
    "    pD = aD / nD if nD > 0 else np.nan\n",
    "    pN = aN / nN if nN > 0 else np.nan\n",
    "    RR = np.nan if (pN == 0 or np.isnan(pN)) else pD / pN\n",
    "    rel = 100 * (RR - 1) if np.isfinite(RR) else np.nan\n",
    "\n",
    "    return tbl, nD, nN, pD, pN, RR, rel\n",
    "\n",
    "# Build all provider tables first (for color scale)\n",
    "provider_tabs = {}\n",
    "provider_stats = {}\n",
    "all_vals = []\n",
    "for nm in provider_names:\n",
    "    tbl, nD, nN, pD, pN, RR, rel = hpsa_global_table_and_counts(AHA_master2, flag_col[nm])\n",
    "    provider_tabs[nm] = tbl\n",
    "    provider_stats[nm] = dict(nD=nD, nN=nN, pD=pD, pN=pN, RR=RR, rel=rel)\n",
    "    all_vals.extend(tbl.values.flatten())\n",
    "\n",
    "# Combine with socioeconomic scale if you want one shared colorbar across figs:\n",
    "# all_vals += all_socio_vals\n",
    "vmin_hrsa = float(pd.Series(all_vals).dropna().min())\n",
    "vmax_hrsa = float(pd.Series(all_vals).dropna().max())\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "for i, nm in enumerate(provider_names):\n",
    "    r, c = divmod(i, 3)\n",
    "    ax = axes[r, c]\n",
    "    tbl = provider_tabs[nm]\n",
    "    st  = provider_stats[nm]\n",
    "\n",
    "    sns.heatmap(tbl, annot=True, fmt=\".1f\", cmap=\"copper_r\", ax=ax,\n",
    "                vmin=vmin_hrsa, vmax=vmax_hrsa, cbar=False,\n",
    "                annot_kws={\"size\": 12, \"weight\": \"bold\"},\n",
    "                linewidths=1, linecolor=\"white\")\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{nm}\\nDesignated vs Not: {100*st['pD']:.1f}% (n={st['nD']}) vs \"\n",
    "        f\"{100*st['pN']:.1f}% (n={st['nN']})\\nRR = {st['RR']:.2f} ({st['rel']:+.0f}% rel.)\",\n",
    "        fontsize=12, fontweight=\"bold\", pad=28\n",
    "    )\n",
    "    ax.set_xlabel(\"Model Type\", fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_xticklabels([\"No\\nModels\",\"Non-AI\\nModels\",\"AI\\nModels\"], fontsize=10)\n",
    "    ax.set_ylabel(\"Designation Status\", fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_yticklabels([\"Designated\",\"Not designated\"], fontsize=10, rotation=0)\n",
    "\n",
    "plt.suptitle(\"HRSA Designations vs AI Implementation (Cells are GLOBAL %; RR is crude)\",\n",
    "             fontsize=16, y=0.99)\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 0.92])\n",
    "cax = fig.add_axes([0.92, 0.18, 0.02, 0.66])\n",
    "sm = plt.cm.ScalarMappable(cmap=\"copper_r\"); sm.set_array([vmin_hrsa, vmax_hrsa])\n",
    "cb = plt.colorbar(sm, cax=cax)\n",
    "cb.set_label(\"Percentage (%)\", fontsize=12, fontweight=\"bold\", rotation=270, labelpad=18)\n",
    "cb.ax.tick_params(labelsize=10)\n",
    "\n",
    "fig.savefig(\"figures/hrsa_designations_global_rr.pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
