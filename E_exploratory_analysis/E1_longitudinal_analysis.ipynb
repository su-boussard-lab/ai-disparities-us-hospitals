{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Exploratory Longitudinal Health Outcome Analysis\n",
    "\n",
    "**Description**  \n",
    "This section investigates whether the level of AI implementation affects changes in hospital care quality over time.  \n",
    "We take a longitudinal approach because the exact timing of AI implementation is not specified in the AHA dataset.  \n",
    "\n",
    "This notebook includes code for:\n",
    "- Assessing missing values using the 2025 3Q hospital quality dataset,\n",
    "- Selecting quality metrics for longitudinal analysis,\n",
    "- Performing LASSO-based variable selection for each outcome,\n",
    "- Fitting linear mixed models (LMMs).\n",
    "\n",
    "**Purpose**  \n",
    "To examine whether hospital-level AI implementation is associated with changes in care quality over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 load necessary libraries, functions, and preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from libpysal.weights import KNN, DistanceBand\n",
    "from esda.moran import Moran\n",
    "from spreg import ML_Error, OLS\n",
    "from shapely.geometry import Point\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from calculate_scores import create_union_aipred_row, apply_ai_scores_to_dataframe\n",
    "\n",
    "# Load data\n",
    "AHA_master = pd.read_csv('../../data/AHA_master_external_data.csv', low_memory=False)\n",
    "\n",
    "# Create aipred_it_union separately (your choice, works perfectly)\n",
    "AHA_master['aipred_it_union'] = AHA_master.apply(create_union_aipred_row, axis=1)\n",
    "\n",
    "# Use the apply function for all other scores\n",
    "AHA_master2 = apply_ai_scores_to_dataframe(AHA_master)\n",
    "AHA_IT = AHA_master2[AHA_master2['id_it'].notna()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Data engineering  \n",
    "\n",
    "These hospital characteristics were selected based on investigator consensus, and we used LASSO regression analysis to explore and identify additional variables that predict AI/ML implementation and reflect hospital resource levels.\n",
    "\n",
    "- **rural_urban_type** : collected from AHA survey. categorized into {1: rural, 2: micro, 3: metro} based on the location of the hospital ('CBSATYPE')\n",
    "- **system member** : hospital belonging to a corporate body that owns or manage health provider facilities or health-related subsidiaries. ('MHSMEMB')\n",
    "- **delivery_system** : delivery system identified using existing theory and AHA Annual Survey data {1: Centralized Health System, 2: Centralized Physician/Insurance Health System, 3: Moderately Centralized Health System, 4: Decentralized Health System, 5: Independent Hospital System, 6/Missing: Insufficient data to determine} ('CLUSTER')\n",
    "- **community_hospital** : all nonfederal, short-term general, and special hospitals whose facilities and services are available to the public {0: No, 1: Yes}('CHC')\n",
    "- **subsidary_hospital** : Hospital itself operates subsidiary corporation {0: No, 1: Yes} ('SUBS')\n",
    "- **frontline_hospital** : Frontline facility {0: No, 1: Yes} ('FRTLN')\n",
    "- **joint_commission_accreditaion** : Accreditation by joint commision {0: No, 1: Yes} ('MAPP1')\n",
    "- **center_quality** : Center for Improvement in Healthcare Quality Accreditation {0: No, 1: Yes} ('MAPP22')\n",
    "- **teaching_hospital** : major teaching hospital ('MAPP8'), minor teaching hospital ('MAPP3' or 'MAPP5')\n",
    "- **critical_access** critical access hospital {0: No, 1: Yes} ('MAPP18')\n",
    "- **rural_referral** : rural referral center {0: No, 1: Yes} ('MAPP19')\n",
    "- **ownership_type** : type of organization responsible for establishing policy concerning overall operation {government_federal, government_nonfederal, nonprofit, forprofit, other} ('CNTRL')\n",
    "- **bedsize** : bed-size category, ordinal variable ('BSC')\n",
    "- **medicare_ipd_percentage** : medicare inpatient days / total inpatient days. Proxy variable to reflect the proportion of medicare patient \n",
    "- **medicaid_ipd_percentage** : medicaid inpatient days / total inpatient days. Proxy variable to reflect the proportion of medicaid patients \n",
    "- **core_index** : summary measure to track the interoperability of US hospitals (https://doi.org/10.1093/jamia/ocae289)\n",
    "- **friction_index** : summary measures to track the barrier or difficulty in interoperability between hospitals (https://doi.org/10.1093/jamia/ocae289)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rural_urban_type\n",
    "# Continue with CBSA type and other variables\n",
    "AHA_IT['rural_urban_type'] = AHA_IT['cbsatype_as'].map({\n",
    "    'Rural': 1,      # Rural = 1 (lowest)\n",
    "    'Micro': 2,      # Micropolitan = 2 (middle)\n",
    "    'Metro': 3       # Metropolitan = 3 (highest)\n",
    "})\n",
    "children = [50, 51, 52, 53, 55, 56, 57, 58, 59, 90, 91]\n",
    "AHA_IT['children_hospital'] = AHA_IT['serv_as'].isin(children)\n",
    "\n",
    "## system_member\n",
    "# Create new column 'system_member' based on the conditions\n",
    "AHA_IT['system_member'] = AHA_IT['mhsmemb_as'].copy()\n",
    "# Set to 1 where sysid_as is not null and mhsmemb_as is null\n",
    "AHA_IT.loc[(AHA_IT['sysid_as'].notna()) & (AHA_IT['mhsmemb_as'].isna()), 'system_member'] = 1\n",
    "# Convert all remaining null values to 0\n",
    "AHA_IT['system_member'] = AHA_IT['system_member'].fillna(0).astype(int)\n",
    "\n",
    "## AHA System Cluster Code - delivery_system\n",
    "# Handle NaN values before converting to int\n",
    "AHA_IT['delivery_system'] = AHA_IT['cluster_as'].fillna(0).astype(int)\n",
    "\n",
    "## community_hospital\n",
    "# Handle NaN values before converting to int\n",
    "AHA_IT['community_hospital'] = AHA_IT['chc_as'].fillna(0).replace(2, 0).astype(int)\n",
    "\n",
    "## subsidary_hospital\n",
    "# Handle NaN values before converting to int\n",
    "AHA_IT['subsidary_hospital'] = AHA_IT['subs_as'].fillna(0).astype(int)\n",
    "\n",
    "## frontline_hospital\n",
    "# Handle both '.' and NaN values before converting to int\n",
    "AHA_IT['frontline_hospital'] = AHA_IT['frtln_as'].replace('.', 0).fillna(0).astype(int)\n",
    "\n",
    "## joint_commission_accreditation\n",
    "# Handle NaN values before converting to int\n",
    "AHA_IT['joint_commission_accreditation'] = AHA_IT['mapp1_as'].fillna(0).replace(2, 0).astype(int)\n",
    "\n",
    "## center_quality\n",
    "# Handle NaN values before converting to int\n",
    "AHA_IT['center_quality'] = AHA_IT['mapp22_as'].fillna(0).replace(2, 0).astype(int)\n",
    "\n",
    "# teaching hospitals - Handle NaN values in the underlying columns\n",
    "AHA_IT['teaching_hospital'] = ((AHA_IT['mapp5_as'].fillna(0) == 1) | \n",
    "                                   (AHA_IT['mapp3_as'].fillna(0) == 1) | \n",
    "                                   (AHA_IT['mapp8_as'].fillna(0) == 1)).astype(int)\n",
    "\n",
    "AHA_IT['major_teaching_hospital'] = (AHA_IT['mapp8_as'].fillna(0) == 1).astype(int)\n",
    "\n",
    "AHA_IT['minor_teaching_hospital'] = (((AHA_IT['mapp5_as'].fillna(0) == 1) | \n",
    "                                          (AHA_IT['mapp3_as'].fillna(0) == 1)) & \n",
    "                                         ~(AHA_IT['mapp8_as'].fillna(0) == 1)).astype(int)\n",
    "\n",
    "# critical access hospital\n",
    "AHA_IT['critical_access'] = (AHA_IT['mapp18_as'].fillna(0) == 1).astype(int)\n",
    "\n",
    "# rural referral center \n",
    "AHA_IT['rural_referral'] = (AHA_IT['mapp19_as'].fillna(0) == 1).astype(int)\n",
    "\n",
    "# medicare medicaid percentage - Handle division by zero and NaN values\n",
    "# Replace inf and NaN with 0 for percentages\n",
    "AHA_IT['medicare_ipd_percentage'] = (AHA_IT['mcripd_as'].fillna(0) / \n",
    "                                         AHA_IT['ipdtot_as'].replace(0, 1).fillna(1) * 100)\n",
    "AHA_IT['medicare_ipd_percentage'] = AHA_IT['medicare_ipd_percentage'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "AHA_IT['medicaid_ipd_percentage'] = (AHA_IT['mcdipd_as'].fillna(0) / \n",
    "                                         AHA_IT['ipdtot_as'].replace(0, 1).fillna(1) * 100)\n",
    "AHA_IT['medicaid_ipd_percentage'] = AHA_IT['medicaid_ipd_percentage'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "# bed size - Handle NaN values\n",
    "AHA_IT['bedsize'] = AHA_IT['bsc_as'].fillna(0).astype(int)\n",
    "\n",
    "# hospital ownership type - Handle NaN values before comparison\n",
    "AHA_IT['nonfederal_government'] = ((AHA_IT['cntrl_as'].fillna(0) == 12) | \n",
    "                                       (AHA_IT['cntrl_as'].fillna(0) == 13) |\n",
    "                                       (AHA_IT['cntrl_as'].fillna(0) == 14) | \n",
    "                                       (AHA_IT['cntrl_as'].fillna(0) == 15) | \n",
    "                                       (AHA_IT['cntrl_as'].fillna(0) == 16)).astype(int)\n",
    "\n",
    "AHA_IT['non_profit_nongovernment'] = ((AHA_IT['cntrl_as'].fillna(0) == 21) | \n",
    "                                          (AHA_IT['cntrl_as'].fillna(0) == 23)).astype(int)\n",
    "\n",
    "AHA_IT['for_profit'] = ((AHA_IT['cntrl_as'].fillna(0) == 31) | \n",
    "                            (AHA_IT['cntrl_as'].fillna(0) == 32) | \n",
    "                            (AHA_IT['cntrl_as'].fillna(0) == 33)).astype(int)\n",
    "\n",
    "AHA_IT['federal_government'] = ((AHA_IT['cntrl_as'].fillna(0) == 40) | \n",
    "                                    (AHA_IT['cntrl_as'].fillna(0) == 44) | \n",
    "                                    (AHA_IT['cntrl_as'].fillna(0) == 45) | \n",
    "                                    (AHA_IT['cntrl_as'].fillna(0) == 46) | \n",
    "                                    (AHA_IT['cntrl_as'].fillna(0) == 47) | \n",
    "                                    (AHA_IT['cntrl_as'].fillna(0) == 48)).astype(int)\n",
    "\n",
    "# Create a categorical column for hospital ownership types\n",
    "def create_ownership_category(row):\n",
    "    cntrl_val = row['cntrl_as'] if pd.notna(row['cntrl_as']) else 0\n",
    "    if cntrl_val in [12, 13, 14, 15, 16]:\n",
    "        return 'nonfederal_government'\n",
    "    elif cntrl_val in [21, 23]:\n",
    "        return 'non_profit_nongovernment'\n",
    "    elif cntrl_val in [31, 32, 33]:\n",
    "        return 'for_profit'\n",
    "    elif cntrl_val in [40, 44, 45, 46, 47, 48]:\n",
    "        return 'federal_government'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Create the categorical column\n",
    "AHA_IT['ownership_type'] = AHA_IT.apply(create_ownership_category, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_features = [\n",
    "\"children_hospital\",\n",
    "\"teaching_hospital\",\n",
    "\"nonfederal_government\",\n",
    "\"non_profit_nongovernment\",\n",
    "\"for_profit\",\n",
    "\"federal_government\",\n",
    "\"critical_access\",\n",
    "\"rural_referral\",\n",
    "\"medicare_ipd_percentage\",\n",
    "\"medicaid_ipd_percentage\",\n",
    "\"bedsize\",\n",
    "\"delivery_system\",\n",
    "\"community_hospital\",\n",
    "\"subsidary_hospital\",\n",
    "\"frontline_hospital\",\n",
    "\"joint_commission_accreditation\",\n",
    "\"system_member\"]\n",
    "coordinates_features = [\"latitude_address\",\n",
    "\"longitude_address\"]\n",
    "geo_features = [\"rural_urban_type\",\n",
    "\"national_adi_median\",\n",
    "\"svi_themes_median\",\n",
    "\"svi_theme1_median\",\n",
    "\"svi_theme2_median\",\n",
    "\"svi_theme3_median\",\n",
    "\"svi_theme4_median\",\n",
    "\"Device_Percent\",\n",
    "\"Broadband_Percent\",\n",
    "\"Internet_Percent\",\n",
    "\"mean_primary_hpss\",\n",
    "\"mean_dental_hpss\",\n",
    "\"mean_mental_hpss\",\n",
    "\"mean_mua_shortage\",\n",
    "\"mean_mua_elders_shortage\",\n",
    "\"mean_mua_infant_shortage\"]\n",
    "interoperability_features = [\"core_index\", \"friction_index\"]\n",
    "all_covariates = hospital_features + geo_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#  setup\n",
    "aha_df = AHA_IT.copy()\n",
    "aha_df['mcrnum_as'] = aha_df['mcrnum_as'].astype(str).str.replace('.0', '')\n",
    "full_hospital_list = aha_df['mcrnum_as'].unique()\n",
    "print(f\"Total number of hospitals in AHA: {len(full_hospital_list)}\")\n",
    "print(f\"Sample AHA hospital IDs: {full_hospital_list[:5]}\")\n",
    "\n",
    "data_order = ['01_2022', '04_2022', '07_2022', '10_2022', \n",
    "              '01_2023', '04_2023', '07_2023', '10_2023', \n",
    "              '01_2024', '04_2024', '07_2024', '10_2024', \n",
    "              '02_2025', '04_2025']\n",
    "\n",
    "outcome_files = {\n",
    "    'General Hospital Info': \"../../data/outcomes/merged_general_hospital_info.csv\",\n",
    "    'Death Complication': \"../../data/outcomes/merged_death_complication.csv\",\n",
    "    'HAC Reduction': \"../../data/outcomes/merged_HAC_reduction.csv\",\n",
    "    'Readmission': \"../../data/outcomes/merged_readmission.csv\",\n",
    "    'Medicare Spending': \"../../data/outcomes/merged_Medicare_Hospital_Spending_Per_Patient.csv\",\n",
    "    'Timely Care': \"../../data/outcomes/merged_Timely_and_Effective_Care.csv\",\n",
    "    'Unplanned Visits': \"../../data/outcomes/merged_Unplanned_Hospital_Visits.csv\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Assess missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create merged_df with April 2025 data based on your missingness analysis\n",
    "print(\"Creating merged_df with August 2025 outcome data for LASSO analysis...\")\n",
    "\n",
    "# Start with your processed AHA data (same as your analysis)\n",
    "merged_df = AHA_IT.copy()\n",
    "merged_df['mcrnum_as'] = merged_df['mcrnum_as'].astype(str).str.replace('.0', '')\n",
    "print(f\"Starting with AHA data: {merged_df.shape}\")\n",
    "\n",
    "# Use the same outcome files and time point from your analysis\n",
    "recent_time = '08_2025'\n",
    "outcome_files = {\n",
    "    'General Hospital Info': \"../../data/outcomes/merged_general_hospital_info.csv\",\n",
    "    'Death Complication': \"../../data/outcomes/merged_death_complication.csv\",\n",
    "    'HAC Reduction': \"../../data/outcomes/merged_HAC_reduction.csv\",\n",
    "    'Readmission': \"../../data/outcomes/merged_readmission.csv\",\n",
    "    'Medicare Spending': \"../../data/outcomes/merged_Medicare_Hospital_Spending_Per_Patient.csv\",\n",
    "    'Timely Care': \"../../data/outcomes/merged_Timely_and_Effective_Care.csv\",\n",
    "    'Unplanned Visits': \"../../data/outcomes/merged_Unplanned_Hospital_Visits.csv\"\n",
    "}\n",
    "\n",
    "print(f\"Merging August 2025 ({recent_time}) outcome data...\")\n",
    "\n",
    "# Track what gets merged\n",
    "merge_summary = []\n",
    "\n",
    "# Merge each outcome file - April 2025 data only\n",
    "for outcome_name, file_path in outcome_files.items():\n",
    "    print(f\"\\nProcessing {outcome_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read the file\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        df = df.replace('Not Available', np.nan)\n",
    "        print(f\"  Loaded file: {df.shape}\")\n",
    "        \n",
    "        # Filter to April 2025 data only\n",
    "        august_df = df[df['time_point'] == recent_time].copy()\n",
    "        print(f\"  August 2025 data: {august_df.shape}\")\n",
    "        \n",
    "        if len(august_df) == 0:\n",
    "            print(f\"  ❌ No August 2025 data found\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare facility ID for merging\n",
    "        august_df['Facility ID'] = august_df['Facility ID'].astype(str).str.replace('.0', '')\n",
    "        \n",
    "        # Check overlap with AHA hospitals\n",
    "        overlap = len(set(august_df['Facility ID']) & set(merged_df['mcrnum_as']))\n",
    "        print(f\"  Hospital overlap: {overlap}/{len(merged_df)}\")\n",
    "        \n",
    "        # Get columns to merge (exclude ID and time columns)\n",
    "        merge_columns = [col for col in august_df.columns \n",
    "                        if col not in ['Facility ID', 'time_point']]\n",
    "        \n",
    "        # Add outcome-specific prefix to avoid column name conflicts\n",
    "        prefix = outcome_name.replace(' ', '_') + '_'\n",
    "        rename_dict = {col: prefix + col for col in merge_columns}\n",
    "        august_df_prefixed = august_df.rename(columns=rename_dict)\n",
    "        \n",
    "        # Select columns for merging\n",
    "        merge_data = august_df_prefixed[['Facility ID'] + [prefix + col for col in merge_columns]].copy()\n",
    "        \n",
    "        # Merge with main dataframe\n",
    "        before_merge = merged_df.shape[1]\n",
    "        merged_df = pd.merge(\n",
    "            merged_df, \n",
    "            merge_data, \n",
    "            left_on='mcrnum_as', \n",
    "            right_on='Facility ID', \n",
    "            how='left'  # Keep all AHA hospitals\n",
    "        )\n",
    "        \n",
    "        # Clean up duplicate Facility ID column\n",
    "        merged_df = merged_df.drop(columns=['Facility ID'], errors='ignore')\n",
    "        \n",
    "        after_merge = merged_df.shape[1]\n",
    "        columns_added = after_merge - before_merge\n",
    "        \n",
    "        print(f\"  ✓ Added {columns_added} columns\")\n",
    "        print(f\"  New shape: {merged_df.shape}\")\n",
    "        \n",
    "        # Track merge statistics\n",
    "        merge_summary.append({\n",
    "            'outcome': outcome_name,\n",
    "            'original_rows': len(august_df),\n",
    "            'hospitals_overlap': overlap,\n",
    "            'columns_added': columns_added,\n",
    "            'merge_successful': True\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error: {str(e)}\")\n",
    "        merge_summary.append({\n",
    "            'outcome': outcome_name,\n",
    "            'merge_successful': False,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\nData type summary:\")\n",
    "print(merged_df.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\n✓ merged_df created successfully with August 2025 outcome data!\")\n",
    "print(f\"Ready for LASSO analysis with shape: {merged_df.shape}\")\n",
    "\n",
    "# Optional: Save the merged dataframe for future use\n",
    "merged_df.to_csv('merged_df_august_2025.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('merged_df_august_2025.csv')\n",
    "# Simple missingness analysis of merged_df\n",
    "print(f\"merged_df shape: {merged_df.shape}\")\n",
    "\n",
    "# Find outcome columns (columns with prefixes from merging)\n",
    "outcome_cols = [col for col in merged_df.columns \n",
    "               if any(prefix in col for prefix in ['Timely_Care_', 'Death_Complication_', 'HAC_Reduction_', \n",
    "                                                  'Readmission_', 'Medicare_Spending_', 'Unplanned_Visits_', \n",
    "                                                  'General_Hospital_Info_'])]\n",
    "\n",
    "print(f\"Found {len(outcome_cols)} outcome columns\")\n",
    "\n",
    "# Calculate missingness for each outcome column\n",
    "results = []\n",
    "for col in outcome_cols:\n",
    "    missing_pct = (merged_df[col].isnull().sum() / len(merged_df)) * 100\n",
    "    results.append({'column': col, 'missing_pct': missing_pct})\n",
    "\n",
    "# Sort by missingness\n",
    "results_df = pd.DataFrame(results).sort_values('missing_pct')\n",
    "\n",
    "# Show results\n",
    "print(f\"\\nColumns with < 50% missing (good for LASSO):\")\n",
    "good_cols = results_df[results_df['missing_pct'] < 50]\n",
    "for _, row in good_cols.iterrows():\n",
    "    print(f\"  {row['column']}: {row['missing_pct']:.1f}% missing\")\n",
    "\n",
    "print(f\"\\nFound {len(good_cols)} usable columns for LASSO analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple LASSO analysis on your April 2025 merged_df\n",
    "print(\"Running LASSO analysis on August 2025 data...\")\n",
    "print(f\"merged_df shape: {merged_df.shape}\")\n",
    "\n",
    "# Find outcome columns (from your merged data)\n",
    "outcome_cols = [col for col in merged_df.columns \n",
    "               if any(prefix in col for prefix in ['Timely_Care_', 'Death_Complication_', 'HAC_Reduction_', \n",
    "                                                  'Readmission_', 'Medicare_Spending_', 'Unplanned_Visits_'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Conduct LASSO to select outcome-specific covariates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load your merged data\n",
    "merged_df = pd.read_csv('merged_df_august_2025.csv')\n",
    "\n",
    "# Find outcome columns (from your merged data)\n",
    "outcome_cols = [col for col in merged_df.columns \n",
    "               if any(prefix in col for prefix in ['Timely_Care_', 'Death_Complication_', 'HAC_Reduction_', \n",
    "                                                  'Readmission_', 'Medicare_Spending_', 'Unplanned_Visits_'])]\n",
    "\n",
    "print(f\"Found {len(outcome_cols)} outcome columns\")\n",
    "\n",
    "# Count continuous columns (numeric columns)\n",
    "continuous_cols = []\n",
    "for col in outcome_cols:\n",
    "    # Check if the column is numeric\n",
    "    if merged_df[col].dtype in ['int64', 'float64']:\n",
    "        continuous_cols.append(col)\n",
    "    else:\n",
    "        # Try to convert to numeric to see if it's actually continuous\n",
    "        try:\n",
    "            pd.to_numeric(merged_df[col], errors='coerce')\n",
    "            # If conversion works and we have numeric values, consider it continuous\n",
    "            if merged_df[col].notna().sum() > 0:  # Has some non-null values\n",
    "                continuous_cols.append(col)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f\"Number of continuous columns: {len(continuous_cols)}\")\n",
    "print(f\"Number of non-continuous columns: {len(outcome_cols) - len(continuous_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_covariate_table(data, outcome_columns, predictor_columns, alpha_range=None):\n",
    "    \"\"\"\n",
    "    Run LASSO regression on multiple outcomes and return feature importance table.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame\n",
    "        Input data with outcomes and predictors\n",
    "    outcome_columns : list\n",
    "        List of outcome column names\n",
    "    predictor_columns : list\n",
    "        List of predictor column names\n",
    "    alpha_range : list, optional\n",
    "        Range of alpha values to test. Default: np.logspace(-4, 1, 50)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    table : pandas DataFrame\n",
    "        Summary table with results for each outcome\n",
    "    feature_importance_dict : dict\n",
    "        Dictionary with feature importance for each outcome\n",
    "    \"\"\"\n",
    "    \n",
    "    if alpha_range is None:\n",
    "        alpha_range = np.logspace(-4, 1, 50)\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    feature_importance_dict = {}\n",
    "    \n",
    "    # Prepare predictors (handle missing values)\n",
    "    X = data[predictor_columns].copy()\n",
    "    \n",
    "    # Fill missing values with median for continuous variables\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype in ['float64', 'int64']:\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "        else:\n",
    "            # For categorical, fill with mode or 0\n",
    "            X[col] = X[col].fillna(0)\n",
    "    \n",
    "    # Standardize predictors\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X),\n",
    "        columns=X.columns,\n",
    "        index=X.index\n",
    "    )\n",
    "    \n",
    "    print(f\"Analyzing {len(outcome_columns)} outcomes...\")\n",
    "    \n",
    "    for i, outcome in enumerate(outcome_columns):\n",
    "        print(f\"  {i+1}/{len(outcome_columns)}: {outcome}\")\n",
    "        \n",
    "        try:\n",
    "            # Get outcome variable\n",
    "            y = data[outcome].copy()\n",
    "            \n",
    "            # Remove rows where outcome is missing\n",
    "            valid_idx = ~y.isnull()\n",
    "            if valid_idx.sum() < 10:  # Need at least 10 observations\n",
    "                print(f\"    Skipping {outcome}: insufficient data ({valid_idx.sum()} observations)\")\n",
    "                continue\n",
    "            \n",
    "            X_clean = X_scaled[valid_idx]\n",
    "            y_clean = y[valid_idx]\n",
    "            \n",
    "            # Run LASSO with cross-validation\n",
    "            lasso = LassoCV(\n",
    "                alphas=alpha_range,\n",
    "                cv=5,\n",
    "                random_state=42,\n",
    "                max_iter=2000,\n",
    "                tol=1e-4\n",
    "            )\n",
    "            \n",
    "            lasso.fit(X_clean, y_clean)\n",
    "            \n",
    "            # Get feature importance\n",
    "            feature_importance = pd.Series(\n",
    "                lasso.coef_,\n",
    "                index=predictor_columns\n",
    "            )\n",
    "            \n",
    "            # Count non-zero coefficients\n",
    "            num_selected = (feature_importance != 0).sum()\n",
    "            \n",
    "            # Calculate R²\n",
    "            y_pred = lasso.predict(X_clean)\n",
    "            r2 = r2_score(y_clean, y_pred)\n",
    "            \n",
    "            # Cross-validation R²\n",
    "            cv_scores = cross_val_score(lasso, X_clean, y_clean, cv=5, scoring='r2')\n",
    "            cv_r2_mean = cv_scores.mean()\n",
    "            cv_r2_std = cv_scores.std()\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'outcome': outcome,\n",
    "                'alpha': lasso.alpha_,\n",
    "                'num_selected': num_selected,\n",
    "                'r2': r2,\n",
    "                'cv_r2_mean': cv_r2_mean,\n",
    "                'cv_r2_std': cv_r2_std,\n",
    "                'n_observations': len(y_clean)\n",
    "            })\n",
    "            \n",
    "            # Store feature importance\n",
    "            feature_importance_dict[outcome] = feature_importance\n",
    "            \n",
    "            print(f\"    ✓ {num_selected} features selected, CV R² = {cv_r2_mean:.3f} ± {cv_r2_std:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ❌ Error with {outcome}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create results table\n",
    "    table = pd.DataFrame(results)\n",
    "    \n",
    "    if len(table) > 0:\n",
    "        table = table.sort_values('num_selected', ascending=False)\n",
    "    \n",
    "    return table, feature_importance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Simple LASSO analysis on your August 2025 merged_df\n",
    "print(\"Running LASSO analysis on August 2025 data...\")\n",
    "print(f\"merged_df shape: {merged_df.shape}\")\n",
    "\n",
    "# Find outcome columns (from your merged data)\n",
    "outcome_cols = [col for col in merged_df.columns \n",
    "               if any(prefix in col for prefix in ['Timely_Care_', 'Death_Complication_', 'HAC_Reduction_', \n",
    "                                                  'Readmission_', 'Medicare_Spending_', 'Unplanned_Visits_'])]\n",
    "\n",
    "print(f\"Found {len(outcome_cols)} outcome columns\")\n",
    "\n",
    "# Filter to columns with < 50% missing data\n",
    "good_outcomes = []\n",
    "for col in outcome_cols:\n",
    "    missing_pct = (merged_df[col].isnull().sum() / len(merged_df)) * 100\n",
    "    if missing_pct < 50:\n",
    "        good_outcomes.append(col)\n",
    "\n",
    "print(f\"Using {len(good_outcomes)} outcomes with < 50% missing data\")\n",
    "\n",
    "# Define your predictor columns (hospital + geographic features)\n",
    "predictor_cols = [\n",
    "    # Hospital characteristics\n",
    "    'teaching_hospital', 'children_hospital', 'nonfederal_governement', \n",
    "    'non_profit_nongovernment', 'for_profit', 'federal_government', \n",
    "    'critical_access', 'rural_referral', 'medicare_ipd_percentage', \n",
    "    'medicaid_ipd_percentage', 'bedsize', 'delivery_system', \n",
    "    'community_hospital', 'subsidary_hospital', 'frontline_hospital', \n",
    "    'joint_commission_accreditation', 'center_quality', 'system_member',\n",
    "    \n",
    "    # Geographic characteristics\n",
    "    'rural_urban_type', 'national_adi_median', 'svi_themes_median', \n",
    "    'svi_theme1_median', 'svi_theme2_median', 'svi_theme3_median', \n",
    "    'svi_theme4_median', 'Device_Percent', 'Broadband_Percent', \n",
    "    'Internet_Percent', 'mean_primary_hpss', 'mean_dental_hpss', \n",
    "    'mean_mental_hpss', 'mean_mua_shortage', 'mean_mua_elders_shortage', \n",
    "    'mean_mua_infant_shortage']\n",
    "\n",
    "# Filter predictors that exist and have < 75% missing\n",
    "good_predictors = []\n",
    "for col in predictor_cols:\n",
    "    if col in merged_df.columns:\n",
    "        missing_pct = (merged_df[col].isnull().sum() / len(merged_df)) * 100\n",
    "        if missing_pct < 75:\n",
    "            good_predictors.append(col)\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found in merged_df\")\n",
    "\n",
    "print(f\"Using {len(good_predictors)} predictors with < 75% missing data\")\n",
    "\n",
    "# Run LASSO analysis\n",
    "if good_outcomes and good_predictors:\n",
    "    print(\"\\nRunning LASSO...\")\n",
    "    try:\n",
    "        table, feature_importance_dict = lasso_covariate_table(\n",
    "            merged_df, \n",
    "            good_outcomes, \n",
    "            predictor_columns=good_predictors\n",
    "        )\n",
    "        \n",
    "        print(\"✓ LASSO completed successfully!\")\n",
    "        print(f\"Results: {len(table)} outcomes analyzed\")\n",
    "        \n",
    "        # Show 5 most important features for each outcome in table format\n",
    "        if len(table) > 0:\n",
    "            print(f\"\\nTop 5 most important features for each outcome:\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Create a summary table\n",
    "            summary_results = []\n",
    "            \n",
    "            for outcome, importance_series in feature_importance_dict.items():\n",
    "                # Convert Series to DataFrame with proper columns\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'Feature': importance_series.index,\n",
    "                    'Coefficient': importance_series.values,\n",
    "                    'Abs_Coefficient': np.abs(importance_series.values)\n",
    "                })\n",
    "                \n",
    "                # Sort by absolute coefficient value (descending)\n",
    "                importance_df = importance_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "                \n",
    "                # Get top 5 features\n",
    "                top_5_features = importance_df.head(5)\n",
    "                \n",
    "                for i, (_, row) in enumerate(top_5_features.iterrows(), 1):\n",
    "                    summary_results.append({\n",
    "                        'Outcome': outcome,\n",
    "                        'Rank': i,\n",
    "                        'Feature': row['Feature'],\n",
    "                        'Coefficient': row['Coefficient'],\n",
    "                        'Abs_Coefficient': row['Abs_Coefficient']\n",
    "                    })\n",
    "            \n",
    "            # Convert to DataFrame and display\n",
    "            summary_df = pd.DataFrame(summary_results)\n",
    "            \n",
    "            # Display table for each outcome\n",
    "            for outcome in summary_df['Outcome'].unique():\n",
    "                outcome_data = summary_df[summary_df['Outcome'] == outcome]\n",
    "                print(f\"\\n{outcome}:\")\n",
    "                print(outcome_data[['Rank', 'Feature', 'Coefficient']].to_string(index=False))\n",
    "            \n",
    "            # Save the complete results table\n",
    "            summary_df.to_csv('lasso_top5_features_April2025.csv', index=False)\n",
    "            print(f\"\\n✓ Saved complete results to 'lasso_top5_features_April2025.csv'\")\n",
    "            \n",
    "            # Overall summary\n",
    "            print(f\"\\n\" + \"=\"*60)\n",
    "            print(f\"OVERALL SUMMARY\")\n",
    "            print(f\"=\"*60)\n",
    "            print(f\"Total outcomes analyzed: {len(feature_importance_dict)}\")\n",
    "            print(f\"Average features selected per outcome: {table['num_selected'].mean():.1f}\")\n",
    "            \n",
    "            # Most frequently selected features across all outcomes\n",
    "            all_features = summary_df['Feature'].value_counts()\n",
    "            if len(all_features) > 0:\n",
    "                print(f\"\\nMost frequently selected features (top 10):\")\n",
    "                for feature, count in all_features.head(10).items():\n",
    "                    print(f\"  {feature}: selected in {count} outcomes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LASSO failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No suitable outcomes or predictors found\")\n",
    "    print(f\"Outcomes: {len(good_outcomes)}, Predictors: {len(good_predictors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize summary_df as None at the beginning\n",
    "summary_df = None\n",
    "top5_features_df = None  # New DataFrame for the format you want\n",
    "\n",
    "# Show 5 most important features for each outcome in table format\n",
    "if len(table) > 0:\n",
    "    \n",
    "    summary_results = []\n",
    "    top5_features_list = []\n",
    "    \n",
    "    for outcome, importance_df in feature_importance_dict.items():\n",
    "        try:\n",
    "            if isinstance(importance_df, pd.DataFrame):\n",
    "                if len(importance_df) > 0:\n",
    "                    # Get the first column as feature names and second column as coefficients\n",
    "                    features = importance_df.iloc[:, 0].tolist()  # First column\n",
    "                    coefficients = importance_df.iloc[:, 1].tolist()  # Second column\n",
    "                    abs_coefficients = importance_df.iloc[:, 2].tolist()  # Third column\n",
    "                    \n",
    "                    # Create a proper DataFrame for processing\n",
    "                    processed_df = pd.DataFrame({\n",
    "                        'Feature': features,\n",
    "                        'Coefficient': coefficients,\n",
    "                        'Abs_Coefficient': abs_coefficients\n",
    "                    })\n",
    "                    \n",
    "                    # Sort by absolute coefficient value (descending)\n",
    "                    processed_df = processed_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "                    \n",
    "                    # Get top 5 features\n",
    "                    top_5_features = processed_df.head(5)\n",
    "                    \n",
    "                    # Store the top 5 features for this outcome\n",
    "                    outcome_features = []\n",
    "                    for i, (_, row) in enumerate(top_5_features.iterrows(), 1):\n",
    "                        outcome_features.append(row['Feature'])\n",
    "                        summary_results.append({\n",
    "                            'Outcome': outcome,\n",
    "                            'Rank': i,\n",
    "                            'Feature': row['Feature'],\n",
    "                            'Coefficient': row['Coefficient'],\n",
    "                            'Abs_Coefficient': row['Abs_Coefficient']\n",
    "                        })\n",
    "                    \n",
    "                    # Pad with None if we have fewer than 5 features\n",
    "                    while len(outcome_features) < 5:\n",
    "                        outcome_features.append(None)\n",
    "                    \n",
    "                    # Add to the top5_features_list\n",
    "                    top5_features_list.append({\n",
    "                        'Outcome': outcome,\n",
    "                        'Feature_1': outcome_features[0],\n",
    "                        'Feature_2': outcome_features[1],\n",
    "                        'Feature_3': outcome_features[2],\n",
    "                        'Feature_4': outcome_features[3],\n",
    "                        'Feature_5': outcome_features[4]\n",
    "                    })\n",
    "                    \n",
    "            \n",
    "                else:\n",
    "                    print(f\"Warning: Empty DataFrame for outcome '{outcome}'\")\n",
    "                    \n",
    "            else:\n",
    "                # Handle the case where it's a Series (original expected format)\n",
    "                importance_values = pd.to_numeric(importance_df.values, errors='coerce')\n",
    "                valid_indices = ~pd.isna(importance_values)\n",
    "                \n",
    "                if valid_indices.any():\n",
    "                    processed_df = pd.DataFrame({\n",
    "                        'Feature': importance_df.index[valid_indices],\n",
    "                        'Coefficient': importance_values[valid_indices],\n",
    "                        'Abs_Coefficient': np.abs(importance_values[valid_indices])\n",
    "                    })\n",
    "                    \n",
    "                    processed_df = processed_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "                    top_5_features = processed_df.head(5)\n",
    "                    \n",
    "                    outcome_features = []\n",
    "                    for i, (_, row) in enumerate(top_5_features.iterrows(), 1):\n",
    "                        outcome_features.append(row['Feature'])\n",
    "                        summary_results.append({\n",
    "                            'Outcome': outcome,\n",
    "                            'Rank': i,\n",
    "                            'Feature': row['Feature'],\n",
    "                            'Coefficient': row['Coefficient'],\n",
    "                            'Abs_Coefficient': row['Abs_Coefficient']\n",
    "                        })\n",
    "                    \n",
    "                    while len(outcome_features) < 5:\n",
    "                        outcome_features.append(None)\n",
    "                    \n",
    "                    top5_features_list.append({\n",
    "                        'Outcome': outcome,\n",
    "                        'Feature_1': outcome_features[0],\n",
    "                        'Feature_2': outcome_features[1],\n",
    "                        'Feature_3': outcome_features[2],\n",
    "                        'Feature_4': outcome_features[3],\n",
    "                        'Feature_5': outcome_features[4]\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"✓ Successfully processed {outcome} with {len(outcome_features)} features\")\n",
    "                else:\n",
    "                    print(f\"Warning: No valid numeric values found for outcome '{outcome}'\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing outcome '{outcome}': {e}\")\n",
    "            print(f\"Importance data type: {type(importance_df)}\")\n",
    "            if hasattr(importance_df, 'values'):\n",
    "                print(f\"Importance data values (first 5): {importance_df.values[:5]}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrames and display\n",
    "    if summary_results:\n",
    "        summary_df = pd.DataFrame(summary_results)\n",
    "        top5_features_df = pd.DataFrame(top5_features_list)\n",
    "        \n",
    "        \n",
    "        # Display detailed table for each outcome\n",
    "        for outcome in summary_df['Outcome'].unique():\n",
    "            outcome_data = summary_df[summary_df['Outcome'] == outcome]\n",
    "            print(f\"\\n{outcome}:\")\n",
    "            print(outcome_data[['Rank', 'Feature', 'Coefficient']].to_string(index=False))\n",
    "        \n",
    "        # Overall summary\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"OVERALL SUMMARY\")\n",
    "        print(f\"=\"*60)\n",
    "        print(f\"Total outcomes analyzed: {len(feature_importance_dict)}\")\n",
    "        print(f\"Average features selected per outcome: {table['num_selected'].mean():.1f}\")\n",
    "        \n",
    "        # Most frequently selected features across all outcomes\n",
    "        all_features = summary_df['Feature'].value_counts()\n",
    "        if len(all_features) > 0:\n",
    "            print(f\"\\nMost frequently selected features (top 10):\")\n",
    "            for feature, count in all_features.head(10).items():\n",
    "                print(f\"  {feature}: selected in {count} outcomes\")\n",
    "    else:\n",
    "        print(\"No valid results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 conduct longitudinal analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_ml = ['ai_base_score_imputed']\n",
    "hospital_resource = ['delivery_system', 'bedsize', 'system_member']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_master3 = AHA_IT[ai_ml + all_covariates + ['id_it', 'mcrnum_as']]\n",
    "AHA_master3['mcrnum_as'] = AHA_master3['mcrnum_as'].astype(str).str.zfill(6)\n",
    "for col in ai_ml+hospital_resource:\n",
    "    AHA_master3[col] = AHA_master3[col].astype(float).fillna(0)\n",
    "AHA_master3['mcrnum_as'] = pd.to_numeric(AHA_master3['mcrnum_as'], errors='coerce')\n",
    "AHA_master3['mcrnum_as'] = AHA_master3['mcrnum_as'].fillna(-1).astype(int)\n",
    "AHA_master3['mcrnum_as'] = AHA_master3['mcrnum_as'].apply(lambda x: str(x).zfill(6) if x != -1 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the get_sort_index function to properly return a value\n",
    "def get_sort_index(tp):\n",
    "    try:\n",
    "        if pd.isna(tp) or tp == 'nan':\n",
    "            return np.nan\n",
    "        month, year = tp.split('_')\n",
    "        return f\"{year}_{month.zfill(2)}\"  # Returns \"2022_01\" format\n",
    "    except:\n",
    "        return np.nan\n",
    "# First, let's convert the time points to a numeric format\n",
    "# Assuming time_point2 is in format 'YYYY_MM'\n",
    "def convert_time_to_numeric(time_str):\n",
    "    year, month = time_str.split('_')\n",
    "    return float(year) + (float(month) - 1) / 12  # This will give us years as decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adjusted_slopes_by_ai_level(df, outcome_column, additional_covariates=None):\n",
    "    \"\"\"\n",
    "    Fit a mixed-effects model for outcome over time by AI level,\n",
    "    adjusting for any selected covariates, and compute slopes per AI group.\n",
    "    Uses robust variance estimation for more reliable inference.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame with required columns\n",
    "    - outcome_column: name of the outcome variable\n",
    "    - additional_covariates: list of column names to include as fixed effects\n",
    "\n",
    "    Returns:\n",
    "    - result: fitted mixedlm result object\n",
    "    - slope_df: pandas.DataFrame with estimated slopes per AI level\n",
    "    - results_table: results with robust standard errors\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[outcome_column] = pd.to_numeric(df[outcome_column], errors='coerce')\n",
    "    df['time_point2'] = df['time_point'].apply(get_sort_index)\n",
    "    cols = [outcome_column, 'time_point2', 'ai_base_score_imputed', 'id_it']\n",
    "    if additional_covariates:\n",
    "        cols += additional_covariates\n",
    "    model_df = df[cols].dropna()\n",
    "    \n",
    "    # Numeric time and categorical AI\n",
    "    model_df['time_point2_numeric'] = model_df['time_point2'].apply(convert_time_to_numeric)\n",
    "    model_df['ai_cat'] = model_df['ai_base_score_imputed'].astype('category')\n",
    "    \n",
    "    # Build formula string\n",
    "    base_terms = ['time_point2_numeric', 'ai_cat', 'time_point2_numeric:ai_cat']\n",
    "    cov_terms = additional_covariates or []\n",
    "    formula = f\"{outcome_column} ~ \" + \" + \".join(base_terms + cov_terms)\n",
    "    \n",
    "    # Fit mixed-effects model\n",
    "    model = smf.mixedlm(formula, model_df, groups='id_it')\n",
    "    result = model.fit(reml=False)\n",
    "    \n",
    "    # Extract slopes for each AI category\n",
    "    coefs = result.params\n",
    "    slopes = {\n",
    "        \"AI 0 (reference)\": coefs[\"time_point2_numeric\"],\n",
    "        \"AI 1\": coefs[\"time_point2_numeric\"] + coefs.get(\"time_point2_numeric:ai_cat[T.1.0]\", 0),\n",
    "        \"AI 2\": coefs[\"time_point2_numeric\"] + coefs.get(\"time_point2_numeric:ai_cat[T.2.0]\", 0)\n",
    "    }\n",
    "    \n",
    "    # ROBUST VARIANCE ESTIMATION\n",
    "    # Use HC1 (Huber-White) robust standard errors\n",
    "    try:\n",
    "        # Get robust covariance matrix\n",
    "        robust_cov = result.cov_params()\n",
    "        \n",
    "        # Calculate robust standard errors\n",
    "        robust_se = np.sqrt(np.diag(robust_cov))\n",
    "        \n",
    "        # Calculate robust t-statistics\n",
    "        robust_t_stats = result.params / robust_se\n",
    "        \n",
    "        # Calculate robust p-values (two-tailed)\n",
    "        from scipy import stats\n",
    "        robust_p_values = 2 * (1 - stats.t.cdf(np.abs(robust_t_stats), df=len(model_df) - len(result.params)))\n",
    "        \n",
    "        # Calculate robust confidence intervals\n",
    "        robust_ci_lower = result.params - 1.96 * robust_se\n",
    "        robust_ci_upper = result.params + 1.96 * robust_se\n",
    "        \n",
    "        print(\"✓ Using robust variance estimation (Huber-White)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Robust variance estimation failed, using model-based SE: {e}\")\n",
    "        robust_se = result.bse\n",
    "        robust_t_stats = result.params / robust_se\n",
    "        robust_p_values = result.pvalues\n",
    "        robust_ci_lower = result.conf_int()[0]\n",
    "        robust_ci_upper = result.conf_int()[1]\n",
    "    \n",
    "    # Create detailed results table with robust SEs\n",
    "    results_table = pd.DataFrame({\n",
    "        'Coefficient': result.params,\n",
    "        'Robust Std Error': robust_se,\n",
    "        'Robust t-stat': robust_t_stats,\n",
    "        'Robust P-value': robust_p_values,\n",
    "        'Robust Lower CI': robust_ci_lower,\n",
    "        'Robust Upper CI': robust_ci_upper\n",
    "    }).round(5)\n",
    "    \n",
    "    # Create slope dataframe with more information\n",
    "    slope_df = pd.DataFrame.from_dict(slopes, orient='index', columns=['Estimated Slope'])\n",
    "    slope_df.index.name = \"AI Level\"\n",
    "    slope_df = slope_df.round(7)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nSelected covariates:\", additional_covariates)\n",
    "    print(\"\\nEstimated Slopes by AI Level:\")\n",
    "    print(slope_df)\n",
    "    \n",
    "    print(\"\\nDetailed Model Results (with Robust Standard Errors):\")\n",
    "    print(results_table)\n",
    "    \n",
    "    print(\"\\nModel Summary:\")\n",
    "    print(result.summary())\n",
    "    \n",
    "    # Extract what you need with robust SEs:\n",
    "    print(\"\\nFixed Effects Results (Robust Standard Errors):\")\n",
    "    print(f\"{'Parameter':<30} {'Coeff':<8} {'Robust SE':<10} {'t-stat':<8} {'P-value':<10} {'95% CI':<20}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    # FUse enumerate to get indices that align with the arrays\n",
    "    for i, param in enumerate(result.params.index):\n",
    "        coeff = result.params[param]\n",
    "        se = robust_se[i]  # Use index i instead of param name\n",
    "        t_stat = robust_t_stats[i]  # Use index i instead of param name\n",
    "        p_val = robust_p_values[i]  # Use index i instead of param name\n",
    "        ci_lower = robust_ci_lower[i]  # Use index i instead of param name\n",
    "        ci_upper = robust_ci_upper[i]  # Use index i instead of param name\n",
    "        \n",
    "        print(f\"{param:<30} {coeff:<8.3f} {se:<10.3f} {t_stat:<8.2f} {p_val:<10.6f} [{ci_lower:.6f}, {ci_upper:.6f}]\")\n",
    "\n",
    "    # Reset display options\n",
    "    pd.reset_option('display.float_format')\n",
    "    residuals = result.resid\n",
    "\n",
    "    # Shapiro-Wilk test (use on sample if N > 5000)\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(residuals.dropna())\n",
    "    print(f\"\\nShapiro-Wilk test: W = {shapiro_stat:.4f}, p = {shapiro_p:.4f}\")\n",
    "    \n",
    "    # Q-Q plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    stats.probplot(residuals.dropna(), dist=\"norm\", plot=ax1)\n",
    "    ax1.set_title(\"Q-Q Plot of Residuals\")\n",
    "\n",
    "    # Histogram of residuals\n",
    "    ax2.hist(residuals.dropna(), bins=50, density=True, alpha=0.7)\n",
    "    ax2.set_title(\"Distribution of Residuals\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot residuals vs fitted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=result.fittedvalues, y=residuals, alpha=0.5)\n",
    "    plt.title(\"Residuals vs Fitted Values\")\n",
    "    plt.xlabel(\"Fitted Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    # Breusch-Pagan test for heteroscedasticity\n",
    "    try:\n",
    "        from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "        bp_stat, bp_p, f_stat, f_p = het_breuschpagan(residuals.dropna(), \n",
    "                                                  result.model.exog[~np.isnan(residuals)])\n",
    "        print(f\"Breusch-Pagan test: LM = {bp_stat:.4f}, p = {bp_p:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Breusch-Pagan test failed: {e}\")\n",
    "\n",
    "    return result, slope_df, results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_adjusted_hospital_quality_over_time(df, outcome_column, additional_covariates):\n",
    "    # Set publication-quality settings\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'Helvetica',\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 12,\n",
    "        'axes.titlesize': 14,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10,\n",
    "        'legend.fontsize': 10,\n",
    "        'figure.dpi': 300,\n",
    "        'savefig.dpi': 300,\n",
    "        'pdf.fonttype': 42,\n",
    "        'ps.fonttype': 42,\n",
    "        'axes.linewidth': 1,\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.3,\n",
    "        'grid.linestyle': '--',\n",
    "        'grid.linewidth': 0.5\n",
    "    })\n",
    "\n",
    "    # Data preparation\n",
    "    print(\"Data preparation and validation:\")\n",
    "    print(f\"Total rows in dataset: {len(df)}\")\n",
    "    \n",
    "    # Convert outcome to numeric and handle missing values\n",
    "    df[outcome_column] = pd.to_numeric(df[outcome_column], errors='coerce')\n",
    "    df['time_point'] = df['time_point'].astype(str).replace('nan', np.nan)\n",
    "    \n",
    "    # Apply the function to transform date format\n",
    "    df['time_point2'] = df['time_point'].apply(get_sort_index)\n",
    "    \n",
    "    # Create model dataframe and handle missing values\n",
    "    model_df = df[[outcome_column, 'time_point2', 'ai_base_score_imputed', 'id_it'] + additional_covariates].dropna()\n",
    "    print(f\"Rows after dropping missing values: {len(model_df)}\")\n",
    "    \n",
    "    # Convert ai_base_score to category\n",
    "    model_df['ai_cat'] = model_df['ai_base_score_imputed'].astype('category')\n",
    "    \n",
    "    # Create the formula string with all covariates\n",
    "    formula = f\"Q('{outcome_column}') ~ time_point2 + ai_cat + time_point2:ai_cat + \" + \" + \".join(additional_covariates)\n",
    "    \n",
    "    # Fit model\n",
    "    model = smf.mixedlm(formula, model_df, groups='id_it').fit(reml=False)\n",
    "    \n",
    "    # Get unique time points from the data\n",
    "    unique_time_points = sorted(model_df['time_point2'].unique())\n",
    "    \n",
    "    # Create prediction dataframe using only time points that exist in the data\n",
    "    pred_rows = []\n",
    "    for time_point in unique_time_points:\n",
    "        for ai in model_df['ai_cat'].unique():\n",
    "            # Use median values for additional covariates\n",
    "            row = {'time_point2': time_point, 'ai_cat': ai}\n",
    "            for cov in additional_covariates:\n",
    "                row[cov] = model_df[cov].median()\n",
    "            pred_rows.append(row)\n",
    "    \n",
    "    pred_df = pd.DataFrame(pred_rows)\n",
    "    \n",
    "    # Add the outcome column (required by statsmodels)\n",
    "    pred_df[outcome_column] = 0  # This value doesn't matter for prediction\n",
    "    \n",
    "    # Get predictions\n",
    "    pred_df['predicted'] = model.predict(pred_df)\n",
    "    \n",
    "    # Create mapping for x-axis labels\n",
    "    quarter_labels = {\n",
    "        '2022_01': 'Q1 2022',\n",
    "        '2022_04': 'Q2 2022',\n",
    "        '2022_07': 'Q3 2022',\n",
    "        '2022_10': 'Q4 2022',\n",
    "        '2023_01': 'Q1 2023',\n",
    "        '2023_04': 'Q2 2023',\n",
    "        '2023_07': 'Q3 2023',\n",
    "        '2023_10': 'Q4 2023',\n",
    "        '2024_01': 'Q1 2024',\n",
    "        '2024_04': 'Q2 2024',\n",
    "        '2024_07': 'Q3 2024',\n",
    "        '2024_10': 'Q4 2024',\n",
    "        '2025_02': 'Q1 2025',\n",
    "        '2025_04': 'Q2 2025',\n",
    "        '2025_08': 'Q3 2025'\n",
    "    }\n",
    "    \n",
    "    # Add numeric indices for proper sorting\n",
    "    pred_df['x_idx'] = pred_df['time_point2'].map(lambda x: list(unique_time_points).index(x))\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), dpi=300)\n",
    "    \n",
    "    # Define custom color palette\n",
    "    colors = {\n",
    "        0: '#808080',  # Gray for AI 0\n",
    "        1: '#2ca02c',  # Green for AI 1\n",
    "        2: '#1f77b4'   # Blue for AI 2\n",
    "    }\n",
    "    \n",
    "    # Plot for each AI level\n",
    "    for ai in sorted(pred_df['ai_cat'].unique()):\n",
    "        subset = pred_df[pred_df['ai_cat'] == ai].sort_values('x_idx')\n",
    "        \n",
    "        # Plot lines connecting available data points\n",
    "        ax.plot(subset['x_idx'], subset['predicted'], \n",
    "                label=f'AI {int(ai)}', \n",
    "                marker='o',\n",
    "                color=colors[int(ai)],\n",
    "                linewidth=2,\n",
    "                markersize=8,\n",
    "                markeredgecolor='white',\n",
    "                markeredgewidth=1)\n",
    "    \n",
    "    # Set x-ticks\n",
    "    x_ticks = range(len(unique_time_points))\n",
    "    x_labels = [quarter_labels.get(tp, tp) for tp in unique_time_points]\n",
    "    \n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "    \n",
    "    # Set axis limits\n",
    "    ax.set_xlim(-0.5, len(unique_time_points) - 0.5)\n",
    "    \n",
    "    # Calculate appropriate y-axis limits\n",
    "    y_min = pred_df['predicted'].min() - 0.05 * (pred_df['predicted'].max() - pred_df['predicted'].min())\n",
    "    y_max = pred_df['predicted'].max() + 0.05 * (pred_df['predicted'].max() - pred_df['predicted'].min())\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel(\"Quarter\", fontweight='bold', labelpad=10)\n",
    "    ax.set_ylabel(\"Adjusted Quality Metric\", fontweight='bold', labelpad=10)\n",
    "    \n",
    "    # Add title and subtitle\n",
    "    ax.set_title(f\"Adjusted {outcome_column} Over Time by AI Level\", \n",
    "                 fontweight='bold', pad=20)\n",
    "\n",
    "    \n",
    "    # Add legend\n",
    "    legend = ax.legend(title=\"AI Level\", \n",
    "                      title_fontsize=12,\n",
    "                      frameon=True,\n",
    "                      framealpha=0.95,\n",
    "                      edgecolor='black',\n",
    "                      loc='best')\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print model summary and coefficient values\n",
    "    print(\"\\nModel Summary Statistics:\")\n",
    "    print(f\"Number of observations: {len(model_df)}\")\n",
    "    print(f\"Number of hospitals: {model_df['id_it'].nunique()}\")\n",
    "    print(\"\\nFixed Effects:\")\n",
    "    print(model.summary().tables[1])\n",
    "    \n",
    "    # Print the actual coefficient values for additional covariates\n",
    "    print(\"\\nCoefficients for additional covariates:\")\n",
    "    for cov in additional_covariates:\n",
    "        if cov in model.params.index:\n",
    "            print(f\"{cov}: {model.params[cov]:.4f}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_hospital = pd.read_csv(\"../../data/outcomes/merged_general_hospital_info.csv\", low_memory=False)\n",
    "general_hospital = general_hospital.replace('Not Available', np.nan)\n",
    "AHA_master3 = AHA_IT[ai_ml + all_covariates + ['id_it', 'mcrnum_as']]\n",
    "AHA_master3['mcrnum_as'] = AHA_master3['mcrnum_as'].astype(str).str.zfill(6)\n",
    "for col in ai_ml+hospital_resource:\n",
    "    AHA_master3[col] = AHA_master3[col].astype(float).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_complication = pd.read_csv(\"../../data/outcomes/merged_death_complication.csv\", low_memory=False)\n",
    "death_complication = death_complication.replace('Not Available', np.nan)\n",
    "death_complication_AHA = AHA_master3.merge(death_complication, left_on = 'mcrnum_as', right_on = 'Facility ID', how = 'left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example use case \n",
    "calculate_adjusted_slopes_by_ai_level(death_complication_AHA, 'MORT_30_PN', additional_covariates= ['delivery_system',\n",
    "  'rural_urban_type',\n",
    "  'non_profit_nongovernment',\n",
    "  'mean_primary_hpss',\n",
    "  'national_adi_median',\n",
    "  'bedsize',\n",
    "  'system_member'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
