{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D1. Feature importance analysis \n",
    "\n",
    "**Description**  \n",
    "This section conducts machine learning (random forest) to identify the most important feature predicting the AI/ML implementation level \n",
    "\n",
    "**Purpose**  \n",
    "To identify the most important feature predicting the AI/ML implementation level \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Load necessary libraries, functions, and pre-processed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning and model evaluation\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report,\n",
    "    r2_score, \n",
    "    mean_squared_error, \n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "import sys\n",
    "from calculate_scores import create_union_aipred_row, apply_ai_scores_to_dataframe\n",
    "\n",
    "# Load data\n",
    "AHA_master = pd.read_csv('AHA_master_external_data.csv', low_memory=False)\n",
    "\n",
    "# Create aipred_it_union separately (your choice, works perfectly)\n",
    "AHA_master['aipred_it_union'] = AHA_master.apply(create_union_aipred_row, axis=1)\n",
    "\n",
    "# Use the apply function for all other scores\n",
    "AHA_IT = apply_ai_scores_to_dataframe(AHA_master)\n",
    "AHA_IT = AHA_IT[AHA_IT['id_it'].notna()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Data engineering \n",
    "\n",
    "These hospital characteristics were selected based on investigator consensus, and we used LASSO regression analysis to explore and identify additional variables that predict AI/ML implementation and reflect hospital resource levels.\n",
    "\n",
    "- **rural_urban_type** : collected from AHA survey. categorized into {1: rural, 2: micro, 3: metro} based on the location of the hospital ('CBSATYPE')\n",
    "- **system member** : hospital belonging to a corporate body that owns or manage health provider facilities or health-related subsidiaries. ('MHSMEMB')\n",
    "- **delivery_system** : delivery system identified using existing theory and AHA Annual Survey data {1: Centralized Health System, 2: Centralized Physician/Insurance Health System, 3: Moderately Centralized Health System, 4: Decentralized Health System, 5: Independent Hospital System, 6/Missing: Insufficient data to determine} ('CLUSTER')\n",
    "- **community_hospital** : all nonfederal, short-term general, and special hospitals whose facilities and services are available to the public {0: No, 1: Yes}('CHC')\n",
    "- **subsidary_hospital** : Hospital itself operates subsidiary corporation {0: No, 1: Yes} ('SUBS')\n",
    "- **frontline_hospital** : Frontline facility {0: No, 1: Yes} ('FRTLN')\n",
    "- **joint_commission_accreditaion** : Accreditation by joint commision {0: No, 1: Yes} ('MAPP1')\n",
    "- **center_quality** : Center for Improvement in Healthcare Quality Accreditation {0: No, 1: Yes} ('MAPP22')\n",
    "- **teaching_hospital** : major teaching hospital ('MAPP8'), minor teaching hospital ('MAPP3' or 'MAPP5')\n",
    "- **critical_access** critical access hospital {0: No, 1: Yes} ('MAPP18')\n",
    "- **rural_referral** : rural referral center {0: No, 1: Yes} ('MAPP19')\n",
    "- **ownership_type** : type of organization responsible for establishing policy concerning overall operation {government_federal, government_nonfederal, nonprofit, forprofit, other} ('CNTRL')\n",
    "- **bedsize** : bed-size category, ordinal variable ('BSC')\n",
    "- **medicare_ipd_percentage** : medicare inpatient days / total inpatient days. Proxy variable to reflect the proportion of medicare patient \n",
    "- **medicaid_ipd_percentage** : medicaid inpatient days / total inpatient days. Proxy variable to reflect the proportion of medicaid patients \n",
    "- **core_index** : summary measure to track the interoperability of US hospitals (https://doi.org/10.1093/jamia/ocae289)\n",
    "- **friction_index** : summary measures to track the barrier or difficulty in interoperability between hospitals (https://doi.org/10.1093/jamia/ocae289)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = [50, 51, 52, 53, 55, 56, 57, 58, 59, 90, 91]\n",
    "AHA_IT['children_hospital'] = AHA_IT['serv_as'].isin(children)\n",
    "AHA_IT['children_hospital'].value_counts()\n",
    "## rural_urban_type\n",
    "# Continue with CBSA type and other variables\n",
    "AHA_IT['rural_urban_type'] = AHA_IT['cbsatype_as'].map({\n",
    "    'Rural': 1,      # Rural = 1 (lowest)\n",
    "    'Micro': 2,      # Micropolitan = 2 (middle)\n",
    "    'Metro': 3       # Metropolitan = 3 (highest)\n",
    "})\n",
    "\n",
    "## system_member\n",
    "# Create new column 'system_member' based on the conditions\n",
    "AHA_IT['system_member'] = AHA_IT['mhsmemb_as'].copy()\n",
    "# Set to 1 where sysid_as is not null and mhsmemb_as is null\n",
    "AHA_IT.loc[(AHA_IT['sysid_as'].notna()) & (AHA_IT['mhsmemb_as'].isna()), 'system_member'] = 1\n",
    "# Convert all remaining null values to 0\n",
    "AHA_IT['system_member'] = AHA_IT['system_member'].fillna(0)\n",
    "\n",
    "## AHA System Cluster Code - delivery_system\n",
    "AHA_IT['delivery_system'] = AHA_IT['cluster_as']\n",
    "\n",
    "## community_hospital\n",
    "AHA_IT['community_hospital'] = AHA_IT['chc_as'].replace(2, 0)\n",
    "\n",
    "## subsidary_hospital\n",
    "AHA_IT['subsidary_hospital'] = AHA_IT['subs_as']\n",
    "\n",
    "## frontline_hospital\n",
    "AHA_IT['frontline_hospital'] = AHA_IT['frtln_as'].replace('.', 0)\n",
    "\n",
    "## joint_commission_accreditation\n",
    "AHA_IT['joint_commission_accreditation'] = AHA_IT['mapp1_as'].replace(2,0)\n",
    "\n",
    "## center_quality\n",
    "AHA_IT['center_quality'] = AHA_IT['mapp22_as'].replace(2,0)\n",
    "\n",
    "# teaching hospitals \n",
    "AHA_IT['teaching_hospital'] = ((AHA_IT['mapp5_as'] == 1) | (AHA_IT['mapp3_as'] == 1) | (AHA_IT['mapp8_as'] == 1)).astype(int)\n",
    "AHA_IT['major_teaching_hospital'] = ((AHA_IT['mapp8_as'] == 1)).astype(int)\n",
    "AHA_IT['minor_teaching_hospital'] = (((AHA_IT['mapp5_as'] == 1) | (AHA_IT['mapp3_as'] == 1))&~(AHA_IT['mapp8_as'] == 1)).astype(int)\n",
    "\n",
    "# critical access hospital\n",
    "AHA_IT['critical_access'] = (AHA_IT['mapp18_as'] == 1).astype(int)\n",
    "\n",
    "\n",
    "# rural referral center \n",
    "AHA_IT['rural_referral'] = (AHA_IT['mapp19_as'] == 1).astype(int)\n",
    "\n",
    "# medicare medicaid percentage\n",
    "AHA_IT['medicare_ipd_percentage'] = AHA_IT['mcripd_as'] / AHA_IT['ipdtot_as'] * 100\n",
    "AHA_IT['medicaid_ipd_percentage'] = AHA_IT['mcdipd_as'] / AHA_IT['ipdtot_as'] * 100\n",
    "\n",
    "# bed size \n",
    "AHA_IT['bedsize'] = AHA_IT['bsc_as'].astype(int)\n",
    "\n",
    "# hospital ownership type \n",
    "\n",
    "AHA_IT['nonfederal_governement'] = ((AHA_IT['cntrl_as'] == 12) | (AHA_IT['cntrl_as'] == 13)|(AHA_IT['cntrl_as'] == 14) | (AHA_IT['cntrl_as'] == 15)| (AHA_IT['cntrl_as'] == 16)).astype(int)\n",
    "AHA_IT['non_profit_nongovernment'] = ((AHA_IT['cntrl_as'] == 21) | (AHA_IT['cntrl_as'] == 23)).astype(int)\n",
    "AHA_IT['for_profit'] = ((AHA_IT['cntrl_as'] == 31) | (AHA_IT['cntrl_as'] == 32) | (AHA_IT['cntrl_as'] == 33)).astype(int)\n",
    "AHA_IT['federal_government'] = ((AHA_IT['cntrl_as'] == 40) | (AHA_IT['cntrl_as'] == 44) | (AHA_IT['cntrl_as'] == 45) | (AHA_IT['cntrl_as'] == 46) | (AHA_IT['cntrl_as'] == 47) | (AHA_IT['cntrl_as'] == 48)).astype(int)\n",
    "# Create a categorical column for hospital ownership types\n",
    "def create_ownership_category(row):\n",
    "    if row['cntrl_as'] in [12, 13, 14, 15, 16]:\n",
    "        return 'nonfederal_government'\n",
    "    elif row['cntrl_as'] in [21, 23]:\n",
    "        return 'non_profit_nongovernment'\n",
    "    elif row['cntrl_as'] in [31, 32, 33]:\n",
    "        return 'for_profit'\n",
    "    elif row['cntrl_as'] in [40, 44, 45, 46, 47, 48]:\n",
    "        return 'federal_government'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Create the categorical column\n",
    "AHA_IT['ownership_type'] = AHA_IT.apply(create_ownership_category, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace invalid SVI values with median of valid values\n",
    "svi_columns = ['svi_themes_median', 'svi_theme1_median', 'svi_theme2_median', \n",
    "              'svi_theme3_median', 'svi_theme4_median']\n",
    "\n",
    "for col in svi_columns:\n",
    "    # Identify valid values (0-1 range)\n",
    "    valid_mask = (AHA_IT[col] >= 0) & (AHA_IT[col] <= 1)\n",
    "    \n",
    "    # Calculate median of valid values\n",
    "    valid_median = AHA_IT.loc[valid_mask, col].median()\n",
    "    \n",
    "    # Replace invalid values with the median\n",
    "    invalid_mask = ~valid_mask\n",
    "    AHA_IT.loc[invalid_mask, col] = valid_median\n",
    "    \n",
    "    print(f\"Fixed {col}:\")\n",
    "    print(f\"  Replaced {invalid_mask.sum()} invalid values with median {valid_median:.4f}\")\n",
    "    print(f\"  New range: {AHA_IT[col].min():.4f} to {AHA_IT[col].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_features = [\"teaching_hospital\",\n",
    "\"nonfederal_governement\",\n",
    "\"non_profit_nongovernment\",\n",
    "\"for_profit\",\n",
    "\"federal_government\",\n",
    "\"critical_access\",\n",
    "\"rural_referral\",\n",
    "\"medicare_ipd_percentage\",\n",
    "\"medicaid_ipd_percentage\",\n",
    "\"bedsize\",\n",
    "\"delivery_system\",\n",
    "\"community_hospital\",\n",
    "\"subsidary_hospital\",\n",
    "\"frontline_hospital\",\n",
    "\"joint_commission_accreditation\",\n",
    "\"center_quality\",\n",
    "\"system_member\"]\n",
    "coordinates_features = [\"latitude_address\",\n",
    "\"longitude_address\"]\n",
    "geo_features = [\"rural_urban_type\",\n",
    "\"national_adi_median\",\n",
    "\"svi_themes_median\",\n",
    "\"svi_theme1_median\",\n",
    "\"svi_theme2_median\",\n",
    "\"svi_theme3_median\",\n",
    "\"svi_theme4_median\",\n",
    "\"Device_Percent\",\n",
    "\"Broadband_Percent\",\n",
    "\"Internet_Percent\",\n",
    "\"mean_primary_hpss\",\n",
    "\"mean_dental_hpss\",\n",
    "\"mean_mental_hpss\",\n",
    "\"mean_mua_shortage\",\n",
    "\"mean_mua_elders_shortage\",\n",
    "\"mean_mua_infant_shortage\"]\n",
    "interoperability_features = ['core_index', \"friction_index\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = hospital_features + geo_features + interoperability_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_IT[\"ai_base_score_imputed\"] = AHA_IT[\"ai_base_score_imputed\"].astype(float).fillna(0)\n",
    "y = AHA_IT['ai_base_score_imputed'].values \n",
    "X = AHA_IT[feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CV setup ===\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# === Storage ===\n",
    "r2_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "feature_importances = []\n",
    "\n",
    "print(\"Starting cross-validation with Random Forest feature importance...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "    print(f\"Processing fold {fold}/{n_splits}...\")\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    imputer = IterativeImputer(\n",
    "    max_iter=10,        # number of MICE iterations\n",
    "    random_state=42,\n",
    "    sample_posterior=False  \n",
    "    )\n",
    "    X_train_imp = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train),\n",
    "    columns=X.columns, index=X_train.index\n",
    "    )\n",
    "    X_val_imp = pd.DataFrame(\n",
    "    imputer.transform(X_val),\n",
    "    columns=X.columns, index=X_val.index\n",
    "    )\n",
    "\n",
    "    # Scale after imputation (fit on train only)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train_imp),\n",
    "        columns=X.columns, index=X_train.index\n",
    "    )\n",
    "    X_val_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_val_imp),\n",
    "        columns=X.columns, index=X_val.index\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Importances\n",
    "    feature_importances.append(rf_model.feature_importances_)\n",
    "\n",
    "    # Predict and metrics\n",
    "    y_pred = rf_model.predict(X_val_scaled)\n",
    "    r2_scores.append(r2_score(y_val, y_pred))\n",
    "    rmse_scores.append(np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "    mae_scores.append(mean_absolute_error(y_val, y_pred))\n",
    "    mape_scores.append(mean_absolute_percentage_error(y_val, y_pred))\n",
    "\n",
    "print(\"Cross-validation completed!\")\n",
    "\n",
    "# === Aggregate feature importance across folds ===\n",
    "feature_importances_arr = np.array(feature_importances)  # (n_folds, n_features)\n",
    "avg_feature_importance = np.mean(feature_importances_arr, axis=0)\n",
    "std_feature_importance = np.std(feature_importances_arr, axis=0)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': avg_feature_importance,\n",
    "    'std_importance': std_feature_importance,\n",
    "    'cv_stability': 1 - (std_feature_importance / (avg_feature_importance + 1e-10))\n",
    "}).sort_values('importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Top 10 features table ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\"*60)\n",
    "print(\"Note: Using Random Forest built-in feature importance (Gini importance)\")\n",
    "top_10 = feature_importance.head(10)[['feature', 'importance', 'std_importance', 'cv_stability']]\n",
    "print(top_10.to_string(index=False, float_format='%.4f'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot top-10 feature importances with std error bars ===\n",
    "fig, ax = plt.subplots(figsize=(10, 8), dpi=300)\n",
    "\n",
    "feature_importance_plot = feature_importance.head(10)\n",
    "bars = ax.barh(\n",
    "    feature_importance_plot['feature'],\n",
    "    feature_importance_plot['importance'],\n",
    "    xerr=feature_importance_plot['std_importance'],\n",
    "    capsize=5,\n",
    "    color='#2E86C1',\n",
    "    alpha=0.8,\n",
    "    edgecolor='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Feature Importance (Gini Importance)', fontweight='bold', labelpad=10)\n",
    "ax.set_title('Top 10 Most Important Features', fontweight='bold', pad=20)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "ax.grid(True, axis='x', linestyle='--', alpha=0.3)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 0.005, bar.get_y() + bar.get_height()/2,\n",
    "            f'{width:.3f}',\n",
    "            ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shap_plot_cv_approach(X, y, n_folds=5, title='SHAP Feature Importance for AI Base Score'):\n",
    "    \"\"\"\n",
    "    Cross-validated SHAP for Random Forest with MICE imputation and scaling each fold.\n",
    "    Uses interventional background and disables additivity check to avoid ExplainerError.\n",
    "    \"\"\"\n",
    "    os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    all_shap = []\n",
    "    all_X_val = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "        print(f\"Processing fold {fold}/{n_folds} for SHAP...\")\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train = y[train_idx]\n",
    "\n",
    "        # Impute (MICE) on train, transform val\n",
    "        imputer = IterativeImputer(max_iter=10, random_state=42, sample_posterior=False)\n",
    "        X_train_imp = pd.DataFrame(imputer.fit_transform(X_train), columns=X.columns, index=X_train.index)\n",
    "        X_val_imp   = pd.DataFrame(imputer.transform(X_val),      columns=X.columns, index=X_val.index)\n",
    "\n",
    "        # Scale on train, transform val\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imp), columns=X.columns, index=X_train.index)\n",
    "        X_val_scaled   = pd.DataFrame(scaler.transform(X_val_imp),       columns=X.columns, index=X_val.index)\n",
    "\n",
    "        # Train model\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # SHAP explainer: interventional background + disable additivity check\n",
    "        masker = shap.maskers.Independent(X_train_scaled, max_samples=200)  # subsample background for speed\n",
    "        explainer = shap.TreeExplainer(model, data=masker, feature_perturbation=\"interventional\", model_output=\"raw\")\n",
    "\n",
    "        # Disable additivity check at call to avoid ExplainerError across folds\n",
    "        sv = explainer.shap_values(X_val_scaled, check_additivity=False)  # shape (n_val, n_features)\n",
    "\n",
    "        all_shap.append(sv)\n",
    "        all_X_val.append(X_val_scaled)\n",
    "\n",
    "    # Combine across folds\n",
    "    combined_shap = np.vstack(all_shap)\n",
    "    combined_X = pd.concat(all_X_val, axis=0, ignore_index=True)\n",
    "\n",
    "    # Plot\n",
    "    plt.rcParams['font.family'] = 'Helvetica'\n",
    "    plt.rcParams['font.size'] = 12\n",
    "\n",
    "    plt.figure(figsize=(12, 8), dpi=300)\n",
    "    shap.summary_plot(combined_shap, combined_X, show=False, max_display=10)\n",
    "    plt.title(title, fontweight='bold', pad=20, fontsize=16)\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/shap_summary_plot_cv.pdf',\n",
    "                bbox_inches='tight', dpi=300, format='pdf',\n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.show()\n",
    "\n",
    "    return combined_shap, combined_X\n",
    "\n",
    "# Usage:\n",
    "shap_values, X_combined = create_shap_plot_cv_approach(X, y, n_folds=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
