{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3_1. Hotspot National Analysis\n",
    "\n",
    "**Description**  \n",
    "This section identify statistically significant hotspots and coldspots of hospital AI adoption across the United States. Using Getis-Ord Gi* and Local Moran's I statistics, this analysis pinpoints specific geographic locations where AI adoption is significantly higher (hotspots) or lower (coldspots) \n",
    "\n",
    "**Purpose**  \n",
    "To identify hotspots and coldspots \n",
    "\n",
    "**Disclaimer**  \n",
    "- This codebase was partially cleaned and annotated using OpenAIâ€™s ChatGPT-4o. Please review and validate before using for critical purposes.  \n",
    "- AHA data is subscription-based and not publicly shareable. All reported results are aggregated at the state or census division level.\n",
    "- All publicly available data should also be independently downlowded from the source \n",
    "\n",
    "**Notebook Workflow**  \n",
    "\n",
    "0. Load necessary libraries, functions, and pre-processed data \n",
    "1. Prepare the data to conduct hotspot analysis \n",
    "2. load hotspot functions \n",
    "3. run hotspots\n",
    "4. get hotspot stats and visualize hotspots  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3_0 load necessary libraries, functions and preprocessed data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C3_0_1 load libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C3_0_2 load custom functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_base_ai_implementation_row(row):\n",
    "    \"\"\"\n",
    "    Calculate base AI implementation score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: Base AI implementation score\n",
    "    \"\"\"\n",
    "    # Base AI implementation score (continuous)\n",
    "    # Return None if the input value is null\n",
    "    if pd.isna(row['aipred_it']):\n",
    "        return None\n",
    "    elif row['aipred_it'] == 1:  # Machine Learning\n",
    "        return 2\n",
    "    elif row['aipred_it'] == 2:  # Other Non-Machine Learning Predictive Models\n",
    "        return 1\n",
    "    else:  # Neither (3) or Do not know (4)\n",
    "        return 0\n",
    "\n",
    "def calculate_ai_implementation_breadth_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI implementation breadth score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI implementation breadth score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        breadth_score = base_score\n",
    "        # Implementation Breadth Score - count use cases\n",
    "        use_case_cols = ['aitraj_it', 'airfol_it', 'aimhea_it', 'airect_it', \n",
    "                     'aibill_it', 'aische_it', 'aipoth_it', 'aicloth_it']\n",
    "        for col in use_case_cols:\n",
    "            if row[col] is None:\n",
    "                breadth_score += 0\n",
    "            else:\n",
    "                breadth_score += row[col] * 0.25  # 0.25 points per use case\n",
    "        return breadth_score\n",
    "\n",
    "def calculate_ai_development_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI development score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI development score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0 \n",
    "    else:\n",
    "        dev_score = base_score\n",
    "        if 'mlsed_it' in row and pd.notna(row['mlsed_it']):\n",
    "            dev_score += row['mlsed_it'] * 2  # Self-developed\n",
    "        if 'mldev_it' in row and pd.notna(row['mldev_it']):\n",
    "            dev_score += row['mldev_it']  # EHR developer\n",
    "        if 'mlthd_it' in row and pd.notna(row['mlthd_it']):\n",
    "            dev_score += row['mlthd_it']  # Third-party\n",
    "        if 'mlpubd_it' in row and pd.notna(row['mlpubd_it']):\n",
    "            dev_score += row['mlpubd_it'] * 0.5  # Public domain\n",
    "        return dev_score\n",
    "\n",
    "def calculate_ai_evaluation_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI evaluation score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI evaluation score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        eval_score = base_score\n",
    "        # For model accuracy (MLACCU)\n",
    "        if row['mlaccu_it'] is None:\n",
    "            eval_score += 0\n",
    "        elif row['mlaccu_it'] == 1:  # All models\n",
    "            eval_score += 1\n",
    "        elif row['mlaccu_it'] == 2:  # Most models\n",
    "            eval_score += 0.75\n",
    "        elif row['mlaccu_it'] == 3:  # Some models\n",
    "            eval_score += 0.5\n",
    "        elif row['mlaccu_it'] == 4:  # Few models\n",
    "            eval_score += 0.25\n",
    "        # For None (5) or Do not know (6), no points added\n",
    "    \n",
    "    # For model bias (MLBIAS)\n",
    "        if row['mlbias_it'] is None:\n",
    "            eval_score += 0\n",
    "        elif row['mlbias_it'] == 1:  # All models\n",
    "            eval_score += 1\n",
    "        elif row['mlbias_it'] == 2:  # Most models\n",
    "            eval_score += 0.75\n",
    "        elif row['mlbias_it'] == 3:  # Some models\n",
    "            eval_score += 0.5\n",
    "        elif row['mlbias_it'] == 4:  # Few models\n",
    "            eval_score += 0.25\n",
    "        # For None (5) or Do not know (6), no points added\n",
    "    \n",
    "        return eval_score\n",
    "\n",
    "def calculate_all_ai_scores_row(row):\n",
    "    \"\"\"\n",
    "    Calculate all AI/ML implementation scores as continuous measures for a single row.\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with all calculated scores\n",
    "    \"\"\"\n",
    "    # Calculate all scores\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    breadth_score = calculate_ai_implementation_breadth_row(row)\n",
    "    dev_score = calculate_ai_development_row(row)\n",
    "    eval_score = calculate_ai_evaluation_row(row)\n",
    "    \n",
    "    return {\n",
    "        'ai_base_score': base_score,\n",
    "        'ai_base_breadth_score': breadth_score,\n",
    "        'ai_base_dev_score': dev_score,\n",
    "        'ai_base_eval_score': eval_score\n",
    "    }\n",
    "\n",
    "def apply_ai_scores_to_dataframe(df):\n",
    "    \"\"\"\n",
    "    Apply all AI score calculations row by row to a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: A pandas DataFrame with hospital data\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with added AI score columns\n",
    "    \"\"\"\n",
    "    # Initialize empty columns for scores\n",
    "    df['ai_base_score'] = float('nan')\n",
    "    df['ai_base_breadth_score'] = float('nan')\n",
    "    df['ai_base_dev_score'] = float('nan')\n",
    "    df['ai_base_eval_score'] = float('nan')\n",
    "    \n",
    "    # Apply row by row calculations\n",
    "    for index, row in df.iterrows():\n",
    "        scores = calculate_all_ai_scores_row(row)\n",
    "        for score_name, score_value in scores.items():\n",
    "            df.at[index, score_name] = score_value\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C3_0_3 load processed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_master = pd.read_csv('./data/AHA_master_external_data.csv', low_memory=False)\n",
    "AHA_IT = AHA_master[~AHA_master.id_it.isnull()]\n",
    "AHA_master2 = apply_ai_scores_to_dataframe(AHA_IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SHAPE_RESTORE_SHX'] = 'YES'\n",
    "states = gpd.read_file('../../../data/map_data/state_boundary.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C3_1 prepare dataframe for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing or invalid coordinates\n",
    "AHA_master2 = AHA_master2.dropna(subset=['latitude_address', 'longitude_address'])\n",
    "\n",
    "# Filter out invalid coordinates\n",
    "valid_coords = (\n",
    "    (AHA_master['latitude_address'] != 0) & \n",
    "    (AHA_master['longitude_address'] != 0) &\n",
    "    (AHA_master['latitude_address'] >= -90) & \n",
    "    (AHA_master['latitude_address'] <= 90) &\n",
    "    (AHA_master['longitude_address'] >= -180) & \n",
    "    (AHA_master['longitude_address'] <= 180)\n",
    ")\n",
    "AHA_master2 = AHA_master2[valid_coords]\n",
    "\n",
    "\n",
    "# Create GeoDataFrame\n",
    "hospitals = gpd.GeoDataFrame(\n",
    "    AHA_master2, \n",
    "    geometry=gpd.points_from_xy(AHA_master2.longitude_address, AHA_master2.latitude_address),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter hospitals with valid coordinates and implementation scores\n",
    "valid_hospitals = hospitals.dropna(subset=['longitude_address', 'latitude_address', 'aipred_it'])\n",
    "valid_geo_hospitals = hospitals.dropna(subset=['longitude_address', 'latitude_address'])\n",
    "# Create a GeoDataFrame\n",
    "hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_hospitals.longitude_address, valid_hospitals.latitude_address),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "geo_hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_geo_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_geo_hospitals.longitude_address, valid_geo_hospitals.latitude_address),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a projected CRS for accurate distance calculations\n",
    "hospitals_gdf_projected = hospitals_gdf.to_crs(epsg=3857) # projected coordinate system using flat, 2D plane to represent Earth's surface \n",
    "geo_hospitals_gdf_projected = geo_hospitals_gdf.to_crs(epsg=3857) # projected coordinate system using flat, 2D plane to represent Earth's surface \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add census division column to the dataframe\n",
    "hospitals_gdf_projected['division'] = hospitals_gdf_projected['mstate_it'].map(state_to_division)\n",
    "geo_hospitals_gdf_projected['division'] = geo_hospitals_gdf_projected['mstate_it'].map(state_to_division)\n",
    "\n",
    "# Loop through each census division and create a heatmap\n",
    "divisions = [\n",
    "    'New England', 'Mid Atlantic', 'South Atlantic', \n",
    "    'East North Central', 'East South Central', 'West North Central',\n",
    "    'West South Central', 'Mountain', 'Pacific'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C3_2 load hotspot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_gi_star(gdf, value_column, k=5):\n",
    "    \"\"\"\n",
    "    Calculate Getis-Ord Gi* statistic for hotspot analysis\n",
    "    \n",
    "    Parameters:\n",
    "    gdf: GeoDataFrame with spatial data\n",
    "    value_column: Column name for analysis\n",
    "    k: Number of nearest neighbors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract coordinates and values\n",
    "    coords = np.vstack((gdf.geometry.x, gdf.geometry.y)).T\n",
    "    values = gdf[value_column].values\n",
    "    n = len(values)\n",
    "    \n",
    "    # Handle missing values\n",
    "    valid_mask = ~np.isnan(values)\n",
    "    if not np.all(valid_mask):\n",
    "        coords = coords[valid_mask]\n",
    "        values = values[valid_mask]\n",
    "        gdf = gdf[valid_mask].copy()\n",
    "        n = len(values)\n",
    "    \n",
    "    if n <= k:\n",
    "        return gdf\n",
    "    \n",
    "    # Create k-nearest neighbors spatial weights (including self for Gi*)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='auto').fit(coords)\n",
    "    distances, indices = nbrs.kneighbors(coords)\n",
    "    \n",
    "    # Create binary weights matrix\n",
    "    W = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        neighbors = indices[i, :]\n",
    "        W[i, neighbors] = 1\n",
    "    \n",
    "    # Global statistics\n",
    "    x_bar = np.mean(values)\n",
    "    s_squared = np.var(values, ddof=1)\n",
    "    \n",
    "    # Calculate Gi* for each location\n",
    "    gi_star = np.zeros(n)\n",
    "    z_scores = np.zeros(n)\n",
    "    p_values = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Calculate Gi* statistic\n",
    "        gi_star[i] = np.sum(W[i, :] * values)\n",
    "        \n",
    "        # Sum of weights\n",
    "        wi_sum = np.sum(W[i, :])\n",
    "        \n",
    "        # Expected value under null hypothesis\n",
    "        expected_gi = wi_sum * x_bar\n",
    "        \n",
    "        # Variance using Getis-Ord formula\n",
    "        variance_gi = (wi_sum * s_squared * (n - wi_sum)) / (n - 1)\n",
    "        \n",
    "        if variance_gi > 0:\n",
    "            # Standardized z-score\n",
    "            z_scores[i] = (gi_star[i] - expected_gi) / np.sqrt(variance_gi)\n",
    "            # Two-tailed p-value\n",
    "            p_values[i] = 2 * (1 - stats.norm.cdf(abs(z_scores[i])))\n",
    "        else:\n",
    "            z_scores[i] = 0\n",
    "            p_values[i] = 1.0\n",
    "    \n",
    "    # Create result dataframe\n",
    "    result_gdf = gdf.copy()\n",
    "    result_gdf['gi_star'] = gi_star\n",
    "    result_gdf['z_score'] = z_scores\n",
    "    result_gdf['p_value'] = p_values\n",
    "    \n",
    "    # Classify hotspots and coldspots\n",
    "    result_gdf['hotspot_type'] = 'Not Significant'\n",
    "    \n",
    "    # 99% confidence\n",
    "    result_gdf.loc[(z_scores > 0) & (p_values <= 0.01), 'hotspot_type'] = 'Hotspot (99%)'\n",
    "    result_gdf.loc[(z_scores < 0) & (p_values <= 0.01), 'hotspot_type'] = 'Coldspot (99%)'\n",
    "    \n",
    "    # 95% confidence\n",
    "    result_gdf.loc[(z_scores > 0) & (p_values > 0.01) & (p_values <= 0.05), 'hotspot_type'] = 'Hotspot (95%)'\n",
    "    result_gdf.loc[(z_scores < 0) & (p_values > 0.01) & (p_values <= 0.05), 'hotspot_type'] = 'Coldspot (95%)'\n",
    "    \n",
    "    # 90% confidence\n",
    "    result_gdf.loc[(z_scores > 0) & (p_values > 0.05) & (p_values <= 0.1), 'hotspot_type'] = 'Hotspot (90%)'\n",
    "    result_gdf.loc[(z_scores < 0) & (p_values > 0.05) & (p_values <= 0.1), 'hotspot_type'] = 'Coldspot (90%)'\n",
    "    \n",
    "    return result_gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3_3 run hotspot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Conduct hotspot analysis for the entire US\n",
    "print(\"Performing hotspot analysis for the entire US...\")\n",
    "base_hotspot_results = calculate_gi_star(hospitals_gdf_projected, 'ai_base_score', k=5)\n",
    "breadth_hotspot_results = calculate_gi_star(hospitals_gdf_projected, 'ai_base_breadth_score', k=5)\n",
    "dev_hotspot_results = calculate_gi_star(hospitals_gdf_projected, 'ai_base_dev_score', k=5)\n",
    "eval_hotspot_results = calculate_gi_star(hospitals_gdf_projected, 'ai_base_eval_score', k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsa_gdf = gpd.read_file('../data/HsaBdry_AK_HI_unmodified.geojson')\n",
    "# Ensure CRS matches\n",
    "hsa_gdf = hsa_gdf.to_crs(hospitals_gdf_projected.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hsa version stat \n",
    "base_hsa_status = base_hotspot_results.groupby('hsacode_as')['hotspot_type'] \\\n",
    "    .agg(lambda x: x.value_counts().idxmax()) \\\n",
    "    .reset_index(name='hsa_hotspot_type')\n",
    "\n",
    "breadth_hsa_status = breadth_hotspot_results.groupby('hsacode_as')['hotspot_type'] \\\n",
    "    .agg(lambda x: x.value_counts().idxmax()) \\\n",
    "    .reset_index(name='hsa_hotspot_type')\n",
    "\n",
    "dev_hsa_status = dev_hotspot_results.groupby('hsacode_as')['hotspot_type'] \\\n",
    "    .agg(lambda x: x.value_counts().idxmax()) \\\n",
    "    .reset_index(name='hsa_hotspot_type')\n",
    "\n",
    "eval_hsa_status = eval_hotspot_results.groupby('hsacode_as')['hotspot_type'] \\\n",
    "    .agg(lambda x: x.value_counts().idxmax()) \\\n",
    "    .reset_index(name='hsa_hotspot_type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_hsa_gdf = hsa_gdf.merge(base_hsa_status, left_on = 'HSA93', right_on = 'hsacode_as', how = 'left')\n",
    "breadth_hsa_gdf = hsa_gdf.merge(breadth_hsa_status, left_on = 'HSA93', right_on = 'hsacode_as', how = 'left')\n",
    "dev_hsa_gdf = hsa_gdf.merge(dev_hsa_status, left_on = 'HSA93', right_on = 'hsacode_as', how = 'left')\n",
    "eval_hsa_gdf = hsa_gdf.merge(eval_hsa_status, left_on = 'HSA93', right_on = 'hsacode_as', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3_4 get stat of hotspot visualize hotspot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"ai base score hotspots stats\")\n",
    "base_df_counts = pd.DataFrame(base_hotspot_results.hotspot_type.value_counts()).reset_index()\n",
    "base_df_counts.columns = ['Hotspot Type', 'Count']\n",
    "base_df_counts['Percentage'] = (base_df_counts['Count'] / base_df_counts['Count'].sum() * 100).round(2)\n",
    "base_df_counts.sort_values('Hotspot Type', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"ai base score hsa hotspots stats\")\n",
    "base_hsa_df_counts = pd.DataFrame(base_hsa_gdf.hsa_hotspot_type.value_counts()).reset_index()\n",
    "base_hsa_df_counts.columns = ['Hotspot Type', 'Count']\n",
    "base_hsa_df_counts['Percentage'] = (base_hsa_df_counts['Count'] / base_hsa_df_counts['Count'].sum() * 100).round(2)\n",
    "base_hsa_df_counts.sort_values('Hotspot Type', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"ai base breadth score hotspots stats\")\n",
    "breadth_df_counts = pd.DataFrame(breadth_hotspot_results.hotspot_type.value_counts()).reset_index()\n",
    "breadth_df_counts.columns = ['Hotspot Type', 'Count']\n",
    "breadth_df_counts['Percentage'] = (breadth_df_counts['Count'] / breadth_df_counts['Count'].sum() * 100).round(2)\n",
    "breadth_df_counts\n",
    "breadth_df_counts.sort_values('Hotspot Type', ascending=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"ai base breadth score hsa hotspots stats\")\n",
    "breadth_hsa_df_counts = pd.DataFrame(breadth_hsa_gdf.hsa_hotspot_type.value_counts()).reset_index()\n",
    "breadth_hsa_df_counts.columns = ['Hotspot Type', 'Count']\n",
    "breadth_hsa_df_counts['Percentage'] = (breadth_hsa_df_counts['Count'] / breadth_hsa_df_counts['Count'].sum() * 100).round(2)\n",
    "breadth_hsa_df_counts.sort_values('Hotspot Type', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"ai base dev score hotspots stats\")\n",
    "dev_df_counts = pd.DataFrame(dev_hotspot_results.hotspot_type.value_counts()).reset_index()\n",
    "dev_df_counts.columns = ['Hotspot Type', 'Count']\n",
    "dev_df_counts['Percentage'] = (dev_df_counts['Count'] / dev_df_counts['Count'].sum() * 100).round(2)\n",
    "dev_df_counts.sort_values('Hotspot Type', ascending=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"ai base dev score hsa hotspots stats\")\n",
    "dev_hsa_df_counts = pd.DataFrame(dev_hsa_gdf.hsa_hotspot_type.value_counts()).reset_index()\n",
    "dev_hsa_df_counts.columns = ['Hotspot Type', 'Count']\n",
    "dev_hsa_df_counts['Percentage'] = (breadth_hsa_df_counts['Count'] / breadth_hsa_df_counts['Count'].sum() * 100).round(2)\n",
    "dev_hsa_df_counts.sort_values('Hotspot Type', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"ai base eval score hotspots stats\")\n",
    "eval_df_counts = pd.DataFrame(eval_hotspot_results.hotspot_type.value_counts()).reset_index()\n",
    "eval_df_counts.columns = ['Hotspot Type', 'Count']\n",
    "eval_df_counts['Percentage'] = (eval_df_counts['Count'] / eval_df_counts['Count'].sum() * 100).round(2)\n",
    "eval_df_counts.sort_values('Hotspot Type', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"ai base eval score hsa hotspots stats\")\n",
    "eval_hsa_df_counts = pd.DataFrame(eval_hsa_gdf.hsa_hotspot_type.value_counts()).reset_index()\n",
    "eval_hsa_df_counts.columns = ['Hotspot Type', 'Count']\n",
    "eval_hsa_df_counts['Percentage'] = (eval_hsa_df_counts['Count'] / eval_hsa_df_counts['Count'].sum() * 100).round(2)\n",
    "eval_hsa_df_counts.sort_values('Hotspot Type', ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3_4 visualize hotspot  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3_4_1 ai_base_score hotspot visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hotspot Visualization\n",
    "\n",
    "# Define color scheme\n",
    "hotspot_colors = {\n",
    "    'Hotspot (99%)': '#001f4d',      # Dark blue\n",
    "    'Hotspot (95%)': '#0050b3',      # Medium blue\n",
    "    'Hotspot (90%)': '#4d94ff',      # Light blue\n",
    "    'Not Significant': '#d9d9d9',    # Light gray\n",
    "    'Coldspot (90%)': '#bfbfbf',     # Medium gray\n",
    "    'Coldspot (95%)': '#737373',     # Dark gray\n",
    "    'Coldspot (99%)': '#333333'      # Very dark gray\n",
    "}\n",
    "\n",
    "# Set opacity levels\n",
    "opacity = {\n",
    "    'Hotspot (99%)': 0.9,\n",
    "    'Hotspot (95%)': 0.8,\n",
    "    'Hotspot (90%)': 0.7,\n",
    "    'Not Significant': 0.1,\n",
    "    'Coldspot (90%)': 0.7,\n",
    "    'Coldspot (95%)': 0.8,\n",
    "    'Coldspot (99%)': 0.9\n",
    "}\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot non-significant points first (background)\n",
    "non_sig = hotspot_results[hotspot_results['hotspot_type'] == 'Not Significant']\n",
    "non_sig.plot(\n",
    "    ax=ax,\n",
    "    color=hotspot_colors['Not Significant'],\n",
    "    markersize=15,\n",
    "    alpha=opacity['Not Significant']\n",
    ")\n",
    "\n",
    "# Plot significant hotspots and coldspots\n",
    "significant_types = ['Coldspot (99%)', 'Coldspot (95%)', 'Coldspot (90%)', \n",
    "                    'Hotspot (90%)', 'Hotspot (95%)', 'Hotspot (99%)']\n",
    "\n",
    "for hotspot_type in significant_types:\n",
    "    subset = hotspot_results[hotspot_results['hotspot_type'] == hotspot_type]\n",
    "    if len(subset) > 0:\n",
    "        subset.plot(\n",
    "            ax=ax,\n",
    "            color=hotspot_colors[hotspot_type],\n",
    "            markersize=25,\n",
    "            alpha=opacity[hotspot_type]\n",
    "        )\n",
    "\n",
    "# Add basemap if available\n",
    "try:\n",
    "    import contextily as ctx\n",
    "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Hotspot (99%)'], \n",
    "              label='Hotspot (99%)', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Hotspot (95%)'], \n",
    "              label='Hotspot (95%)', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Hotspot (90%)'], \n",
    "              label='Hotspot (90%)', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Not Significant'], \n",
    "              label='Not Significant', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Coldspot (90%)'], \n",
    "              label='Coldspot (90%)', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Coldspot (95%)'], \n",
    "              label='Coldspot (95%)', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Coldspot (99%)'], \n",
    "              label='Coldspot (99%)', markersize=10)\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "ax.set_axis_off()\n",
    "ax.set_title('Hotspots and Coldspots of AI Implementation', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Hotspot Analysis Summary:\")\n",
    "for hotspot_type in hotspot_colors.keys():\n",
    "    count = len(hotspot_results[hotspot_results['hotspot_type'] == hotspot_type])\n",
    "    print(f\"{hotspot_type}: {count} locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject everything to EPSG:3857 for basemap compatibility\n",
    "base_hsa_gdf = base_hsa_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Define colors\n",
    "custom_colors = {\n",
    "    'Hotspot (90%)': '#4d94ff',\n",
    "    'Hotspot (95%)': '#0050b3',\n",
    "    'Hotspot (99%)': '#001f4d',\n",
    "    'Coldspot (90%)': '#a6a6a6',\n",
    "    'Coldspot (95%)': '#595959',\n",
    "    'Coldspot (99%)': '#262626',\n",
    "    'Not Significant': '#f2f2f2'  # very light gray\n",
    "}\n",
    "\n",
    "# Fill missing values as 'Not Significant'\n",
    "base_hsa_gdf['hsa_hotspot_type'] = base_hsa_gdf['hsa_hotspot_type'].fillna('Not Significant')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(13, 11))\n",
    "\n",
    "for category, color in custom_colors.items():\n",
    "    subset = base_hsa_gdf[base_hsa_gdf['hsa_hotspot_type'] == category]\n",
    "    subset.plot(ax=ax, color=color, label=category, edgecolor='black', linewidth=0.2)\n",
    "\n",
    "# Add basemap\n",
    "#ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "\n",
    "ax.set_title('Hotspots and Coldspots of ML Implementation by Hospital Service Area', fontsize=15)\n",
    "ax.set_axis_off()\n",
    "ax.legend(title='HSA Classification', loc='upper right', frameon=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure if needed\n",
    "fig.savefig('figures/base_hsa_hotspot_map.pdf', \n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0.1,\n",
    "            facecolor='white',\n",
    "            edgecolor='none',\n",
    "            format='pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C3_4_2 secondary scores hotspot visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define colors for hotspot types - purples for hotspots, grays for coldspots\n",
    "secondary_hotspot_colors = {\n",
    "    'Hotspot (99%)': '#4a1486',      # Very dark purple\n",
    "    'Hotspot (95%)': '#807dba',      # Medium purple\n",
    "    'Hotspot (90%)': '#bcbddc',      # Light purple\n",
    "    'Not Significant': '#f0f0f0',    # Very light gray\n",
    "    'Coldspot (90%)': '#bdbdbd',     # Light gray\n",
    "    'Coldspot (95%)': '#636363',     # Medium gray\n",
    "    'Coldspot (99%)': '#252525'      # Dark gray\n",
    "}\n",
    "\n",
    "# Create custom color map for z-scores\n",
    "secondary_hotspot_cmap = LinearSegmentedColormap.from_list(\n",
    "    'eval_hotspot_cmap', \n",
    "    ['#252525', '#636363', '#bdbdbd', '#f0f0f0', '#bcbddc', '#807dba', '#4a1486']\n",
    ")\n",
    "\n",
    "# Set the opacity values\n",
    "opacity = {\n",
    "    'Hotspot (99%)': 0.9,\n",
    "    'Hotspot (95%)': 0.8,\n",
    "    'Hotspot (90%)': 0.7,\n",
    "    'Not Significant': 0.1,  # Very low opacity for non-significant points\n",
    "    'Coldspot (90%)': 0.7,\n",
    "    'Coldspot (95%)': 0.8,\n",
    "    'Coldspot (99%)': 0.9\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a single figure\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# MAIN PLOT - HOTSPOTS\n",
    "# First plot non-significant points with very low opacity\n",
    "non_sig = breadth_hotspot_results[breadth_hotspot_results['hotspot_type'] == 'Not Significant']\n",
    "non_sig.plot(\n",
    "    ax=ax,\n",
    "    color=secondary_hotspot_colors['Not Significant'],\n",
    "    markersize=15,\n",
    "    alpha=opacity['Not Significant']\n",
    ")\n",
    "\n",
    "for hotspot_type in ['Coldspot (90%)', 'Coldspot (95%)', 'Coldspot (99%)', \n",
    "                     'Hotspot (90%)', 'Hotspot (95%)', 'Hotspot (99%)']:\n",
    "    subset = breadth_hotspot_results[breadth_hotspot_results['hotspot_type'] == hotspot_type]\n",
    "    subset.plot(\n",
    "        ax=ax,\n",
    "        color=secondary_hotspot_colors[hotspot_type],\n",
    "        markersize=25,\n",
    "        alpha=opacity[hotspot_type]\n",
    "    )\n",
    "\n",
    "# Create legend for hotspot plot\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Hotspot (99%)'], \n",
    "              label='Hotspot (99% confidence)', markersize=10, alpha=opacity['Hotspot (99%)']),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Hotspot (95%)'], \n",
    "              label='Hotspot (95% confidence)', markersize=10, alpha=opacity['Hotspot (95%)']),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Hotspot (90%)'], \n",
    "              label='Hotspot (90% confidence)', markersize=10, alpha=opacity['Hotspot (90%)']),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Not Significant'], \n",
    "              label='Not Significant', markersize=10, alpha=0.5),  # Higher opacity in legend for visibility\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Coldspot (90%)'], \n",
    "              label='Coldspot (90% confidence)', markersize=10, alpha=opacity['Coldspot (90%)']),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Coldspot (95%)'], \n",
    "              label='Coldspot (95% confidence)', markersize=10, alpha=opacity['Coldspot (95%)']),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Coldspot (99%)'], \n",
    "              label='Coldspot (99% confidence)', markersize=10, alpha=opacity['Coldspot (99%)'])\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_elements, loc='upper right', frameon=True)\n",
    "ax.set_title('Hotspots and Coldspots of ML Implementation in US Hospitals', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject everything to EPSG:3857 for basemap compatibility\n",
    "breadth_hsa_gdf = breadth_hsa_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Fill missing values as 'Not Significant'\n",
    "breadth_hsa_gdf['hsa_hotspot_type'] = breadth_hsa_gdf['hsa_hotspot_type'].fillna('Not Significant')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(13, 11))\n",
    "\n",
    "for category, color in secondary_hotspot_colors.items():\n",
    "    subset = breadth_hsa_gdf[breadth_hsa_gdf['hsa_hotspot_type'] == category]\n",
    "    subset.plot(ax=ax, color=color, label=category, edgecolor='black', linewidth=0.2)\n",
    "\n",
    "\n",
    "ax.set_title('Hotspots and Coldspots of ML Implementation by Hospital Service Area', fontsize=15)\n",
    "ax.set_axis_off()\n",
    "ax.legend(title='HSA Classification', loc='upper right', frameon=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
