{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3. Hotspot National Analysis\n",
    "\n",
    "**Description**  \n",
    "This section identify statistically significant hotspots and coldspots of hospital AI adoption across the United States. Using Getis-Ord Gi* and Local Moran's I statistics, this analysis pinpoints specific geographic locations where AI adoption is significantly higher (hotspots) or lower (coldspots) \n",
    "\n",
    "**Purpose**  \n",
    "To identify hotspots and coldspots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. load necessary libraries, functions and preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_master = pd.read_csv('./data/AHA_master_external_data.csv', low_memory=False)\n",
    "AHA_IT = AHA_master[~AHA_master.id_it.isnull()]\n",
    "AHA_master2 = apply_ai_scores_to_dataframe(AHA_IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SHAPE_RESTORE_SHX'] = 'YES'\n",
    "states = gpd.read_file('../../../data/map_data/state_boundary.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out invalid coordinates\n",
    "valid_coords = (\n",
    "    (AHA_master['lat_as'] != 0) & \n",
    "    (AHA_master['long_as'] != 0) &\n",
    "    (AHA_master['lat_as'] >= -90) & \n",
    "    (AHA_master['lat_as'] <= 90) &\n",
    "    (AHA_master['long_as'] >= -180) & \n",
    "    (AHA_master['long_as'] <= 180)\n",
    ")\n",
    "AHA_master2 = AHA_master2[valid_coords]\n",
    "\n",
    "\n",
    "# Create GeoDataFrame\n",
    "hospitals = gpd.GeoDataFrame(\n",
    "    AHA_master2, \n",
    "    geometry=gpd.points_from_xy(AHA_master2.long_as, AHA_master2.lat_as),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter hospitals with valid coordinates and implementation scores\n",
    "valid_hospitals = hospitals.dropna(subset=['long_as', 'lat_as', 'aipred_it'])\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "geo_hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_hospitals.long_as, valid_hospitals.lat_as),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n",
    "geo_hospitals_gdf_projected = geo_hospitals_gdf.to_crs(epsg=3857) # projected coordinate system using flat, 2D plane to represent Earth's surface \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "geo_hospitals_gdf_projected['division'] = geo_hospitals_gdf_projected['mstate_it'].map(state_to_division)\n",
    "\n",
    "# Loop through each census division and create a heatmap\n",
    "divisions = [\n",
    "    'New England', 'Mid Atlantic', 'South Atlantic', \n",
    "    'East North Central', 'East South Central', 'West North Central',\n",
    "    'West South Central', 'Mountain', 'Pacific'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 hotspot function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# ========= Storey q-value helpers =========\n",
    "def storey_qvalue_python(pvals, lambdas=None):\n",
    "    \"\"\"\n",
    "    Simple Storey q-value fallback in pure Python.\n",
    "    \"\"\"\n",
    "    p = np.asarray(pvals, dtype=float)\n",
    "    m = p.size\n",
    "    if m == 0:\n",
    "        return p\n",
    "    if lambdas is None:\n",
    "        lambdas = np.arange(0.05, 0.95, 0.05)\n",
    "    pi0_vals = []\n",
    "    for lam in lambdas:\n",
    "        denom = 1.0 - lam\n",
    "        if denom <= 0:\n",
    "            continue\n",
    "        pi0_vals.append((p > lam).mean() / denom)\n",
    "    pi0 = min(1.0, np.min(pi0_vals)) if len(pi0_vals) else 1.0\n",
    "    order = np.argsort(p)\n",
    "    p_sorted = p[order]\n",
    "    q_sorted = pi0 * m * p_sorted / (np.arange(1, m + 1))\n",
    "    q_sorted = np.minimum.accumulate(q_sorted[::-1])[::-1]\n",
    "    q = np.empty_like(q_sorted)\n",
    "    q[order] = q_sorted\n",
    "    return np.clip(q, 0, 1)\n",
    "\n",
    "\n",
    "# ========= Classification =========\n",
    "def _classify_levels(sign, p_like):\n",
    "    \"\"\"\n",
    "    Map sign and p-like values to hotspot classes at 99, 95, 90 percent levels.\n",
    "    p_like can be unadjusted p, Bonferroni p, BH-FDR p, or Storey q.\n",
    "    \"\"\"\n",
    "    lab = np.array([\"Not Significant\"] * len(p_like), dtype=object)\n",
    "    lab[(sign > 0) & (p_like <= 0.01)] = \"Hotspot (99%)\"\n",
    "    lab[(sign < 0) & (p_like <= 0.01)] = \"Coldspot (99%)\"\n",
    "    lab[(sign > 0) & (p_like > 0.01) & (p_like <= 0.05)] = \"Hotspot (95%)\"\n",
    "    lab[(sign < 0) & (p_like > 0.01) & (p_like <= 0.05)] = \"Coldspot (95%)\"\n",
    "    lab[(sign > 0) & (p_like > 0.05) & (p_like <= 0.1)] = \"Hotspot (90%)\"\n",
    "    lab[(sign < 0) & (p_like > 0.05) & (p_like <= 0.1)] = \"Coldspot (90%)\"\n",
    "    return lab\n",
    "\n",
    "# ========= Core Gi* function =========\n",
    "def calculate_gi_star_all(gdf, value_column, k=6):\n",
    "    \"\"\"\n",
    "    Compute local Getis-Ord Gi* with k-NN (including self) and multiple testing corrections.\n",
    "\n",
    "    Inputs\n",
    "    - gdf: GeoDataFrame with Point geometry (prefer EPSG:5070)\n",
    "    - value_column: column to analyze (numeric)\n",
    "    - k: number of nearest neighbors (self included via k+1)\n",
    "\n",
    "    Outputs\n",
    "    Returns a copy of gdf (rows with non-null value) with these columns:\n",
    "    - gi_star: raw Gi* sum over neighbors\n",
    "    - z_score: standardized Gi* Z\n",
    "    - p_unadj: unadjusted two-sided p\n",
    "    - p_bonf: Bonferroni adjusted p\n",
    "    - p_bh: BH-FDR adjusted p\n",
    "    - q_storey: Storey q-value\n",
    "    - hotspot_unadj: class from p_unadj\n",
    "    - hotspot_bonf: class from p_bonf\n",
    "    - hotspot_bh: class from p_bh\n",
    "    - hotspot_storey: class from q_storey\n",
    "    \"\"\"\n",
    "    # coords and values\n",
    "    coords = np.vstack((gdf.geometry.x, gdf.geometry.y)).T\n",
    "    values = gdf[value_column].to_numpy(dtype=float)\n",
    "    valid = ~np.isnan(values)\n",
    "    coords, values = coords[valid], values[valid]\n",
    "    out = gdf.loc[valid].copy()\n",
    "    n = len(values)\n",
    "    if n <= k:\n",
    "        print(f\"Error: Not enough observations ({n}) for k={k}\")\n",
    "        return gdf\n",
    "\n",
    "    # kNN including self (k+1)\n",
    "    nn = NearestNeighbors(n_neighbors=k+1)\n",
    "    nn.fit(coords)\n",
    "    _, indices = nn.kneighbors(coords)  # shape (n, k+1)\n",
    "\n",
    "    # global stats (Ord & Getis, 1995)\n",
    "    x_bar = values.mean()\n",
    "    S = np.sqrt((np.sum(values**2) / n) - x_bar**2)\n",
    "\n",
    "    # Gi*, Z, p\n",
    "    Gs = np.zeros(n)\n",
    "    Zs = np.zeros(n)\n",
    "    Ps = np.ones(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        neigh = indices[i]             # length k+1\n",
    "        wi_sum = float(len(neigh))     # sum w_ij (binary)\n",
    "        wi2_sum = wi_sum               # sum w_ij^2 (binary)\n",
    "        Gs[i] = np.sum(values[neigh])  # observed sum\n",
    "        EGi = x_bar * wi_sum           # expected\n",
    "        varGi = (S**2) * ((n * wi2_sum - wi_sum**2) / (n - 1)) if n > 1 else 0.0\n",
    "        if varGi > 0 and not np.isnan(varGi):\n",
    "            Zs[i] = (Gs[i] - EGi) / np.sqrt(varGi)\n",
    "            Ps[i] = 2 * (1 - stats.norm.cdf(abs(Zs[i])))\n",
    "        else:\n",
    "            Zs[i] = 0.0\n",
    "            Ps[i] = 1.0\n",
    "\n",
    "    # multiple testing corrections\n",
    "    m = n\n",
    "    p_unadj = Ps\n",
    "    p_bonf = np.clip(p_unadj * m, 0, 1)\n",
    "    _, p_bh, _, _ = multipletests(p_unadj, alpha=0.05, method='fdr_bh')\n",
    "    q_storey = storey_qvalue_python(p_unadj)\n",
    "\n",
    "    # labels\n",
    "    sign = np.sign(Zs)\n",
    "    out['gi_star'] = Gs\n",
    "    out['z_score'] = Zs\n",
    "    out['p_unadj'] = p_unadj\n",
    "    out['p_bonf'] = p_bonf\n",
    "    out['p_bh'] = p_bh\n",
    "    out['q_storey'] = q_storey\n",
    "    out['hotspot_unadj'] = _classify_levels(sign, p_unadj)\n",
    "    out['hotspot_bonf'] = _classify_levels(sign, p_bonf)\n",
    "    out['hotspot_bh'] = _classify_levels(sign, p_bh)\n",
    "    out['hotspot_storey'] = _classify_levels(sign, q_storey)\n",
    "\n",
    "    # column annotations for reference\n",
    "    out.attrs[\"columns_doc\"] = {\n",
    "        \"gi_star\": \"raw Gi* statistic (Ord & Getis, 1995)\",\n",
    "        \"z_score\": \"standardized Z-score for Gi*\",\n",
    "        \"p_unadj\": \"unadjusted two-sided p-value\",\n",
    "        \"p_bonf\": \"Bonferroni-adjusted p-value\",\n",
    "        \"p_bh\": \"Benjamini-Hochberg FDR-adjusted p-value\",\n",
    "        \"q_storey\": \"Storey q-value (FDR with pi0 estimated)\",\n",
    "        \"hotspot_unadj\": \"hotspot class from unadjusted p\",\n",
    "        \"hotspot_bonf\": \"hotspot class from Bonferroni p\",\n",
    "        \"hotspot_bh\": \"hotspot class from BH-FDR p\",\n",
    "        \"hotspot_storey\": \"hotspot class from Storey q-value\"\n",
    "    }\n",
    "\n",
    "    # quick summary\n",
    "    def _cnt(col, key): return int(out[col].str.contains(key).sum())\n",
    "    print(f\"Total locations: {n}\")\n",
    "    print(f\"Unadj  Hot:{_cnt('hotspot_unadj','Hotspot')}  Cold:{_cnt('hotspot_unadj','Coldspot')}\")\n",
    "    print(f\"BH-FDR Hot:{_cnt('hotspot_bh','Hotspot')}  Cold:{_cnt('hotspot_bh','Coldspot')}\")\n",
    "    print(f\"Storey Hot:{_cnt('hotspot_storey','Hotspot')}  Cold:{_cnt('hotspot_storey','Coldspot')}\")\n",
    "    print(f\"Bonf  Hot:{_cnt('hotspot_bonf','Hotspot')}  Cold:{_cnt('hotspot_bonf','Coldspot')}\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Run hotspot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Conduct hotspot analysis for the entire US\n",
    "print(\"Performing hotspot analysis for the entire US...\")\n",
    "base_hotspot_results = calculate_gi_star_all(geo_hospitals_gdf_projected, 'ai_base_score_imputed', k=6)\n",
    "breadth_hotspot_results = calculate_gi_star_all(geo_hospitals_gdf_projected, 'ai_base_breadth_score_imputed', k=6)\n",
    "dev_hotspot_results = calculate_gi_star_all(geo_hospitals_gdf_projected, 'ai_base_dev_score_imputed', k=6)\n",
    "eval2023_hotspot_results = calculate_gi_star_all(geo_hospitals_gdf_projected, 'ai_base_eval_score_2023_imputed', k=6)\n",
    "eval2024_hotspot_results = calculate_gi_star_all(geo_hospitals_gdf_projected, 'ai_base_eval_score_2024_imputed', k=6)\n",
    "llm_hotspot_results = calculate_gi_star_all(geo_hospitals_gdf_projected, 'llm_readiness_score', k=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsa_gdf = gpd.read_file('../data/HsaBdry_AK_HI_unmodified.geojson')\n",
    "# Ensure CRS matches\n",
    "hsa_gdf = hsa_gdf.to_crs(geo_hospitals_gdf_projected.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_hsa_status = (\n",
    "    base_hotspot_results\n",
    "      .groupby(\"hsacode_as\")[[\"hotspot_unadj\",\"hotspot_bh\",\"hotspot_storey\"]]\n",
    "      .agg(lambda x: x.value_counts().idxmax())\n",
    "      .reset_index()\n",
    "      .rename(columns={\n",
    "          \"hotspot_unadj\":\"hsa_unadj_type\",\n",
    "          \"hotspot_bh\":\"hsa_bh_type\",\n",
    "          \"hotspot_storey\":\"hsa_storey_type\"\n",
    "      })\n",
    ")\n",
    "\n",
    "breadth_hsa_status = (\n",
    "    breadth_hotspot_results\n",
    "      .groupby(\"hsacode_as\")[[\"hotspot_unadj\",\"hotspot_bh\",\"hotspot_storey\"]]\n",
    "      .agg(lambda x: x.value_counts().idxmax())\n",
    "      .reset_index()\n",
    "      .rename(columns={\n",
    "          \"hotspot_unadj\":\"hsa_unadj_type\",\n",
    "          \"hotspot_bh\":\"hsa_bh_type\",\n",
    "          \"hotspot_storey\":\"hsa_storey_type\"\n",
    "      })\n",
    ")\n",
    "\n",
    "dev_hsa_status = (\n",
    "    dev_hotspot_results\n",
    "      .groupby(\"hsacode_as\")[[\"hotspot_unadj\",\"hotspot_bh\",\"hotspot_storey\"]]\n",
    "      .agg(lambda x: x.value_counts().idxmax())\n",
    "      .reset_index()\n",
    "      .rename(columns={\n",
    "          \"hotspot_unadj\":\"hsa_unadj_type\",\n",
    "          \"hotspot_bh\":\"hsa_bh_type\",\n",
    "          \"hotspot_storey\":\"hsa_storey_type\"\n",
    "      })\n",
    ")\n",
    "\n",
    "eval2023_hsa_status = (\n",
    "    eval2023_hotspot_results\n",
    "      .groupby(\"hsacode_as\")[[\"hotspot_unadj\",\"hotspot_bh\",\"hotspot_storey\"]]\n",
    "      .agg(lambda x: x.value_counts().idxmax())\n",
    "      .reset_index()\n",
    "      .rename(columns={\n",
    "          \"hotspot_unadj\":\"hsa_unadj_type\",\n",
    "          \"hotspot_bh\":\"hsa_bh_type\",\n",
    "          \"hotspot_storey\":\"hsa_storey_type\"\n",
    "      })\n",
    ")\n",
    "eval2024_hsa_status = (\n",
    "    eval2024_hotspot_results\n",
    "      .groupby(\"hsacode_as\")[[\"hotspot_unadj\",\"hotspot_bh\",\"hotspot_storey\"]]\n",
    "      .agg(lambda x: x.value_counts().idxmax())\n",
    "      .reset_index()\n",
    "      .rename(columns={\n",
    "          \"hotspot_unadj\":\"hsa_unadj_type\",\n",
    "          \"hotspot_bh\":\"hsa_bh_type\",\n",
    "          \"hotspot_storey\":\"hsa_storey_type\"\n",
    "      })\n",
    ")\n",
    "\n",
    "llm_hsa_status = (\n",
    "    llm_hotspot_results\n",
    "      .groupby(\"hsacode_as\")[[\"hotspot_unadj\",\"hotspot_bh\",\"hotspot_storey\"]]\n",
    "      .agg(lambda x: x.value_counts().idxmax())\n",
    "      .reset_index()\n",
    "      .rename(columns={\n",
    "          \"hotspot_unadj\":\"hsa_unadj_type\",\n",
    "          \"hotspot_bh\":\"hsa_bh_type\",\n",
    "          \"hotspot_storey\":\"hsa_storey_type\"\n",
    "      })\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_hsa_gdf = hsa_gdf.merge(base_hsa_status, left_on = 'HSA93', right_on = 'hsacode_as', how = 'left')\n",
    "breadth_hsa_gdf = hsa_gdf.merge(breadth_hsa_status, left_on = 'HSA93', right_on = 'hsacode_as', how = 'left')\n",
    "dev_hsa_gdf = hsa_gdf.merge(dev_hsa_status, left_on = 'HSA93', right_on = 'hsacode_as', how = 'left')\n",
    "eval2023_hsa_gdf = hsa_gdf.merge(eval2023_hsa_status, left_on = 'HSA93', right_on = 'hsacode_as', how = 'left')\n",
    "eval2024_hsa_gdf = hsa_gdf.merge(eval2024_hsa_status, left_on = 'HSA93', right_on = 'hsacode_as', how = 'left')\n",
    "llm_hsa_gdf = hsa_gdf.merge(llm_hsa_status, left_on = 'HSA93', right_on = 'hsacode_as', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Hotspot stat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def summarize_hotspots(df, id_col=\"id_it\",\n",
    "                       value_cols=(\"hotspot_unadj\",\"hotspot_bh\",\"hotspot_storey\"),\n",
    "                       row_order=(\"Unadjusted\",\"Storey\",\"BH\")):\n",
    "    # reshape\n",
    "    long = df.melt(id_vars=id_col, value_vars=list(value_cols),\n",
    "                   var_name=\"method\", value_name=\"category\")\n",
    "    method_map = {\n",
    "        \"hotspot_unadj\": \"Unadjusted\",\n",
    "        \"hotspot_bh\": \"BH\",\n",
    "        \"hotspot_storey\": \"Storey\"\n",
    "    }\n",
    "    long[\"method\"] = long[\"method\"].map(method_map)\n",
    "\n",
    "    # parse status + level\n",
    "    def parse_category(cat):\n",
    "        if pd.isna(cat): \n",
    "            return pd.Series({\"status\":\"NotSig\", \"level\":None})\n",
    "        s = str(cat).lower()\n",
    "        if \"hotspot\" in s:\n",
    "            m = re.search(r\"(90|95|99)\", s)\n",
    "            return pd.Series({\"status\":\"Hotspot\", \"level\":m.group(1) if m else None})\n",
    "        if \"coldspot\" in s:\n",
    "            m = re.search(r\"(90|95|99)\", s)\n",
    "            return pd.Series({\"status\":\"Coldspot\", \"level\":m.group(1) if m else None})\n",
    "        return pd.Series({\"status\":\"NotSig\", \"level\":None})\n",
    "\n",
    "    parsed = long.join(long[\"category\"].apply(parse_category))\n",
    "\n",
    "    # combined label\n",
    "    parsed[\"label\"] = parsed.apply(\n",
    "        lambda x: f\"{x['status']}_{x['level']}\" if x[\"status\"] != \"NotSig\" else \"NotSig\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # pivot\n",
    "    wide = (pd.pivot_table(parsed, index=\"method\", columns=\"label\", values=id_col,\n",
    "                           aggfunc=\"count\", fill_value=0)\n",
    "              .reset_index())\n",
    "\n",
    "    # ensure consistent columns if some levels are missing\n",
    "    desired_cols = [\"method\",\n",
    "                    \"Hotspot_90\",\"Hotspot_95\",\"Hotspot_99\",\n",
    "                    \"Coldspot_90\",\"Coldspot_95\",\"Coldspot_99\",\n",
    "                    \"NotSig\"]\n",
    "    for c in desired_cols:\n",
    "        if c not in wide.columns:\n",
    "            wide[c] = 0\n",
    "    wide = wide[[c for c in desired_cols if c in wide.columns]]\n",
    "\n",
    "    # row order\n",
    "    wide = wide.set_index(\"method\").reindex(row_order).reset_index()\n",
    "\n",
    "    return wide\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def summarize_hsa_hotspots(hsa_df,\n",
    "                           id_col=\"HSA93\",\n",
    "                           value_cols=(\"hsa_unadj_type\",\"hsa_bh_type\",\"hsa_storey_type\"),\n",
    "                           row_order=(\"Unadjusted\",\"Storey\",\"BH\")):\n",
    "    # reshape to long\n",
    "    long = hsa_df.melt(id_vars=id_col, value_vars=list(value_cols),\n",
    "                       var_name=\"method\", value_name=\"category\")\n",
    "\n",
    "    # method map for HSA-level columns\n",
    "    m = {\n",
    "        \"hsa_unadj_type\": \"Unadjusted\",\n",
    "        \"hsa_bh_type\": \"BH\",\n",
    "        \"hsa_storey_type\": \"Storey\",\n",
    "        # fallbacks for hospital-level names if passed accidentally\n",
    "        \"hotspot_unadj\": \"Unadjusted\",\n",
    "        \"hotspot_bh\": \"BH\",\n",
    "        \"hotspot_storey\": \"Storey\",\n",
    "    }\n",
    "    long[\"method\"] = long[\"method\"].map(m)\n",
    "\n",
    "    # parse labels like \"90% Hotspot\", \"95% Coldspot\", \"Not Significant\"\n",
    "    def parse(cat: object):\n",
    "        if pd.isna(cat):\n",
    "            return (\"NotSig\", None)\n",
    "        s = str(cat).strip()\n",
    "        s_low = s.lower()\n",
    "        if s_low.startswith(\"not\"):\n",
    "            return (\"NotSig\", None)\n",
    "        lvl = None\n",
    "        mnum = re.search(r\"(90|95|99)\", s_low)\n",
    "        if mnum:\n",
    "            lvl = mnum.group(1)\n",
    "        if \"hotspot\" in s_low:\n",
    "            return (\"Hotspot\", lvl)\n",
    "        if \"coldspot\" in s_low:\n",
    "            return (\"Coldspot\", lvl)\n",
    "        return (\"NotSig\", None)\n",
    "\n",
    "    long[[\"status\",\"level\"]] = long[\"category\"].apply(lambda x: pd.Series(parse(x)))\n",
    "    long[\"label\"] = long.apply(\n",
    "        lambda r: f\"{r['status']}_{r['level']}\" if r[\"status\"] != \"NotSig\" else \"NotSig\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # pivot to wide counts\n",
    "    wide = (pd.pivot_table(long, index=\"method\", columns=\"label\", values=id_col,\n",
    "                           aggfunc=\"count\", fill_value=0)\n",
    "              .reset_index())\n",
    "\n",
    "    # ensure expected columns exist and ordered\n",
    "    desired_cols = [\"method\",\n",
    "                    \"Hotspot_90\",\"Hotspot_95\",\"Hotspot_99\",\n",
    "                    \"Coldspot_90\",\"Coldspot_95\",\"Coldspot_99\",\n",
    "                    \"NotSig\"]\n",
    "    for c in desired_cols:\n",
    "        if c not in wide.columns:\n",
    "            wide[c] = 0\n",
    "    wide = wide[desired_cols]\n",
    "\n",
    "    # row order\n",
    "    wide = wide.set_index(\"method\").reindex(row_order).reset_index()\n",
    "\n",
    "    # ints\n",
    "    count_cols = [c for c in desired_cols if c != \"method\"]\n",
    "    wide[count_cols] = wide[count_cols].astype(int)\n",
    "\n",
    "    return wide\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_hotspots(base_hotspot_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_hsa_hotspots(base_hsa_gdf,\n",
    "                                  id_col=\"HSA93\",\n",
    "                                  value_cols=(\"hsa_unadj_type\",\"hsa_bh_type\",\"hsa_storey_type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 Hotspot visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hotspot Visualization\n",
    "\n",
    "# Define color scheme\n",
    "hotspot_colors = {\n",
    "    'Hotspot (99%)': '#001f4d',      # Dark blue\n",
    "    'Hotspot (95%)': '#0050b3',      # Medium blue\n",
    "    'Hotspot (90%)': '#4d94ff',      # Light blue\n",
    "    'Not Significant': '#d9d9d9',    # Light gray\n",
    "    'Coldspot (90%)': '#bfbfbf',     # Medium gray\n",
    "    'Coldspot (95%)': '#737373',     # Dark gray\n",
    "    'Coldspot (99%)': '#333333'      # Very dark gray\n",
    "}\n",
    "\n",
    "# Set opacity levels\n",
    "opacity = {\n",
    "    'Hotspot (99%)': 0.9,\n",
    "    'Hotspot (95%)': 0.8,\n",
    "    'Hotspot (90%)': 0.7,\n",
    "    'Not Significant': 0.1,\n",
    "    'Coldspot (90%)': 0.7,\n",
    "    'Coldspot (95%)': 0.8,\n",
    "    'Coldspot (99%)': 0.9\n",
    "}\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot non-significant points first (background)\n",
    "non_sig = hotspot_results[hotspot_results['hotspot_type'] == 'Not Significant']\n",
    "non_sig.plot(\n",
    "    ax=ax,\n",
    "    color=hotspot_colors['Not Significant'],\n",
    "    markersize=15,\n",
    "    alpha=opacity['Not Significant']\n",
    ")\n",
    "\n",
    "# Plot significant hotspots and coldspots\n",
    "significant_types = ['Coldspot (99%)', 'Coldspot (95%)', 'Coldspot (90%)', \n",
    "                    'Hotspot (90%)', 'Hotspot (95%)', 'Hotspot (99%)']\n",
    "\n",
    "for hotspot_type in significant_types:\n",
    "    subset = hotspot_results[hotspot_results['hotspot_type'] == hotspot_type]\n",
    "    if len(subset) > 0:\n",
    "        subset.plot(\n",
    "            ax=ax,\n",
    "            color=hotspot_colors[hotspot_type],\n",
    "            markersize=25,\n",
    "            alpha=opacity[hotspot_type]\n",
    "        )\n",
    "\n",
    "# Add basemap if available\n",
    "try:\n",
    "    import contextily as ctx\n",
    "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Hotspot (99%)'], \n",
    "              label='Hotspot (99%)', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Hotspot (95%)'], \n",
    "              label='Hotspot (95%)', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Hotspot (90%)'], \n",
    "              label='Hotspot (90%)', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Not Significant'], \n",
    "              label='Not Significant', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Coldspot (90%)'], \n",
    "              label='Coldspot (90%)', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Coldspot (95%)'], \n",
    "              label='Coldspot (95%)', markersize=10),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "              markerfacecolor=hotspot_colors['Coldspot (99%)'], \n",
    "              label='Coldspot (99%)', markersize=10)\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "ax.set_axis_off()\n",
    "ax.set_title('Hotspots and Coldspots of AI Implementation', fontsize=16)\n",
    "\n",
    "plt.tight_lat()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Hotspot Analysis Summary:\")\n",
    "for hotspot_type in hotspot_colors.keys():\n",
    "    count = len(hotspot_results[hotspot_results['hotspot_type'] == hotspot_type])\n",
    "    print(f\"{hotspot_type}: {count} locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject everything to EPSG:3857 for basemap compatibility\n",
    "base_hsa_gdf = base_hsa_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Define colors\n",
    "custom_colors = {\n",
    "    'Hotspot (90%)': '#4d94ff',\n",
    "    'Hotspot (95%)': '#0050b3',\n",
    "    'Hotspot (99%)': '#001f4d',\n",
    "    'Coldspot (90%)': '#a6a6a6',\n",
    "    'Coldspot (95%)': '#595959',\n",
    "    'Coldspot (99%)': '#262626',\n",
    "    'Not Significant': '#f2f2f2'  # very light gray\n",
    "}\n",
    "\n",
    "# Fill missing values as 'Not Significant'\n",
    "base_hsa_gdf['hsa_hotspot_type'] = base_hsa_gdf['hsa_hotspot_type'].fillna('Not Significant')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(13, 11))\n",
    "\n",
    "for category, color in custom_colors.items():\n",
    "    subset = base_hsa_gdf[base_hsa_gdf['hsa_hotspot_type'] == category]\n",
    "    subset.plot(ax=ax, color=color, label=category, edgecolor='black', linewidth=0.2)\n",
    "\n",
    "# Add basemap\n",
    "#ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "\n",
    "ax.set_title('Hotspots and Coldspots of ML Implementation by Hospital Service Area', fontsize=15)\n",
    "ax.set_axis_off()\n",
    "ax.legend(title='HSA Classification', loc='upper right', frameon=True)\n",
    "plt.tight_lat()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure if needed\n",
    "fig.savefig('figures/base_hsa_hotspot_map.pdf', \n",
    "            bbox_inches='tight',\n",
    "            pad_inches=0.1,\n",
    "            facecolor='white',\n",
    "            edgecolor='none',\n",
    "            format='pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 secondary measures hotspot visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define colors for hotspot types - purples for hotspots, grays for coldspots\n",
    "secondary_hotspot_colors = {\n",
    "    'Hotspot (99%)': '#4a1486',      # Very dark purple\n",
    "    'Hotspot (95%)': '#807dba',      # Medium purple\n",
    "    'Hotspot (90%)': '#bcbddc',      # Light purple\n",
    "    'Not Significant': '#f0f0f0',    # Very light gray\n",
    "    'Coldspot (90%)': '#bdbdbd',     # Light gray\n",
    "    'Coldspot (95%)': '#636363',     # Medium gray\n",
    "    'Coldspot (99%)': '#252525'      # Dark gray\n",
    "}\n",
    "\n",
    "# Create custom color map for z-scores\n",
    "secondary_hotspot_cmap = LinearSegmentedColormap.from_list(\n",
    "    'eval_hotspot_cmap', \n",
    "    ['#252525', '#636363', '#bdbdbd', '#f0f0f0', '#bcbddc', '#807dba', '#4a1486']\n",
    ")\n",
    "\n",
    "# Set the opacity values\n",
    "opacity = {\n",
    "    'Hotspot (99%)': 0.9,\n",
    "    'Hotspot (95%)': 0.8,\n",
    "    'Hotspot (90%)': 0.7,\n",
    "    'Not Significant': 0.1,  # Very low opacity for non-significant points\n",
    "    'Coldspot (90%)': 0.7,\n",
    "    'Coldspot (95%)': 0.8,\n",
    "    'Coldspot (99%)': 0.9\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a single figure\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# MAIN PLOT - HOTSPOTS\n",
    "# First plot non-significant points with very low opacity\n",
    "non_sig = breadth_hotspot_results[breadth_hotspot_results['hotspot_type'] == 'Not Significant']\n",
    "non_sig.plot(\n",
    "    ax=ax,\n",
    "    color=secondary_hotspot_colors['Not Significant'],\n",
    "    markersize=15,\n",
    "    alpha=opacity['Not Significant']\n",
    ")\n",
    "\n",
    "for hotspot_type in ['Coldspot (90%)', 'Coldspot (95%)', 'Coldspot (99%)', \n",
    "                     'Hotspot (90%)', 'Hotspot (95%)', 'Hotspot (99%)']:\n",
    "    subset = breadth_hotspot_results[breadth_hotspot_results['hotspot_type'] == hotspot_type]\n",
    "    subset.plot(\n",
    "        ax=ax,\n",
    "        color=secondary_hotspot_colors[hotspot_type],\n",
    "        markersize=25,\n",
    "        alpha=opacity[hotspot_type]\n",
    "    )\n",
    "\n",
    "# Create legend for hotspot plot\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Hotspot (99%)'], \n",
    "              label='Hotspot (99% confidence)', markersize=10, alpha=opacity['Hotspot (99%)']),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Hotspot (95%)'], \n",
    "              label='Hotspot (95% confidence)', markersize=10, alpha=opacity['Hotspot (95%)']),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Hotspot (90%)'], \n",
    "              label='Hotspot (90% confidence)', markersize=10, alpha=opacity['Hotspot (90%)']),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Not Significant'], \n",
    "              label='Not Significant', markersize=10, alpha=0.5),  # Higher opacity in legend for visibility\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Coldspot (90%)'], \n",
    "              label='Coldspot (90% confidence)', markersize=10, alpha=opacity['Coldspot (90%)']),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Coldspot (95%)'], \n",
    "              label='Coldspot (95% confidence)', markersize=10, alpha=opacity['Coldspot (95%)']),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=secondary_hotspot_colors['Coldspot (99%)'], \n",
    "              label='Coldspot (99% confidence)', markersize=10, alpha=opacity['Coldspot (99%)'])\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_elements, loc='upper right', frameon=True)\n",
    "ax.set_title('Hotspots and Coldspots of ML Implementation in US Hospitals', fontsize=16)\n",
    "plt.tight_lat()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproject everything to EPSG:3857 for basemap compatibility\n",
    "breadth_hsa_gdf = breadth_hsa_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Fill missing values as 'Not Significant'\n",
    "breadth_hsa_gdf['hsa_hotspot_type'] = breadth_hsa_gdf['hsa_hotspot_type'].fillna('Not Significant')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(13, 11))\n",
    "\n",
    "for category, color in secondary_hotspot_colors.items():\n",
    "    subset = breadth_hsa_gdf[breadth_hsa_gdf['hsa_hotspot_type'] == category]\n",
    "    subset.plot(ax=ax, color=color, label=category, edgecolor='black', linewidth=0.2)\n",
    "\n",
    "\n",
    "ax.set_title('Hotspots and Coldspots of ML Implementation by Hospital Service Area', fontsize=15)\n",
    "ax.set_axis_off()\n",
    "ax.legend(title='HSA Classification', loc='upper right', frameon=True)\n",
    "plt.tight_lat()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
