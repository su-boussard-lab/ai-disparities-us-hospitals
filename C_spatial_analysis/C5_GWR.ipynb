{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C5. Geographically Weighted Regression \n",
    "\n",
    "**Description**  \n",
    "This section conducts geographically weighted regression between hosptial/community characteristics and AI implementation level\n",
    "\n",
    "**Purpose**  \n",
    "To model and analyze spatial relationships between hosptial/community characteristics and AI implementation level \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Load necessary libraries, functions, and pre-processed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import contextily as ctx\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from calculate_scores import create_union_aipred_row, apply_ai_scores_to_dataframe\n",
    "\n",
    "# Load data\n",
    "AHA_master = pd.read_csv('../../data/AHA_master_external_data.csv', low_memory=False)\n",
    "\n",
    "# Create aipred_it_union separately (your choice, works perfectly)\n",
    "AHA_master['aipred_it_union'] = AHA_master.apply(create_union_aipred_row, axis=1)\n",
    "\n",
    "# Use the apply function for all other scores\n",
    "AHA_IT = apply_ai_scores_to_dataframe(AHA_master)\n",
    "AHA_IT = AHA_IT[AHA_IT['id_it'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SHAPE_RESTORE_SHX'] = 'YES'\n",
    "states = gpd.read_file('../../temp_shp/cb_2018_us_state_500k.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing or invalid coordinates\n",
    "AHA_IT= AHA_IT.dropna(subset=['lat_as', 'long_as'])\n",
    "\n",
    "# Filter out invalid coordinates\n",
    "valid_coords = (\n",
    "    (AHA_master['lat_as'] != 0) & \n",
    "    (AHA_master['long_as'] != 0) &\n",
    "    (AHA_master['lat_as'] >= -90) & \n",
    "    (AHA_master['lat_as'] <= 90) &\n",
    "    (AHA_master['long_as'] >= -180) & \n",
    "    (AHA_master['long_as'] <= 180)\n",
    ")\n",
    "AHA_IT = AHA_IT[valid_coords]\n",
    "\n",
    "print(f\"Number of hospitals with valid coordinates: {len(AHA_IT)}\")\n",
    "\n",
    "# Create GeoDataFrame\n",
    "hospitals = gpd.GeoDataFrame(\n",
    "    AHA_IT, \n",
    "    geometry=gpd.points_from_xy(AHA_IT.long_as, AHA_IT.lat_as),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "print(f\"Successfully created GeoDataFrame with {len(hospitals)} hospitals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Data engineering \n",
    "\n",
    "These hospital characteristics were selected based on investigator consensus, and we used LASSO regression analysis to explore and identify additional variables that predict AI/ML implementation and reflect hospital resource levels.\n",
    "\n",
    "- **rural_urban_type** : collected from AHA survey. categorized into {1: rural, 2: micro, 3: metro} based on the location of the hospital ('CBSATYPE')\n",
    "- **system member** : hospital belonging to a corporate body that owns or manage health provider facilities or health-related subsidiaries. ('MHSMEMB')\n",
    "- **delivery_system** : delivery system identified using existing theory and AHA Annual Survey data {1: Centralized Health System, 2: Centralized Physician/Insurance Health System, 3: Moderately Centralized Health System, 4: Decentralized Health System, 5: Independent Hospital System, 6/Missing: Insufficient data to determine} ('CLUSTER')\n",
    "- **community_hospital** : all nonfederal, short-term general, and special hospitals whose facilities and services are available to the public {0: No, 1: Yes}('CHC')\n",
    "- **subsidary_hospital** : Hospital itself operates subsidiary corporation {0: No, 1: Yes} ('SUBS')\n",
    "- **frontline_hospital** : Frontline facility {0: No, 1: Yes} ('FRTLN')\n",
    "- **joint_commission_accreditaion** : Accreditation by joint commision {0: No, 1: Yes} ('MAPP1')\n",
    "- **center_quality** : Center for Improvement in Healthcare Quality Accreditation {0: No, 1: Yes} ('MAPP22')\n",
    "- **teaching_hospital** : major teaching hospital ('MAPP8'), minor teaching hospital ('MAPP3' or 'MAPP5')\n",
    "- **critical_access** critical access hospital {0: No, 1: Yes} ('MAPP18')\n",
    "- **rural_referral** : rural referral center {0: No, 1: Yes} ('MAPP19')\n",
    "- **ownership_type** : type of organization responsible for establishing policy concerning overall operation {government_federal, government_nonfederal, nonprofit, forprofit, other} ('CNTRL')\n",
    "- **bedsize** : bed-size category, ordinal variable ('BSC')\n",
    "- **medicare_ipd_percentage** : medicare inpatient days / total inpatient days. Proxy variable to reflect the proportion of medicare patient \n",
    "- **medicaid_ipd_percentage** : medicaid inpatient days / total inpatient days. Proxy variable to reflect the proportion of medicaid patients \n",
    "- **core_index** : summary measure to track the interoperability of US hospitals (https://doi.org/10.1093/jamia/ocae289)\n",
    "- **friction_index** : summary measures to track the barrier or difficulty in interoperability between hospitals (https://doi.org/10.1093/jamia/ocae289)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rural_urban_type\n",
    "# Continue with CBSA type and other variables\n",
    "AHA_IT['rural_urban_type'] = AHA_IT['cbsatype_as'].map({\n",
    "    'Rural': 1,      # Rural = 1 (lowest)\n",
    "    'Micro': 2,      # Micropolitan = 2 (middle)\n",
    "    'Metro': 3       # Metropolitan = 3 (highest)\n",
    "})\n",
    "\n",
    "## system_member\n",
    "# Create new column 'system_member' based on the conditions\n",
    "AHA_IT['system_member'] = AHA_IT['mhsmemb_as'].copy()\n",
    "# Set to 1 where sysid_as is not null and mhsmemb_as is null\n",
    "AHA_IT.loc[(AHA_IT['sysid_as'].notna()) & (AHA_IT['mhsmemb_as'].isna()), 'system_member'] = 1\n",
    "# Convert all remaining null values to 0\n",
    "AHA_IT['system_member'] = AHA_IT['system_member'].fillna(0)\n",
    "\n",
    "## AHA System Cluster Code - delivery_system\n",
    "AHA_IT['delivery_system'] = AHA_IT['cluster_as']\n",
    "\n",
    "## community_hospital\n",
    "AHA_IT['community_hospital'] = AHA_IT['chc_as'].replace(2, 0)\n",
    "\n",
    "## subsidary_hospital\n",
    "AHA_IT['subsidary_hospital'] = AHA_IT['subs_as']\n",
    "\n",
    "## frontline_hospital\n",
    "AHA_IT['frontline_hospital'] = AHA_IT['frtln_as'].replace('.', 0)\n",
    "\n",
    "## joint_commission_accreditation\n",
    "AHA_IT['joint_commission_accreditation'] = AHA_IT['mapp1_as'].replace(2,0)\n",
    "\n",
    "## center_quality\n",
    "AHA_IT['center_quality'] = AHA_IT['mapp22_as'].replace(2,0)\n",
    "\n",
    "# teaching hospitals \n",
    "AHA_IT['teaching_hospital'] = ((AHA_IT['mapp5_as'] == 1) | (AHA_IT['mapp3_as'] == 1) | (AHA_IT['mapp8_as'] == 1)).astype(int)\n",
    "AHA_IT['major_teaching_hospital'] = ((AHA_IT['mapp8_as'] == 1)).astype(int)\n",
    "AHA_IT['minor_teaching_hospital'] = (((AHA_IT['mapp5_as'] == 1) | (AHA_IT['mapp3_as'] == 1))&~(AHA_IT['mapp8_as'] == 1)).astype(int)\n",
    "\n",
    "# critical access hospital\n",
    "AHA_IT['critical_access'] = (AHA_IT['mapp18_as'] == 1).astype(int)\n",
    "\n",
    "\n",
    "# rural referral center \n",
    "AHA_IT['rural_referral'] = (AHA_IT['mapp19_as'] == 1).astype(int)\n",
    "\n",
    "# medicare medicaid percentage\n",
    "AHA_IT['medicare_ipd_percentage'] = AHA_IT['mcripd_as'] / AHA_IT['ipdtot_as'] * 100\n",
    "AHA_IT['medicaid_ipd_percentage'] = AHA_IT['mcdipd_as'] / AHA_IT['ipdtot_as'] * 100\n",
    "\n",
    "# bed size \n",
    "AHA_IT['bedsize'] = AHA_IT['bsc_as'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospital ownership type \n",
    "\n",
    "AHA_IT['nonfederal_governement'] = ((AHA_IT['cntrl_as'] == 12) | (AHA_IT['cntrl_as'] == 13)|(AHA_master['cntrl_as'] == 14) | (AHA_master['cntrl_as'] == 15)| (AHA_master['cntrl_as'] == 16)).astype(int)\n",
    "AHA_IT['non_profit_nongovernment'] = ((AHA_IT['cntrl_as'] == 21) | (AHA_IT['cntrl_as'] == 23)).astype(int)\n",
    "AHA_IT['for_profit'] = ((AHA_IT['cntrl_as'] == 31) | (AHA_IT['cntrl_as'] == 32) | (AHA_master['cntrl_as'] == 33)).astype(int)\n",
    "AHA_IT['federal_governement'] = ((AHA_IT['cntrl_as'] == 40) | (AHA_IT['cntrl_as'] == 44) | (AHA_IT['cntrl_as'] == 45) | (AHA_IT['cntrl_as'] == 46) | (AHA_master['cntrl_as'] == 47) | (AHA_master['cntrl_as'] == 48)).astype(int)\n",
    "# Create a categorical column for hospital ownership types\n",
    "def create_ownership_category(row):\n",
    "    if row['cntrl_as'] in [12, 13, 14, 15, 16]:\n",
    "        return 'nonfederal_government'\n",
    "    elif row['cntrl_as'] in [21, 23]:\n",
    "        return 'non_profit_nongovernment'\n",
    "    elif row['cntrl_as'] in [31, 32, 33]:\n",
    "        return 'for_profit'\n",
    "    elif row['cntrl_as'] in [40, 44, 45, 46, 47, 48]:\n",
    "        return 'federal_government'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Create the categorical column\n",
    "AHA_IT['ownership_type'] = AHA_IT.apply(create_ownership_category, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_geo_hospitals = AHA_IT.dropna(subset=['long_as', 'lat_as'])\n",
    "# Create a GeoDataFrame\n",
    "hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_geo_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_geo_hospitals.long_as, valid_geo_hospitals.lat_as),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n",
    "\n",
    "# Convert to a projected CRS for accurate distance calculations\n",
    "geo_hospitals_gdf_projected\n",
    " = hospitals_gdf.to_crs(epsg=5070) # projected coordinate system using flat, 2D plane to represent Earth's surface "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Run GWR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 1. Prepare coordinates and target variable\n",
    "coords = np.array(geo_hospitals_gdf_projected[['long_as', 'lat_as']])\n",
    "y = geo_hospitals_gdf_projected['ai_base_score_imputed'].values\n",
    "\n",
    "X_data = pd.DataFrame(index=geo_hospitals_gdf_projected.index)\n",
    "\n",
    "X_data['subsidary_hospital'] = geo_hospitals_gdf_projected['subsidary_hospital'].astype(float)\n",
    "X_data['frontline_hospital'] = geo_hospitals_gdf_projected['frontline_hospital'].astype(float)\n",
    "X_data['joint_commission_accreditation'] = geo_hospitals_gdf_projected['joint_commission_accreditation'].astype(float)\n",
    "X_data['delivery_system'] = geo_hospitals_gdf_projected['delivery_system'].astype(float)\n",
    "X_data['teaching_hospital'] = geo_hospitals_gdf_projected['teaching_hospital'].astype(float)\n",
    "X_data['critical_access'] = geo_hospitals_gdf_projected['critical_access'].astype(float)\n",
    "X_data['rural_referral'] = geo_hospitals_gdf_projected['rural_referral'].astype(float)\n",
    "X_data['for_profit'] = (geo_hospitals_gdf_projected['ownership_type'] == 'for_profit').astype(float)\n",
    "X_data['bedsize'] = geo_hospitals_gdf_projected['bedsize'].astype(float)\n",
    "X_data['medicare_ipd_percentage'] = geo_hospitals_gdf_projected['medicare_ipd_percentage'].astype(float)\n",
    "X_data['medicaid_ipd_percentage'] = geo_hospitals_gdf_projected['medicaid_ipd_percentage'].astype(float)\n",
    "X_data['core_index'] = geo_hospitals_gdf_projected['core_index'].astype(float)\n",
    "X_data['friction_index'] = geo_hospitals_gdf_projected['friction_index'].astype(float)\n",
    "\n",
    "# 2. Apply MICE imputation\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42, sample_posterior=False)\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X_data), columns=X_data.columns, index=X_data.index)\n",
    "\n",
    "# 3. Standardize continuous variables (not binary ones)\n",
    "continuous_vars = [\n",
    "    'delivery_system', 'bedsize',\n",
    "    'medicare_ipd_percentage', 'medicaid_ipd_percentage',\n",
    "    'core_index', 'friction_index'\n",
    "]\n",
    "scaler = StandardScaler()\n",
    "X_imputed[continuous_vars] = scaler.fit_transform(X_imputed[continuous_vars])\n",
    "\n",
    "# Final feature matrix for GWR\n",
    "X_data = X_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_gwr_robust(coords, y, X, bandwidth):\n",
    "    \"\"\"\n",
    "    Simple GWR with better numerical stability\n",
    "    \"\"\"\n",
    "    n = len(coords)\n",
    "    p = X.shape[1]\n",
    "    results = np.zeros((n, p))\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Calculate distances\n",
    "        dists = np.sqrt(np.sum((coords - coords[i])**2, axis=1))\n",
    "        \n",
    "        # Calculate weights - use larger sigma to prevent numerical issues\n",
    "        weights = np.exp(-0.5 * (dists / bandwidth)**2)\n",
    "        \n",
    "        # Only use observations with reasonable weights\n",
    "        threshold = 0.001  # Minimum meaningful weight\n",
    "        mask = weights >= threshold\n",
    "        \n",
    "        if np.sum(mask) < p:  # Not enough observations\n",
    "            results[i] = np.nan\n",
    "            continue\n",
    "        \n",
    "        # Subset data\n",
    "        X_local = X[mask]\n",
    "        y_local = y[mask]\n",
    "        w_local = weights[mask]\n",
    "        \n",
    "        # Weighted least squares with regularization\n",
    "        W = np.diag(w_local)\n",
    "        XtW = X_local.T @ W\n",
    "        XtWX = XtW @ X_local\n",
    "        \n",
    "        # Add small regularization to prevent singularity\n",
    "        reg_param = 1e-6 * np.trace(XtWX) / p\n",
    "        XtWX_reg = XtWX + reg_param * np.eye(p)\n",
    "        \n",
    "        try:\n",
    "            beta = np.linalg.solve(XtWX_reg, XtW @ y_local)\n",
    "            results[i] = beta\n",
    "        except np.linalg.LinAlgError:\n",
    "            results[i] = np.nan\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Updated workflow\n",
    "def run_gwr(X_data, continuous_vars, selected_bandwidth):\n",
    "    \"\"\"Complete improved GWR workflow with data preparation\"\"\"\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_data[continuous_vars] = scaler.fit_transform(X_data[continuous_vars])\n",
    "    \n",
    "    # Add constant for intercept\n",
    "    X_data['const'] = 1.0\n",
    "    \n",
    "    print(f\"Feature matrix shape: {X_data.shape}\")\n",
    "    print(f\"Variables: {list(X_data.columns)}\")\n",
    "    \n",
    "    # Convert to array\n",
    "    X_array = X_data.values.astype(float)\n",
    "    \n",
    "    \n",
    "    # 5. Run final GWR with optimal bandwidth\n",
    "    print(f\"\\n  Running GWR with selected bandwidth ({selected_bandwidth:.3f})...\")\n",
    "    final_params = simple_gwr_robust(coords, y, X_array, selected_bandwidth)\n",
    "    \n",
    "    # 6. Create results DataFrame\n",
    "    key_vars = [col for col in X_data.columns if col != 'const']\n",
    "    var_coeffs = pd.DataFrame()\n",
    "    \n",
    "    for var in key_vars:\n",
    "        col_idx = list(X_data.columns).index(var)\n",
    "        var_coeffs[var] = final_params[:, col_idx]\n",
    "    \n",
    "    # Add coordinates\n",
    "    var_coeffs['longitude'] = coords[:, 0]\n",
    "    var_coeffs['latitude'] = coords[:, 1]\n",
    "    \n",
    "    # Print final results summary\n",
    "    print(f\"\\n Final Results Summary:\")\n",
    "    print(f\"   Optimal bandwidth: {selected_bandwidth:.3f} degrees\")\n",
    "    total_valid = np.sum(np.all(np.isfinite(final_params), axis=1))\n",
    "    print(f\"   Valid observations: {total_valid}/{len(coords)} ({100*total_valid/len(coords):.1f}%)\")\n",
    "    \n",
    "    for var in key_vars:\n",
    "        coeffs = var_coeffs[var].dropna()\n",
    "        if len(coeffs) > 0:\n",
    "            print(f\"   {var}: Range=[{coeffs.min():.4f}, {coeffs.max():.4f}], SD={coeffs.std():.4f}\")\n",
    "    \n",
    "    return var_coeffs, selected_bandwidth, X_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_gwr_robust(coords, y, X, bandwidth):\n",
    "    n = len(coords)\n",
    "    p = X.shape[1]\n",
    "    results = np.full((n, p), np.nan, dtype=float)\n",
    "\n",
    "    y = np.asarray(y, dtype=float).ravel()    # <-- (n,) 보장\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    coords = np.asarray(coords, dtype=float)\n",
    "\n",
    "    for i in range(n):\n",
    "        dists = np.sqrt(np.sum((coords - coords[i])**2, axis=1))\n",
    "        weights = np.exp(-0.5 * (dists / bandwidth)**2)\n",
    "\n",
    "        threshold = 1e-3\n",
    "        mask = weights >= threshold\n",
    "        if np.sum(mask) < p:\n",
    "            continue\n",
    "\n",
    "        X_local = X[mask]\n",
    "        y_local = y[mask]\n",
    "        w_local = weights[mask]\n",
    "\n",
    "        W = np.diag(w_local)\n",
    "        XtW = X_local.T @ W\n",
    "        XtWX = XtW @ X_local\n",
    "\n",
    "        reg_param = 1e-6 * (np.trace(XtWX) / p + 1e-12)\n",
    "        XtWX_reg = XtWX + reg_param * np.eye(p)\n",
    "\n",
    "        try:\n",
    "            beta = np.linalg.solve(XtWX_reg, XtW @ y_local.reshape(-1, 1)) \n",
    "            results[i] = beta.ravel() \n",
    "        except np.linalg.LinAlgError:\n",
    "            pass\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def plot_gwr_results_large(var_coeffs, states_gdf, figsize=(25, 30), point_size=80):\n",
    "    \"\"\"\n",
    "    Large format GWR plotting with continental US state boundaries - SINGLE FIGURE ONLY\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    var_coeffs : DataFrame\n",
    "        GWR results with coefficient columns + longitude/latitude\n",
    "    states_gdf : GeoDataFrame\n",
    "        US state boundaries (your loaded states variable)\n",
    "    figsize : tuple\n",
    "        Figure size - make it large for better visibility\n",
    "    point_size : int\n",
    "        Size of scatter points\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter for continental US only\n",
    "    exclude_states = ['AK', 'HI', 'Alaska', 'Hawaii', 'PR', 'Puerto Rico', \n",
    "                     'VI', 'Virgin Islands', 'GU', 'Guam', 'AS', 'American Samoa', \n",
    "                     'MP', 'Northern Mariana Islands']\n",
    "    \n",
    "    state_col = None\n",
    "    for col in ['STUSPS', 'STATE_ABBR', 'ABBR', 'NAME', 'STATE_NAME', 'STATE']:\n",
    "        if col in states_gdf.columns:\n",
    "            state_col = col\n",
    "            break\n",
    "    \n",
    "    if state_col:\n",
    "        continental_states = states_gdf[~states_gdf[state_col].isin(exclude_states)]\n",
    "        print(f\"Filtering states using column '{state_col}', kept {len(continental_states)} states\")\n",
    "    else:\n",
    "        continental_states = states_gdf.cx[-130:-65, 20:50]\n",
    "        print(\"Filtering by geographic bounds (no state name column found)\")\n",
    "    \n",
    "    # Get variable names (exclude coordinates)\n",
    "    variables = [col for col in var_coeffs.columns if col not in ['longitude', 'latitude']]\n",
    "    n_vars = len(variables)\n",
    "    \n",
    "    # Remove rows with NaN coefficients\n",
    "    plot_data = var_coeffs.dropna()\n",
    "    \n",
    "    print(f\"Plotting {len(plot_data)} valid observations\")\n",
    "    \n",
    "    # Use 2 columns for better visibility\n",
    "    n_cols = 2\n",
    "    n_rows = (n_vars + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Create ONE single figure\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    if n_vars == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes.reshape(1, -1)\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    fig.suptitle('GWR Spatial Coefficients - Local Variation Analysis', \n",
    "                 fontsize=24, fontweight='bold', y=0.98)\n",
    "    \n",
    "    for i, var in enumerate(variables):\n",
    "        row = i // n_cols\n",
    "        col = i % n_cols\n",
    "        ax = axes[row, col] if n_rows > 1 else axes[col]\n",
    "        \n",
    "        # Get data\n",
    "        coeffs = plot_data[var].values\n",
    "        x = plot_data['longitude'].values\n",
    "        y = plot_data['latitude'].values\n",
    "        \n",
    "        # Color normalization (centered at 0)\n",
    "        vmax = np.abs(coeffs).max()\n",
    "        if vmax == 0:\n",
    "            vmax = 1\n",
    "        norm = TwoSlopeNorm(vmin=-vmax, vcenter=0, vmax=vmax)\n",
    "        \n",
    "        # Add continental US state boundaries only\n",
    "        continental_states.boundary.plot(ax=ax, color='black', linewidth=0.8, alpha=0.6)\n",
    "        \n",
    "        # Set axis limits to continental US\n",
    "        ax.set_xlim(-130, -65)\n",
    "        ax.set_ylim(20, 50)\n",
    "        \n",
    "        # Change the color normalization part\n",
    "        # Instead of using the maximum absolute value, we'll set fixed limits\n",
    "        norm = TwoSlopeNorm(vmin=-1, vcenter=0, vmax=1)  # Fixed range from -1 to 1\n",
    "\n",
    "        # The rest of the scatter plot remains the same\n",
    "        scatter = ax.scatter(x, y, c=coeffs, cmap='RdBu', norm=norm, \n",
    "                    s=point_size, alpha=0.8, edgecolors='black', linewidth=0.8, zorder=5)\n",
    "        \n",
    "        # Clean formatting - no overlapping text\n",
    "        ax.set_title(f'Spatial Variation: {var}', fontweight='bold', fontsize=18, pad=20)\n",
    "        ax.set_xlabel('Longitude', fontsize=14)\n",
    "        ax.set_ylabel('Latitude', fontsize=14)\n",
    "        ax.grid(True, alpha=0.3, linewidth=0.5)\n",
    "        ax.tick_params(labelsize=12)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=ax, shrink=0.8, aspect=30, pad=0.02)\n",
    "        cbar.set_label('Coefficient Value', fontsize=12, labelpad=15)\n",
    "        cbar.ax.tick_params(labelsize=10)\n",
    "        \n",
    "    \n",
    "    # Hide empty subplots if any\n",
    "    if n_vars < n_rows * n_cols:\n",
    "        for i in range(n_vars, n_rows * n_cols):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            ax = axes[row, col] if n_rows > 1 else axes[col]\n",
    "            ax.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_vars = ['delivery_system', 'bedsize', 'medicare_ipd_percentage', 'medicaid_ipd_percentage', \n",
    "                      'core_index', 'friction_index']\n",
    "# Load your state boundaries first:\n",
    "# import os\n",
    "os.environ['SHAPE_RESTORE_SHX'] = 'YES'  \n",
    "states = gpd.read_file('../../../../../data/map_data/state_boundary.shp')\n",
    "\n",
    "# Create ONE single figure with all variables\n",
    "var_coeffs, selected_bandwidth, X_data = run_gwr(X_data, continuous_vars, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plot_gwr_results_large(var_coeffs, states, figsize=(25, 30))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.2 Conduct GWR for geospatial/community characteristics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace invalid SVI values with median of valid values\n",
    "svi_columns = ['svi_themes_median', 'svi_theme1_median', 'svi_theme2_median', \n",
    "              'svi_theme3_median', 'svi_theme4_median']\n",
    "\n",
    "for col in svi_columns:\n",
    "    # Identify valid values (0-1 range)\n",
    "    valid_mask = (geo_hospitals_gdf_projected[col] >= 0) & (geo_hospitals_gdf_projected[col] <= 1)\n",
    "    \n",
    "    # Calculate median of valid values\n",
    "    valid_median = geo_hospitals_gdf_projected.loc[valid_mask, col].median()\n",
    "    \n",
    "    # Replace invalid values with the median\n",
    "    invalid_mask = ~valid_mask\n",
    "    geo_hospitals_gdf_projected.loc[invalid_mask, col] = valid_median\n",
    "    \n",
    "    print(f\"Fixed {col}:\")\n",
    "    print(f\"  Replaced {invalid_mask.sum()} invalid values with median {valid_median:.4f}\")\n",
    "    print(f\"  New range: {geo_hospitals_gdf_projected[col].min():.4f} to {geo_hospitals_gdf_projected[col].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare coordinates and target variable\n",
    "coords = np.array(geo_hospitals_gdf_projected[['long_as', 'lat_as']])\n",
    "y = geo_hospitals_gdf_projected['ai_base_score_imputed'].values\n",
    "    \n",
    "# 2. Prepare feature matrix (same as your original code)\n",
    "print(\"Preparing feature matrix...\")\n",
    "X_data2 = pd.DataFrame(index=geo_hospitals_gdf_projected.index)\n",
    "    \n",
    "# Hospital characteristics missing value imputation \n",
    "X_data2['svi_themes_median'] = geo_hospitals_gdf_projected['svi_themes_median'].fillna(geo_hospitals_gdf_projected['svi_themes_median'].median())\n",
    "X_data2['svi_theme1_median'] = geo_hospitals_gdf_projected['svi_theme1_median'].fillna(geo_hospitals_gdf_projected['svi_theme1_median'].median())\n",
    "X_data2['svi_theme2_median'] = geo_hospitals_gdf_projected['svi_theme2_median'].fillna(geo_hospitals_gdf_projected['svi_theme2_median'].median())    \n",
    "X_data2['svi_theme3_median'] = geo_hospitals_gdf_projected['svi_theme3_median'].fillna(geo_hospitals_gdf_projected['svi_theme3_median'].median())\n",
    "X_data2['svi_theme4_median'] = geo_hospitals_gdf_projected['svi_theme4_median'].fillna(geo_hospitals_gdf_projected['svi_theme4_median'].median())\n",
    "X_data2['national_adi_median'] = geo_hospitals_gdf_projected['national_adi_median'].fillna(geo_hospitals_gdf_projected['national_adi_median'].median())\n",
    "X_data2['mean_primary_hpss'] = geo_hospitals_gdf_projected['mean_primary_hpss'].fillna(0)\n",
    "X_data2['mean_dental_hpss'] = geo_hospitals_gdf_projected['mean_dental_hpss'].fillna(0)\n",
    "X_data2['mean_mental_hpss'] = geo_hospitals_gdf_projected['mean_mental_hpss'].fillna(0)\n",
    "X_data2['mean_mua_shortage'] = geo_hospitals_gdf_projected['mean_mua_shortage'].fillna(0)\n",
    "X_data2['mean_mua_elders_shortage'] = geo_hospitals_gdf_projected['mean_mua_elders_shortage'].fillna(0)\n",
    "X_data2['mean_mua_infant_shortage'] = geo_hospitals_gdf_projected['mean_mua_infant_shortage'].fillna(0)\n",
    "X_data2['rural_urban_type'] = geo_hospitals_gdf_projected['rural_urban_type'].fillna(geo_hospitals_gdf_projected['rural_urban_type'].median())\n",
    "X_data2['Device_Percent'] = geo_hospitals_gdf_projected['Device_Percent'].fillna(geo_hospitals_gdf_projected['Device_Percent'].median())\n",
    "X_data2['Broadband_Percent'] = geo_hospitals_gdf_projected['Broadband_Percent'].fillna(geo_hospitals_gdf_projected['Broadband_Percent'].median())\n",
    "X_data2['Internet_Percent'] = geo_hospitals_gdf_projected['Internet_Percent'].fillna(geo_hospitals_gdf_projected['Internet_Percent'].median())\n",
    "    \n",
    "# Standardize continuous variables (except binary variables)\n",
    "continuous_vars = ['svi_themes_median', 'svi_theme1_median', 'svi_theme2_median', 'svi_theme3_median', 'svi_theme4_median', 'national_adi_median', 'Device_Percent', 'Broadband_Percent', 'Internet_Percent', 'mean_primary_hpss', 'mean_dental_hpss', 'mean_mental_hpss', 'mean_mua_score', 'mean_mua_elder_score', 'mean_mua_infant_score', 'rural_urban_type']\n",
    "scaler = StandardScaler()\n",
    "X_data2[continuous_vars] = scaler.fit_transform(X_data2[continuous_vars])\n",
    "\n",
    "# 3. Apply MICE imputation\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42, sample_posterior=False)\n",
    "X_data2 = pd.DataFrame(imputer.fit_transform(X_data2), columns=X_data2.columns, index=X_data2.index)\n",
    "\n",
    " # 4. Run GWR\n",
    "var_coeffs, selected_bandwidth, X_data2 = run_gwr(X_data2, continuous_vars, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
