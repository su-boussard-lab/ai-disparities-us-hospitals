{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4. Hotspot Regional Analysis\n",
    "\n",
    "**Description**  \n",
    "This section identify statistically significant hotspots and coldspots of hospital AI adoption across the United States. Using Getis-Ord Gi* and Local Moran's I statistics, this analysis pinpoints specific geographic locations where AI adoption is significantly higher (hotspots) or lower (coldspots) \n",
    "\n",
    "**Purpose**  \n",
    "To identify hotspots and coldspots \n",
    "\n",
    "**Disclaimer**  \n",
    "- AHA data is subscription-based and not publicly shareable. All reported results are aggregated at the state or census division level.\n",
    "- All publicly available data should also be independently downlowded from the source \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4_0 load necessary libraries, functions and preprocessed data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C4_0_1 load libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C4_0_2 load custom functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_base_ai_implementation_row(row):\n",
    "    \"\"\"\n",
    "    Calculate base AI implementation score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: Base AI implementation score\n",
    "    \"\"\"\n",
    "    # Base AI implementation score (continuous)\n",
    "    # Return None if the input value is null\n",
    "    if pd.isna(row['aipred_it']):\n",
    "        return None\n",
    "    elif row['aipred_it'] == 1:  # Machine Learning\n",
    "        return 2\n",
    "    elif row['aipred_it'] == 2:  # Other Non-Machine Learning Predictive Models\n",
    "        return 1\n",
    "    else:  # Neither (3) or Do not know (4)\n",
    "        return 0\n",
    "\n",
    "def calculate_ai_implementation_breadth_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI implementation breadth score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI implementation breadth score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        breadth_score = base_score\n",
    "        # Implementation Breadth Score - count use cases\n",
    "        use_case_cols = ['aitraj_it', 'airfol_it', 'aimhea_it', 'airect_it', \n",
    "                     'aibill_it', 'aische_it', 'aipoth_it', 'aicloth_it']\n",
    "        for col in use_case_cols:\n",
    "            if row[col] is None:\n",
    "                breadth_score += 0\n",
    "            else:\n",
    "                breadth_score += row[col] * 0.25  # 0.25 points per use case\n",
    "        return breadth_score\n",
    "\n",
    "def calculate_ai_development_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI development score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI development score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0 \n",
    "    else:\n",
    "        dev_score = base_score\n",
    "        if 'mlsed_it' in row and pd.notna(row['mlsed_it']):\n",
    "            dev_score += row['mlsed_it'] * 2  # Self-developed\n",
    "        if 'mldev_it' in row and pd.notna(row['mldev_it']):\n",
    "            dev_score += row['mldev_it']  # EHR developer\n",
    "        if 'mlthd_it' in row and pd.notna(row['mlthd_it']):\n",
    "            dev_score += row['mlthd_it']  # Third-party\n",
    "        if 'mlpubd_it' in row and pd.notna(row['mlpubd_it']):\n",
    "            dev_score += row['mlpubd_it'] * 0.5  # Public domain\n",
    "        return dev_score\n",
    "\n",
    "def calculate_ai_evaluation_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI evaluation score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI evaluation score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        eval_score = base_score\n",
    "        # For model accuracy (MLACCU)\n",
    "        if row['mlaccu_it'] is None:\n",
    "            eval_score += 0\n",
    "        elif row['mlaccu_it'] == 1:  # All models\n",
    "            eval_score += 1\n",
    "        elif row['mlaccu_it'] == 2:  # Most models\n",
    "            eval_score += 0.75\n",
    "        elif row['mlaccu_it'] == 3:  # Some models\n",
    "            eval_score += 0.5\n",
    "        elif row['mlaccu_it'] == 4:  # Few models\n",
    "            eval_score += 0.25\n",
    "        # For None (5) or Do not know (6), no points added\n",
    "    \n",
    "    # For model bias (MLBIAS)\n",
    "        if row['mlbias_it'] is None:\n",
    "            eval_score += 0\n",
    "        elif row['mlbias_it'] == 1:  # All models\n",
    "            eval_score += 1\n",
    "        elif row['mlbias_it'] == 2:  # Most models\n",
    "            eval_score += 0.75\n",
    "        elif row['mlbias_it'] == 3:  # Some models\n",
    "            eval_score += 0.5\n",
    "        elif row['mlbias_it'] == 4:  # Few models\n",
    "            eval_score += 0.25\n",
    "        # For None (5) or Do not know (6), no points added\n",
    "    \n",
    "        return eval_score\n",
    "\n",
    "def calculate_all_ai_scores_row(row):\n",
    "    \"\"\"\n",
    "    Calculate all AI/ML implementation scores as continuous measures for a single row.\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with all calculated scores\n",
    "    \"\"\"\n",
    "    # Calculate all scores\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    breadth_score = calculate_ai_implementation_breadth_row(row)\n",
    "    dev_score = calculate_ai_development_row(row)\n",
    "    eval_score = calculate_ai_evaluation_row(row)\n",
    "    \n",
    "    return {\n",
    "        'ai_base_score': base_score,\n",
    "        'ai_base_breadth_score': breadth_score,\n",
    "        'ai_base_dev_score': dev_score,\n",
    "        'ai_base_eval_score': eval_score\n",
    "    }\n",
    "\n",
    "def apply_ai_scores_to_dataframe(df):\n",
    "    \"\"\"\n",
    "    Apply all AI score calculations row by row to a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: A pandas DataFrame with hospital data\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with added AI score columns\n",
    "    \"\"\"\n",
    "    # Initialize empty columns for scores\n",
    "    df['ai_base_score'] = float('nan')\n",
    "    df['ai_base_breadth_score'] = float('nan')\n",
    "    df['ai_base_dev_score'] = float('nan')\n",
    "    df['ai_base_eval_score'] = float('nan')\n",
    "    \n",
    "    # Apply row by row calculations\n",
    "    for index, row in df.iterrows():\n",
    "        scores = calculate_all_ai_scores_row(row)\n",
    "        for score_name, score_value in scores.items():\n",
    "            df.at[index, score_name] = score_value\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C4_0_3 load processed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_master = pd.read_csv('./data/AHA_master_external_data.csv', low_memory=False)\n",
    "AHA_IT = AHA_master[~AHA_master.id_it.isnull()]\n",
    "AHA_master2 = apply_ai_scores_to_dataframe(AHA_IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SHAPE_RESTORE_SHX'] = 'YES'\n",
    "states = gpd.read_file('../../../data/map_data/state_boundary.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C4_1 prepare dataframe for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing or invalid coordinates\n",
    "AHA_master2 = AHA_master2.dropna(subset=['latitude_address', 'longitude_address'])\n",
    "\n",
    "# Filter out invalid coordinates\n",
    "valid_coords = (\n",
    "    (AHA_master['latitude_address'] != 0) & \n",
    "    (AHA_master['longitude_address'] != 0) &\n",
    "    (AHA_master['latitude_address'] >= -90) & \n",
    "    (AHA_master['latitude_address'] <= 90) &\n",
    "    (AHA_master['longitude_address'] >= -180) & \n",
    "    (AHA_master['longitude_address'] <= 180)\n",
    ")\n",
    "AHA_master2 = AHA_master2[valid_coords]\n",
    "\n",
    "print(f\"Number of hospitals with valid coordinates: {len(AHA_master2)}\")\n",
    "\n",
    "# Create GeoDataFrame\n",
    "hospitals = gpd.GeoDataFrame(\n",
    "    AHA_master2, \n",
    "    geometry=gpd.points_from_xy(AHA_master2.longitude_address, AHA_master2.latitude_address),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "print(f\"Successfully created GeoDataFrame with {len(hospitals)} hospitals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter hospitals with valid coordinates and implementation scores\n",
    "valid_hospitals = hospitals.dropna(subset=['longitude_address', 'latitude_address', 'aipred_it'])\n",
    "valid_geo_hospitals = hospitals.dropna(subset=['longitude_address', 'latitude_address'])\n",
    "# Create a GeoDataFrame\n",
    "hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_hospitals.longitude_address, valid_hospitals.latitude_address),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "geo_hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_geo_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_geo_hospitals.longitude_address, valid_geo_hospitals.latitude_address),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a projected CRS for accurate distance calculations\n",
    "hospitals_gdf_projected = hospitals_gdf.to_crs(epsg=3857) # projected coordinate system using flat, 2D plane to represent Earth's surface \n",
    "geo_hospitals_gdf_projected = geo_hospitals_gdf.to_crs(epsg=3857) # projected coordinate system using flat, 2D plane to represent Earth's surface \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add census division column to the dataframe\n",
    "hospitals_gdf_projected['division'] = hospitals_gdf_projected['mstate_it'].map(state_to_division)\n",
    "geo_hospitals_gdf_projected['division'] = geo_hospitals_gdf_projected['mstate_it'].map(state_to_division)\n",
    "\n",
    "# Loop through each census division and create a heatmap\n",
    "divisions = [\n",
    "    'New England', 'Mid Atlantic', 'South Atlantic', \n",
    "    'East North Central', 'East South Central', 'West North Central',\n",
    "    'West South Central', 'Mountain', 'Pacific'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C4_2 load hotspot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gi_star(gdf, value_column, k=5):\n",
    "    # Extract coordinates and values\n",
    "    coords = np.vstack((gdf.geometry.x, gdf.geometry.y)).T\n",
    "    values = gdf[value_column].values\n",
    "    n = len(values)\n",
    "    \n",
    "    # Handle missing values\n",
    "    valid_mask = ~np.isnan(values)\n",
    "    if not np.all(valid_mask):\n",
    "        coords = coords[valid_mask]\n",
    "        values = values[valid_mask]\n",
    "        gdf = gdf[valid_mask].copy()\n",
    "        n = len(values)\n",
    "    \n",
    "    if n <= k:\n",
    "        return gdf\n",
    "    \n",
    "    # Create k-nearest neighbors spatial weights matrix\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='auto').fit(coords)\n",
    "    distances, indices = nbrs.kneighbors(coords)\n",
    "    \n",
    "    W = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        W[i, indices[i, :]] = 1\n",
    "    \n",
    "    # Global statistics\n",
    "    x_bar = np.mean(values)\n",
    "    s_squared = np.var(values, ddof=1)\n",
    "    \n",
    "    # Calculate Gi* for each location\n",
    "    gi_star = np.zeros(n)\n",
    "    z_scores = np.zeros(n)\n",
    "    p_values = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        gi_star[i] = np.sum(W[i, :] * values)\n",
    "        wi_sum = np.sum(W[i, :])\n",
    "        expected_gi = wi_sum * x_bar\n",
    "        variance_gi = (wi_sum * s_squared * (n - wi_sum)) / (n - 1)\n",
    "        \n",
    "        if variance_gi > 0:\n",
    "            z_scores[i] = (gi_star[i] - expected_gi) / np.sqrt(variance_gi)\n",
    "            p_values[i] = 2 * (1 - stats.norm.cdf(abs(z_scores[i])))\n",
    "        else:\n",
    "            z_scores[i] = 0\n",
    "            p_values[i] = 1.0\n",
    "    \n",
    "    # Create results\n",
    "    result_gdf = gdf.copy()\n",
    "    result_gdf['gi_star'] = gi_star\n",
    "    result_gdf['z_score'] = z_scores\n",
    "    result_gdf['p_value'] = p_values\n",
    "    result_gdf['hotspot_type'] = 'Not Significant'\n",
    "    \n",
    "    # Classify significance levels\n",
    "    result_gdf.loc[(z_scores > 0) & (p_values <= 0.01), 'hotspot_type'] = 'Hotspot (99%)'\n",
    "    result_gdf.loc[(z_scores < 0) & (p_values <= 0.01), 'hotspot_type'] = 'Coldspot (99%)'\n",
    "    result_gdf.loc[(z_scores > 0) & (p_values > 0.01) & (p_values <= 0.05), 'hotspot_type'] = 'Hotspot (95%)'\n",
    "    result_gdf.loc[(z_scores < 0) & (p_values > 0.01) & (p_values <= 0.05), 'hotspot_type'] = 'Coldspot (95%)'\n",
    "    result_gdf.loc[(z_scores > 0) & (p_values > 0.05) & (p_values <= 0.1), 'hotspot_type'] = 'Hotspot (90%)'\n",
    "    result_gdf.loc[(z_scores < 0) & (p_values > 0.05) & (p_values <= 0.1), 'hotspot_type'] = 'Coldspot (90%)'\n",
    "    \n",
    "    return result_gdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4_3 run hotspot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct hotspot analysis for the entire US\n",
    "print(\"Performing hotspot analysis for the entire US...\")\n",
    "base_hotspot_results = calculate_gi_star(hospitals_gdf_projected, 'ai_base_score', k=5)\n",
    "breadth_hotspot_results = calculate_gi_star(hospitals_gdf_projected, 'ai_base_breadth_score', k=5)\n",
    "dev_hotspot_results = calculate_gi_star(hospitals_gdf_projected, 'ai_base_dev_score', k=5)\n",
    "eval_hotspot_results = calculate_gi_star(hospitals_gdf_projected, 'ai_base_eval_score', k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4_4 get visualization and results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hotspot Bubble Chart Visualizations\n",
    "\n",
    "\n",
    "# Create colormap\n",
    "hotspot_cmap = LinearSegmentedColormap.from_list(\n",
    "    'hotspot_cmap', \n",
    "    ['#333333', '#737373', '#bfbfbf', '#f2f2f2', '#4d94ff', '#0050b3', '#001f4d']\n",
    ")\n",
    "\n",
    "# 1. State-Level Analysis\n",
    "def create_state_bubble_chart(hotspot_results):\n",
    "    # Group by state\n",
    "    state_summary = hotspot_results.groupby('mstate_it').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'Hotspot %': 100 * len(x[x['hotspot_type'].str.contains('Hotspot')]) / len(x),\n",
    "            'Coldspot %': 100 * len(x[x['hotspot_type'].str.contains('Coldspot')]) / len(x),\n",
    "            'Total': len(x),\n",
    "            'Mean Z': x['z_score'].mean()\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    # Filter states with sufficient data\n",
    "    state_summary = state_summary[state_summary['Total'] >= 20]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 9))\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = ax.scatter(\n",
    "        state_summary['Hotspot %'],\n",
    "        state_summary['Coldspot %'],\n",
    "        s=state_summary['Total'] * 3.5,\n",
    "        c=state_summary['Mean Z'],\n",
    "        cmap=hotspot_cmap,\n",
    "        alpha=0.85,\n",
    "        edgecolors='black',\n",
    "        linewidths=0.5,\n",
    "        vmin=-2.5,\n",
    "        vmax=2.5\n",
    "    )\n",
    "    \n",
    "    # Add state labels\n",
    "    for state, row in state_summary.iterrows():\n",
    "        ax.annotate(\n",
    "            state,\n",
    "            (row['Hotspot %'], row['Coldspot %']),\n",
    "            xytext=(3, 3),\n",
    "            textcoords='offset points',\n",
    "            fontsize=9,\n",
    "            fontweight='bold',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.85)\n",
    "        )\n",
    "    \n",
    "    # Add reference lines\n",
    "    ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    ax.axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = fig.colorbar(scatter, ax=ax, shrink=0.9)\n",
    "    cbar.set_label('Mean Z-Score', fontsize=12)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_title('Spatial Clustering Patterns by State', fontsize=16)\n",
    "    ax.set_xlabel('Hotspot Percentage', fontsize=13)\n",
    "    ax.set_ylabel('Coldspot Percentage', fontsize=13)\n",
    "    ax.set_xlim(-5, 105)\n",
    "    ax.set_ylim(-5, 105)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, state_summary\n",
    "\n",
    "# 2. Division-Level Analysis\n",
    "def create_division_bubble_chart(hotspot_results):\n",
    "    # Group by division\n",
    "    division_summary = hotspot_results.groupby('division').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'Hotspot %': 100 * len(x[x['hotspot_type'].str.contains('Hotspot')]) / len(x),\n",
    "            'Coldspot %': 100 * len(x[x['hotspot_type'].str.contains('Coldspot')]) / len(x),\n",
    "            'Total': len(x),\n",
    "            'Mean Z': x['z_score'].mean()\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 9))\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = ax.scatter(\n",
    "        division_summary['Hotspot %'],\n",
    "        division_summary['Coldspot %'],\n",
    "        s=division_summary['Total'] * 1.2,\n",
    "        c=division_summary['Mean Z'],\n",
    "        cmap=hotspot_cmap,\n",
    "        alpha=0.85,\n",
    "        edgecolors='black',\n",
    "        linewidths=0.7,\n",
    "        vmin=-2.5,\n",
    "        vmax=2.5\n",
    "    )\n",
    "    \n",
    "    # Add division labels\n",
    "    for division, row in division_summary.iterrows():\n",
    "        ax.annotate(\n",
    "            division,\n",
    "            (row['Hotspot %'], row['Coldspot %']),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=11,\n",
    "            fontweight='bold',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.4\", fc=\"white\", ec=\"gray\", alpha=0.9)\n",
    "        )\n",
    "    \n",
    "    # Add reference lines\n",
    "    ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    ax.axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = fig.colorbar(scatter, ax=ax, shrink=0.9)\n",
    "    cbar.set_label('Mean Z-Score', fontsize=12)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_title('Spatial Clustering Patterns by Census Division', fontsize=16)\n",
    "    ax.set_xlabel('Hotspot Percentage', fontsize=13)\n",
    "    ax.set_ylabel('Coldspot Percentage', fontsize=13)\n",
    "    ax.set_xlim(-5, 105)\n",
    "    ax.set_ylim(-5, 105)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, division_summary\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
