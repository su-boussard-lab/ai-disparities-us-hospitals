{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4. Hotspot Regional Analysis\n",
    "\n",
    "**Description**  \n",
    "This section identify statistically significant hotspots and coldspots of hospital AI adoption across the United States. Using Getis-Ord Gi* and Local Moran's I statistics, this analysis pinpoints specific geographic locations where AI adoption is significantly higher (hotspots) or lower (coldspots) \n",
    "\n",
    "**Purpose**  \n",
    "To identify hotspots and coldspots \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 load necessary libraries, functions and preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_master = pd.read_csv('./data/AHA_master_external_data.csv', low_memory=False)\n",
    "AHA_IT = AHA_master[~AHA_master.id_it.isnull()]\n",
    "AHA_master2 = apply_ai_scores_to_dataframe(AHA_IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SHAPE_RESTORE_SHX'] = 'YES'\n",
    "states = gpd.read_file('../../../data/map_data/state_boundary.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 data engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing or invalid coordinates\n",
    "AHA_master2 = AHA_master2.dropna(subset=['lat_as', 'long_as'])\n",
    "\n",
    "# Filter out invalid coordinates\n",
    "valid_coords = (\n",
    "    (AHA_master['lat_as'] != 0) & \n",
    "    (AHA_master['long_as'] != 0) &\n",
    "    (AHA_master['lat_as'] >= -90) & \n",
    "    (AHA_master['lat_as'] <= 90) &\n",
    "    (AHA_master['long_as'] >= -180) & \n",
    "    (AHA_master['long_as'] <= 180)\n",
    ")\n",
    "AHA_master2 = AHA_master2[valid_coords]\n",
    "\n",
    "print(f\"Number of hospitals with valid coordinates: {len(AHA_master2)}\")\n",
    "\n",
    "# Create GeoDataFrame\n",
    "hospitals = gpd.GeoDataFrame(\n",
    "    AHA_master2, \n",
    "    geometry=gpd.points_from_xy(AHA_master2.long_as, AHA_master2.lat_as),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "print(f\"Successfully created GeoDataFrame with {len(hospitals)} hospitals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter hospitals with valid coordinates and implementation scores\n",
    "valid_hospitals = hospitals.dropna(subset=['long_as', 'lat_as', 'aipred_it'])\n",
    "valid_geo_hospitals = hospitals.dropna(subset=['long_as', 'lat_as'])\n",
    "# Create a GeoDataFrame\n",
    "hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_hospitals.long_as, valid_hospitals.lat_as),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "geo_hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_geo_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_geo_hospitals.long_as, valid_geo_hospitals.lat_as),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a projected CRS for accurate distance calculations\n",
    "hospitals_gdf_projected = hospitals_gdf.to_crs(epsg=3857) # projected coordinate system using flat, 2D plane to represent Earth's surface \n",
    "geo_hospitals_gdf_projected = geo_hospitals_gdf.to_crs(epsg=3857) # projected coordinate system using flat, 2D plane to represent Earth's surface \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add census division column to the dataframe\n",
    "hospitals_gdf_projected['division'] = hospitals_gdf_projected['mstate_it'].map(state_to_division)\n",
    "geo_hospitals_gdf_projected['division'] = geo_hospitals_gdf_projected['mstate_it'].map(state_to_division)\n",
    "\n",
    "# Loop through each census division and create a heatmap\n",
    "divisions = [\n",
    "    'New England', 'Mid Atlantic', 'South Atlantic', \n",
    "    'East North Central', 'East South Central', 'West North Central',\n",
    "    'West South Central', 'Mountain', 'Pacific'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Run Hotspot analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# ========= Storey q-value helpers =========\n",
    "def storey_qvalue_python(pvals, lambdas=None):\n",
    "    \"\"\"\n",
    "    Simple Storey q-value fallback in pure Python.\n",
    "    \"\"\"\n",
    "    p = np.asarray(pvals, dtype=float)\n",
    "    m = p.size\n",
    "    if m == 0:\n",
    "        return p\n",
    "    if lambdas is None:\n",
    "        lambdas = np.arange(0.05, 0.95, 0.05)\n",
    "    pi0_vals = []\n",
    "    for lam in lambdas:\n",
    "        denom = 1.0 - lam\n",
    "        if denom <= 0:\n",
    "            continue\n",
    "        pi0_vals.append((p > lam).mean() / denom)\n",
    "    pi0 = min(1.0, np.min(pi0_vals)) if len(pi0_vals) else 1.0\n",
    "    order = np.argsort(p)\n",
    "    p_sorted = p[order]\n",
    "    q_sorted = pi0 * m * p_sorted / (np.arange(1, m + 1))\n",
    "    q_sorted = np.minimum.accumulate(q_sorted[::-1])[::-1]\n",
    "    q = np.empty_like(q_sorted)\n",
    "    q[order] = q_sorted\n",
    "    return np.clip(q, 0, 1)\n",
    "\n",
    "\n",
    "# ========= Classification =========\n",
    "def _classify_levels(sign, p_like):\n",
    "    \"\"\"\n",
    "    Map sign and p-like values to hotspot classes at 99, 95, 90 percent levels.\n",
    "    p_like can be unadjusted p, Bonferroni p, BH-FDR p, or Storey q.\n",
    "    \"\"\"\n",
    "    lab = np.array([\"Not Significant\"] * len(p_like), dtype=object)\n",
    "    lab[(sign > 0) & (p_like <= 0.01)] = \"Hotspot (99%)\"\n",
    "    lab[(sign < 0) & (p_like <= 0.01)] = \"Coldspot (99%)\"\n",
    "    lab[(sign > 0) & (p_like > 0.01) & (p_like <= 0.05)] = \"Hotspot (95%)\"\n",
    "    lab[(sign < 0) & (p_like > 0.01) & (p_like <= 0.05)] = \"Coldspot (95%)\"\n",
    "    lab[(sign > 0) & (p_like > 0.05) & (p_like <= 0.1)] = \"Hotspot (90%)\"\n",
    "    lab[(sign < 0) & (p_like > 0.05) & (p_like <= 0.1)] = \"Coldspot (90%)\"\n",
    "    return lab\n",
    "\n",
    "# ========= Core Gi* function =========\n",
    "def calculate_gi_star_all(gdf, value_column, k=6):\n",
    "    \"\"\"\n",
    "    Compute local Getis-Ord Gi* with k-NN (including self) and multiple testing corrections.\n",
    "\n",
    "    Inputs\n",
    "    - gdf: GeoDataFrame with Point geometry (prefer EPSG:5070)\n",
    "    - value_column: column to analyze (numeric)\n",
    "    - k: number of nearest neighbors (self included via k+1)\n",
    "\n",
    "    Outputs\n",
    "    Returns a copy of gdf (rows with non-null value) with these columns:\n",
    "    - gi_star: raw Gi* sum over neighbors\n",
    "    - z_score: standardized Gi* Z\n",
    "    - p_unadj: unadjusted two-sided p\n",
    "    - p_bonf: Bonferroni adjusted p\n",
    "    - p_bh: BH-FDR adjusted p\n",
    "    - q_storey: Storey q-value\n",
    "    - hotspot_unadj: class from p_unadj\n",
    "    - hotspot_bonf: class from p_bonf\n",
    "    - hotspot_bh: class from p_bh\n",
    "    - hotspot_storey: class from q_storey\n",
    "    \"\"\"\n",
    "    # coords and values\n",
    "    coords = np.vstack((gdf.geometry.x, gdf.geometry.y)).T\n",
    "    values = gdf[value_column].to_numpy(dtype=float)\n",
    "    valid = ~np.isnan(values)\n",
    "    coords, values = coords[valid], values[valid]\n",
    "    out = gdf.loc[valid].copy()\n",
    "    n = len(values)\n",
    "    if n <= k:\n",
    "        print(f\"Error: Not enough observations ({n}) for k={k}\")\n",
    "        return gdf\n",
    "\n",
    "    # kNN including self (k+1)\n",
    "    nn = NearestNeighbors(n_neighbors=k+1)\n",
    "    nn.fit(coords)\n",
    "    _, indices = nn.kneighbors(coords)  # shape (n, k+1)\n",
    "\n",
    "    # global stats (Ord & Getis, 1995)\n",
    "    x_bar = values.mean()\n",
    "    S = np.sqrt((np.sum(values**2) / n) - x_bar**2)\n",
    "\n",
    "    # Gi*, Z, p\n",
    "    Gs = np.zeros(n)\n",
    "    Zs = np.zeros(n)\n",
    "    Ps = np.ones(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        neigh = indices[i]             # length k+1\n",
    "        wi_sum = float(len(neigh))     # sum w_ij (binary)\n",
    "        wi2_sum = wi_sum               # sum w_ij^2 (binary)\n",
    "        Gs[i] = np.sum(values[neigh])  # observed sum\n",
    "        EGi = x_bar * wi_sum           # expected\n",
    "        varGi = (S**2) * ((n * wi2_sum - wi_sum**2) / (n - 1)) if n > 1 else 0.0\n",
    "        if varGi > 0 and not np.isnan(varGi):\n",
    "            Zs[i] = (Gs[i] - EGi) / np.sqrt(varGi)\n",
    "            Ps[i] = 2 * (1 - stats.norm.cdf(abs(Zs[i])))\n",
    "        else:\n",
    "            Zs[i] = 0.0\n",
    "            Ps[i] = 1.0\n",
    "\n",
    "    # multiple testing corrections\n",
    "    m = n\n",
    "    p_unadj = Ps\n",
    "    p_bonf = np.clip(p_unadj * m, 0, 1)\n",
    "    _, p_bh, _, _ = multipletests(p_unadj, alpha=0.05, method='fdr_bh')\n",
    "    q_storey = storey_qvalue_python(p_unadj)\n",
    "\n",
    "    # labels\n",
    "    sign = np.sign(Zs)\n",
    "    out['gi_star'] = Gs\n",
    "    out['z_score'] = Zs\n",
    "    out['p_unadj'] = p_unadj\n",
    "    out['p_bonf'] = p_bonf\n",
    "    out['p_bh'] = p_bh\n",
    "    out['q_storey'] = q_storey\n",
    "    out['hotspot_unadj'] = _classify_levels(sign, p_unadj)\n",
    "    out['hotspot_bonf'] = _classify_levels(sign, p_bonf)\n",
    "    out['hotspot_bh'] = _classify_levels(sign, p_bh)\n",
    "    out['hotspot_storey'] = _classify_levels(sign, q_storey)\n",
    "\n",
    "    # column annotations for reference\n",
    "    out.attrs[\"columns_doc\"] = {\n",
    "        \"gi_star\": \"raw Gi* statistic (Ord & Getis, 1995)\",\n",
    "        \"z_score\": \"standardized Z-score for Gi*\",\n",
    "        \"p_unadj\": \"unadjusted two-sided p-value\",\n",
    "        \"p_bonf\": \"Bonferroni-adjusted p-value\",\n",
    "        \"p_bh\": \"Benjamini-Hochberg FDR-adjusted p-value\",\n",
    "        \"q_storey\": \"Storey q-value (FDR with pi0 estimated)\",\n",
    "        \"hotspot_unadj\": \"hotspot class from unadjusted p\",\n",
    "        \"hotspot_bonf\": \"hotspot class from Bonferroni p\",\n",
    "        \"hotspot_bh\": \"hotspot class from BH-FDR p\",\n",
    "        \"hotspot_storey\": \"hotspot class from Storey q-value\"\n",
    "    }\n",
    "\n",
    "    # quick summary\n",
    "    def _cnt(col, key): return int(out[col].str.contains(key).sum())\n",
    "    print(f\"Total locations: {n}\")\n",
    "    print(f\"Unadj  Hot:{_cnt('hotspot_unadj','Hotspot')}  Cold:{_cnt('hotspot_unadj','Coldspot')}\")\n",
    "    print(f\"BH-FDR Hot:{_cnt('hotspot_bh','Hotspot')}  Cold:{_cnt('hotspot_bh','Coldspot')}\")\n",
    "    print(f\"Storey Hot:{_cnt('hotspot_storey','Hotspot')}  Cold:{_cnt('hotspot_storey','Coldspot')}\")\n",
    "    print(f\"Bonf  Hot:{_cnt('hotspot_bonf','Hotspot')}  Cold:{_cnt('hotspot_bonf','Coldspot')}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4_3 run hotspot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Conduct hotspot analysis for the entire US\n",
    "print(\"Performing hotspot analysis for the entire US...\")\n",
    "base_hotspot_results = calculate_gi_star_all(hospitals_gdf_projected, 'ai_base_score_imputed', k=6)\n",
    "breadth_hotspot_results = calculate_gi_star_all(hospitals_gdf_projected, 'ai_base_breadth_score_imputed', k=6)\n",
    "dev_hotspot_results = calculate_gi_star_all(hospitals_gdf_projected, 'ai_base_dev_score_imputed', k=6)\n",
    "eval2023_hotspot_results = calculate_gi_star_all(hospitals_gdf_projected, 'ai_base_eval_score_2023_imputed', k=6)\n",
    "eval2024_hotspot_results = calculate_gi_star_all(hospitals_gdf_projected, 'ai_base_eval_score_2024_imputed', k=6)\n",
    "llm_hotspot_results = calculate_gi_star_all(hospitals_gdf_projected, 'llm_readiness_score', k=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Hotspot visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hotspot Bubble Chart Visualizations\n",
    "\n",
    "\n",
    "# Create colormap\n",
    "hotspot_cmap = LinearSegmentedColormap.from_list(\n",
    "    'hotspot_cmap', \n",
    "    ['#333333', '#737373', '#bfbfbf', '#f2f2f2', '#4d94ff', '#0050b3', '#001f4d']\n",
    ")\n",
    "\n",
    "# 1. State-Level Analysis\n",
    "def create_state_bubble_chart(hotspot_results):\n",
    "    # Group by state\n",
    "    state_summary = hotspot_results.groupby('mstate_it').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'Hotspot %': 100 * len(x[x['hotspot_type'].str.contains('Hotspot')]) / len(x),\n",
    "            'Coldspot %': 100 * len(x[x['hotspot_type'].str.contains('Coldspot')]) / len(x),\n",
    "            'Total': len(x),\n",
    "            'Mean Z': x['z_score'].mean()\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    # Filter states with sufficient data\n",
    "    state_summary = state_summary[state_summary['Total'] >= 20]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 9))\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = ax.scatter(\n",
    "        state_summary['Hotspot %'],\n",
    "        state_summary['Coldspot %'],\n",
    "        s=state_summary['Total'] * 3.5,\n",
    "        c=state_summary['Mean Z'],\n",
    "        cmap=hotspot_cmap,\n",
    "        alpha=0.85,\n",
    "        edgecolors='black',\n",
    "        linewidths=0.5,\n",
    "        vmin=-2.5,\n",
    "        vmax=2.5\n",
    "    )\n",
    "    \n",
    "    # Add state labels\n",
    "    for state, row in state_summary.iterrows():\n",
    "        ax.annotate(\n",
    "            state,\n",
    "            (row['Hotspot %'], row['Coldspot %']),\n",
    "            xytext=(3, 3),\n",
    "            textcoords='offset points',\n",
    "            fontsize=9,\n",
    "            fontweight='bold',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.85)\n",
    "        )\n",
    "    \n",
    "    # Add reference lines\n",
    "    ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    ax.axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = fig.colorbar(scatter, ax=ax, shrink=0.9)\n",
    "    cbar.set_label('Mean Z-Score', fontsize=12)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_title('Spatial Clustering Patterns by State', fontsize=16)\n",
    "    ax.set_xlabel('Hotspot Percentage', fontsize=13)\n",
    "    ax.set_ylabel('Coldspot Percentage', fontsize=13)\n",
    "    ax.set_xlim(-5, 105)\n",
    "    ax.set_ylim(-5, 105)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    \n",
    "    plt.tight_lat()\n",
    "    return fig, state_summary\n",
    "\n",
    "# 2. Division-Level Analysis\n",
    "def create_division_bubble_chart(hotspot_results):\n",
    "    # Group by division\n",
    "    division_summary = hotspot_results.groupby('division').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'Hotspot %': 100 * len(x[x['hotspot_type'].str.contains('Hotspot')]) / len(x),\n",
    "            'Coldspot %': 100 * len(x[x['hotspot_type'].str.contains('Coldspot')]) / len(x),\n",
    "            'Total': len(x),\n",
    "            'Mean Z': x['z_score'].mean()\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 9))\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = ax.scatter(\n",
    "        division_summary['Hotspot %'],\n",
    "        division_summary['Coldspot %'],\n",
    "        s=division_summary['Total'] * 1.2,\n",
    "        c=division_summary['Mean Z'],\n",
    "        cmap=hotspot_cmap,\n",
    "        alpha=0.85,\n",
    "        edgecolors='black',\n",
    "        linewidths=0.7,\n",
    "        vmin=-2.5,\n",
    "        vmax=2.5\n",
    "    )\n",
    "    \n",
    "    # Add division labels\n",
    "    for division, row in division_summary.iterrows():\n",
    "        ax.annotate(\n",
    "            division,\n",
    "            (row['Hotspot %'], row['Coldspot %']),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=11,\n",
    "            fontweight='bold',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.4\", fc=\"white\", ec=\"gray\", alpha=0.9)\n",
    "        )\n",
    "    \n",
    "    # Add reference lines\n",
    "    ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    ax.axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = fig.colorbar(scatter, ax=ax, shrink=0.9)\n",
    "    cbar.set_label('Mean Z-Score', fontsize=12)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_title('Spatial Clustering Patterns by Census Division', fontsize=16)\n",
    "    ax.set_xlabel('Hotspot Percentage', fontsize=13)\n",
    "    ax.set_ylabel('Coldspot Percentage', fontsize=13)\n",
    "    ax.set_xlim(-5, 105)\n",
    "    ax.set_ylim(-5, 105)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    \n",
    "    plt.tight_lat()\n",
    "    return fig, division_summary\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
