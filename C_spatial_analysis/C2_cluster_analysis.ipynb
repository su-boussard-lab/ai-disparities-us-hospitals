{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2. Spatial Cluster Analysis\n",
    "\n",
    "**Description**  \n",
    "This section conducts comprehensive spatial analysis to identify geographic patterns and clustering in hospital-level AI adoption across the United States. The analysis employs multiple complementary approaches to characterize spatial autocorrelation, identify statistically significant clusters, and examine regional disparities in AI implementation.\n",
    "\n",
    "**Purpose**  \n",
    "To explore patterns in hospital-level AI adoption across geographic regions (state or census division), aiding interpretation of implementation disparities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 load necessary libraries, functions and preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import contextily as ctx\n",
    "import scipy.spatial as spatial\n",
    "from scipy.stats import zscore\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom functions to calculate AI scores \n",
    "# 1. Primay variable of interest: ai_base_score \n",
    "# 2. Seconoday variables of interest: ai_base_breadth_score, ai_base_dev_score, ai_base_eval_score \n",
    "# 2.1 ai_base_breadth_score : This score reflects the breadth of the use_cases \n",
    "# 2.2 ai_base_dev_score : This score reflects the degree of model development \n",
    "# 2.3 ai_base_eval_score : This score reflects the degree of model evaluation in bias and accuracy \n",
    "\n",
    "def calculate_base_ai_implementation_row(row):\n",
    "    \"\"\"\n",
    "    Calculate base AI implementation score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: Base AI implementation score\n",
    "    \"\"\"\n",
    "    # Base AI implementation score (continuous)\n",
    "    # Return None if the input value is null\n",
    "    if pd.isna(row['aipred_it']):\n",
    "        return None\n",
    "    elif row['aipred_it'] == 1:  # Machine Learning\n",
    "        return 2\n",
    "    elif row['aipred_it'] == 2:  # Other Non-Machine Learning Predictive Models\n",
    "        return 1\n",
    "    else:  # Neither (3) or Do not know (4)\n",
    "        return 0\n",
    "\n",
    "def calculate_ai_implementation_breadth_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI implementation breadth score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI implementation breadth score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        breadth_score = base_score\n",
    "        # Implementation Breadth Score - count use cases\n",
    "        use_case_cols = ['aitraj_it', 'airfol_it', 'aimhea_it', 'airect_it', \n",
    "                     'aibill_it', 'aische_it', 'aipoth_it', 'aicloth_it']\n",
    "        for col in use_case_cols:\n",
    "            if row[col] is None:\n",
    "                breadth_score += 0\n",
    "            else:\n",
    "                breadth_score += row[col] * 0.25  # 0.25 points per use case\n",
    "        return breadth_score\n",
    "\n",
    "def calculate_ai_development_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI development score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI development score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0 \n",
    "    else:\n",
    "        dev_score = base_score\n",
    "        if 'mlsed_it' in row and pd.notna(row['mlsed_it']):\n",
    "            dev_score += row['mlsed_it'] * 2  # Self-developed\n",
    "        if 'mldev_it' in row and pd.notna(row['mldev_it']):\n",
    "            dev_score += row['mldev_it']  # EHR developer\n",
    "        if 'mlthd_it' in row and pd.notna(row['mlthd_it']):\n",
    "            dev_score += row['mlthd_it']  # Third-party\n",
    "        if 'mlpubd_it' in row and pd.notna(row['mlpubd_it']):\n",
    "            dev_score += row['mlpubd_it'] * 0.5  # Public domain\n",
    "        return dev_score\n",
    "\n",
    "def calculate_ai_evaluation_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI evaluation score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI evaluation score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        eval_score = base_score\n",
    "        # For model accuracy (MLACCU)\n",
    "        if row['mlaccu_it'] is None:\n",
    "            eval_score += 0\n",
    "        elif row['mlaccu_it'] == 1:  # All models\n",
    "            eval_score += 1\n",
    "        elif row['mlaccu_it'] == 2:  # Most models\n",
    "            eval_score += 0.75\n",
    "        elif row['mlaccu_it'] == 3:  # Some models\n",
    "            eval_score += 0.5\n",
    "        elif row['mlaccu_it'] == 4:  # Few models\n",
    "            eval_score += 0.25\n",
    "        # For None (5) or Do not know (6), no points added\n",
    "    \n",
    "    # For model bias (MLBIAS)\n",
    "        if row['mlbias_it'] is None:\n",
    "            eval_score += 0\n",
    "        elif row['mlbias_it'] == 1:  # All models\n",
    "            eval_score += 1\n",
    "        elif row['mlbias_it'] == 2:  # Most models\n",
    "            eval_score += 0.75\n",
    "        elif row['mlbias_it'] == 3:  # Some models\n",
    "            eval_score += 0.5\n",
    "        elif row['mlbias_it'] == 4:  # Few models\n",
    "            eval_score += 0.25\n",
    "        # For None (5) or Do not know (6), no points added\n",
    "    \n",
    "        return eval_score\n",
    "\n",
    "def calculate_all_ai_scores_row(row):\n",
    "    \"\"\"\n",
    "    Calculate all AI/ML implementation scores as continuous measures for a single row.\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with all calculated scores\n",
    "    \"\"\"\n",
    "    # Calculate all scores\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    breadth_score = calculate_ai_implementation_breadth_row(row)\n",
    "    dev_score = calculate_ai_development_row(row)\n",
    "    eval_score = calculate_ai_evaluation_row(row)\n",
    "    \n",
    "    return {\n",
    "        'ai_base_score': base_score,\n",
    "        'ai_base_breadth_score': breadth_score,\n",
    "        'ai_base_dev_score': dev_score,\n",
    "        'ai_base_eval_score': eval_score\n",
    "    }\n",
    "\n",
    "def apply_ai_scores_to_dataframe(df):\n",
    "    \"\"\"\n",
    "    Apply all AI score calculations row by row to a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: A pandas DataFrame with hospital data\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with added AI score columns\n",
    "    \"\"\"\n",
    "    # Initialize empty columns for scores\n",
    "    df['ai_base_score'] = float('nan')\n",
    "    df['ai_base_breadth_score'] = float('nan')\n",
    "    df['ai_base_dev_score'] = float('nan')\n",
    "    df['ai_base_eval_score'] = float('nan')\n",
    "    \n",
    "    # Apply row by row calculations\n",
    "    for index, row in df.iterrows():\n",
    "        scores = calculate_all_ai_scores_row(row)\n",
    "        for score_name, score_value in scores.items():\n",
    "            df.at[index, score_name] = score_value\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed AHA dataframe from A2 notebook \n",
    "AHA_master = pd.read_csv('./data/AHA_master_external_data.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop hospitals that did not respond to IT supplement \n",
    "AHA_IT = AHA_master[~AHA_master.id_it.isnull()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Data engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_IT2 = apply_ai_scores_to_dataframe(AHA_IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load STATE shape file \n",
    "os.environ['SHAPE_RESTORE_SHX'] = 'YES'\n",
    "states = gpd.read_file('../temp_shp/cb_2018_us_state_500k.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing or invalid coordinates\n",
    "AHA_IT = AHA_IT.dropna(subset=['lat_as', 'long_as'])\n",
    "\n",
    "# Filter out invalid coordinates\n",
    "valid_coords = (\n",
    "    (AHA_IT['lat_as'] != 0) & \n",
    "    (AHA_IT['long_as'] != 0) &\n",
    "    (AHA_IT['lat_as'] >= -90) & \n",
    "    (AHA_IT['lat_as'] <= 90) &\n",
    "    (AHA_IT['long_as'] >= -180) & \n",
    "    (AHA_IT['long_as'] <= 180)\n",
    ")\n",
    "AHA_IT = AHA_IT[valid_coords]\n",
    "\n",
    "# Create GeoDataFrame\n",
    "hospitals = gpd.GeoDataFrame(\n",
    "    AHA_IT, \n",
    "    geometry=gpd.points_from_xy(AHA_IT.long_as, AHA_IT.lat_as),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter hospitals with valid coordinates and implementation scores\n",
    "valid_hospitals = hospitals.dropna(subset=['long_as', 'lat_as', 'aipred_it'])\n",
    "valid_geo_hospitals = hospitals.dropna(subset=['long_as', 'lat_as'])\n",
    "# Create a GeoDataFrame\n",
    "hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_hospitals.long_as, valid_hospitals.lat_as),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "geo_hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_geo_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_geo_hospitals.long_as, valid_geo_hospitals.lat_as),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a projected CRS for accurate distance calculations\n",
    "hospitals_gdf_projected = hospitals_gdf.to_crs(epsg=3857) # projected coordinate system using flat, 2D plane to represent Earth's surface \n",
    "geo_hospitals_gdf_projected = geo_hospitals_gdf.to_crs(epsg=3857) # projected coordinate system using flat, 2D plane to represent Earth's surface \n",
    "\n",
    "\n",
    "geo_hospitals_gdf_projected['ML_implementation_score'] = geo_hospitals_gdf_projected['aipred_it'].map({\n",
    "    1: 3,  # ML gets highest score\n",
    "    2: 2,  # Non-ML gets middle score\n",
    "    3: 1,  # Neither gets lowest score\n",
    "    4: 1,   # Don't know gets lowest score\n",
    "    None: 0,\n",
    "    0: 0 \n",
    "})\n",
    "geo_hospitals_gdf_projected.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 AI/ML barplot (across census division)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of states to regions\n",
    "# Create a mapping of states to census divisions based on provided image\n",
    "state_to_division = {\n",
    "    # Division 1: New England\n",
    "    'ME': 'New England', 'NH': 'New England', 'VT': 'New England', \n",
    "    'MA': 'New England', 'RI': 'New England', 'CT': 'New England',\n",
    "    \n",
    "    # Division 2: Mid Atlantic\n",
    "    'NY': 'Mid Atlantic', 'NJ': 'Mid Atlantic', 'PA': 'Mid Atlantic',\n",
    "    \n",
    "    # Division 3: South Atlantic\n",
    "    'DE': 'South Atlantic', 'MD': 'South Atlantic', 'DC': 'South Atlantic',\n",
    "    'VA': 'South Atlantic', 'WV': 'South Atlantic', 'NC': 'South Atlantic',\n",
    "    'SC': 'South Atlantic', 'GA': 'South Atlantic', 'FL': 'South Atlantic',\n",
    "    \n",
    "    # Division 4: East North Central\n",
    "    'OH': 'East North Central', 'IN': 'East North Central', 'IL': 'East North Central',\n",
    "    'MI': 'East North Central', 'WI': 'East North Central',\n",
    "    \n",
    "    # Division 5: East South Central\n",
    "    'KY': 'East South Central', 'TN': 'East South Central', \n",
    "    'AL': 'East South Central', 'MS': 'East South Central',\n",
    "    \n",
    "    # Division 6: West North Central\n",
    "    'MN': 'West North Central', 'IA': 'West North Central', 'MO': 'West North Central',\n",
    "    'ND': 'West North Central', 'SD': 'West North Central', 'NE': 'West North Central',\n",
    "    'KS': 'West North Central',\n",
    "    \n",
    "    # Division 7: West South Central\n",
    "    'AR': 'West South Central', 'LA': 'West South Central', \n",
    "    'OK': 'West South Central', 'TX': 'West South Central',\n",
    "    \n",
    "    # Division 8: Mountain\n",
    "    'MT': 'Mountain', 'ID': 'Mountain', 'WY': 'Mountain', 'CO': 'Mountain',\n",
    "    'NM': 'Mountain', 'AZ': 'Mountain', 'UT': 'Mountain', 'NV': 'Mountain',\n",
    "    \n",
    "    # Division 9: Pacific\n",
    "    'WA': 'Pacific', 'OR': 'Pacific', 'CA': 'Pacific', \n",
    "    'AK': 'Pacific', 'HI': 'Pacific',\n",
    "    \n",
    "    # Territories\n",
    "    'PR': 'Territories', 'GU': 'Territories', 'VI': 'Territories', \n",
    "    'AS': 'Territories', 'MP': 'Territories'\n",
    "}\n",
    "\n",
    "# Add census division column to the dataframe\n",
    "hospitals_gdf_projected['division'] = hospitals_gdf_projected['mstate_it'].map(state_to_division)\n",
    "geo_hospitals_gdf_projected['division'] = geo_hospitals_gdf_projected['mstate_it'].map(state_to_division)\n",
    "\n",
    "# Loop through each census division \n",
    "divisions = [\n",
    "    'New England', 'Mid Atlantic', 'South Atlantic', \n",
    "    'East North Central', 'East South Central', 'West North Central',\n",
    "    'West South Central', 'Mountain', 'Pacific'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use  existing ai_types\n",
    "ai_types = [3, 2, 1]  \n",
    "\n",
    "# Map the numeric AI types to labels for better readability\n",
    "ai_type_labels = {\n",
    "    3: 'ML',\n",
    "    2: 'Non-ML Predictive Model',\n",
    "    1: 'Do not know/Neither',\n",
    "    0: 'No Response'  # Assuming 0 or NaN is used for no response\n",
    "}\n",
    "\n",
    "# Create a DataFrame to hold the distribution of AI types by division\n",
    "division_ai_counts = pd.DataFrame(index=divisions, columns=ai_types + [0])  # Include 0 for no response\n",
    "# Fill the DataFrame with counts\n",
    "for division in divisions:\n",
    "    geo_division_data = geo_hospitals_gdf_projected[geo_hospitals_gdf_projected['division'] == division]\n",
    "    \n",
    "    # Count no responses (either 0 or NaN)\n",
    "    no_response_count = len(geo_division_data[geo_division_data['ML_implementation_score'].isna() | \n",
    "                                         (geo_division_data['ML_implementation_score'] == 0)])\n",
    "    division_ai_counts.loc[division, 0] = no_response_count\n",
    "    \n",
    "    # Count other AI types\n",
    "    for ai_type in ai_types:\n",
    "        division_ai_counts.loc[division, ai_type] = len(geo_division_data[geo_division_data['ML_implementation_score'] == ai_type])\n",
    "division_ai_counts\n",
    "\n",
    "# Calculate total hospitals per division\n",
    "division_totals = division_ai_counts.sum(axis=1)\n",
    "\n",
    "# Calculate percentages\n",
    "division_ai_dist = division_ai_counts.div(division_ai_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Calculate grid dimensions\n",
    "n_divisions = len(divisions)\n",
    "n_cols = 3  # Number of columns in the grid\n",
    "n_rows = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Map the numeric AI types to labels for better readability\n",
    "ai_type_labels = {\n",
    "    3: 'AI/ML',\n",
    "    2: 'Non-AI/ML Predictive Model',\n",
    "    1: 'Do not know/Neither',\n",
    "    0: 'No Response'\n",
    "}\n",
    "\n",
    "# Rename columns for better readability\n",
    "division_ai_counts = division_ai_counts.rename(columns=ai_type_labels)\n",
    "\n",
    "# Calculate percentages\n",
    "percentages = division_ai_counts.div(division_ai_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Sort divisions by total\n",
    "total_hospitals = division_ai_counts.sum(axis=1)\n",
    "sorted_divisions = division_ai_counts.loc[total_hospitals.sort_values(ascending=False).index]\n",
    "sorted_percentages = percentages.loc[total_hospitals.sort_values(ascending=False).index]\n",
    "\n",
    "# Define publication-quality colors\n",
    "colors = ['#3366CC', '#66CCEE', '#EEEEFF', '#CCCCCC']\n",
    "\n",
    "# Create figure with a higher DPI for better quality\n",
    "fig, ax = plt.subplots(figsize=(10, 7), dpi=300)\n",
    "\n",
    "# Create stacked bar chart\n",
    "bars = sorted_divisions.plot(kind='bar', stacked=True, color=colors, ax=ax, width=0.7, edgecolor='white', linewidth=0.5)\n",
    "\n",
    "# Add percentage labels on top of each segment\n",
    "for i, division in enumerate(sorted_divisions.index):\n",
    "    cumulative_height = 0\n",
    "    for j, col in enumerate(sorted_divisions.columns):\n",
    "        height = sorted_divisions.loc[division, col]\n",
    "        pct = sorted_percentages.loc[division, col]\n",
    "        \n",
    "        if height > 0:  # Only label non-zero values\n",
    "            # Position at the top of each segment\n",
    "            y_pos = cumulative_height + height\n",
    "            \n",
    "            # For small percentages, only show if they're at least 1%\n",
    "            if pct >= 1:\n",
    "                # Position text at the top of each segment\n",
    "                ax.text(i, cumulative_height + (height * 0.5), f'{pct:.1f}%', \n",
    "                        ha='center', va='center', fontsize=9,\n",
    "                        color='black', fontweight='bold')\n",
    "            \n",
    "        cumulative_height += height\n",
    "\n",
    "ax.set_xlabel('Census Division', fontsize=14, labelpad=10, color='black')\n",
    "ax.set_ylabel('Number of Hospitals', fontsize=14, labelpad=10, color='black')\n",
    "ax.tick_params(axis='both', which='major', labelsize=12, colors='black')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right', color='black')\n",
    "plt.yticks(color='black')\n",
    "\n",
    "# Calculate combined percentage of ML and Non-ML Predictive Model\n",
    "combined_percentage = sorted_percentages['AI/ML'] + sorted_percentages['Non-AI/ML Predictive Model']\n",
    "\n",
    "# Add combined percentage labels above the total count\n",
    "for i, division in enumerate(sorted_divisions.index):\n",
    "    total = total_hospitals[division]\n",
    "    combined_pct = combined_percentage[division]\n",
    "    # Position the combined percentage above the total count\n",
    "    plt.text(i, total + 20, f'Model: {combined_pct:.1f}%', \n",
    "             ha='center', fontsize=8,  # Smaller font size\n",
    "             fontweight='normal',      # Normal weight instead of bold\n",
    "             color='#666666')          # Gray color\n",
    "\n",
    "\n",
    "plt.tight_lat()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Hospital clustering - nearest neighbor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nearest neighbor analysis (without relying on pointpats)\n",
    "print(\"Performing Custom Nearest Neighbor Analysis...\")\n",
    "\n",
    "# Extract coordinates\n",
    "coords = np.vstack((geo_hospitals_gdf_projected.geometry.x, geo_hospitals_gdf_projected.geometry.y)).T\n",
    "\n",
    "# Calculate distances between all pairs of points\n",
    "kdtree = spatial.KDTree(coords)\n",
    "distances, indices = kdtree.query(coords, k=2)  # k=2 to get the nearest neighbor (first one is self)\n",
    "mean_observed_nn_distance = np.mean(distances[:, 1])\n",
    "\n",
    "# Calculate the area - use the bounding box as an approximation\n",
    "x_min, y_min, x_max, y_max = geo_hospitals_gdf_projected.total_bounds\n",
    "area = (x_max - x_min) * (y_max - y_min)\n",
    "\n",
    "# Calculate point density\n",
    "n = len(coords)\n",
    "density = n / area\n",
    "\n",
    "# Expected mean distance for random distribution\n",
    "mean_expected_nn_distance = 0.5 / np.sqrt(density)\n",
    "\n",
    "# Calculate nearest neighbor ratio\n",
    "nn_ratio = mean_observed_nn_distance / mean_expected_nn_distance\n",
    "\n",
    "# Standard error\n",
    "se = 0.26136 / np.sqrt(n * density)\n",
    "\n",
    "# Z-score\n",
    "z_score = (mean_observed_nn_distance - mean_expected_nn_distance) / se\n",
    "\n",
    "# Calculate approximate p-value\n",
    "from scipy.stats import norm\n",
    "p_value = 2 * (1 - norm.cdf(abs(z_score)))  # two-tailed test\n",
    "\n",
    "print(f\"Nearest Neighbor Ratio: {nn_ratio:.3f}\")\n",
    "print(f\"z-score: {z_score:.3f}\")\n",
    "print(f\"p-value: {p_value:.3f}\")\n",
    "\n",
    "if z_score < -1.96:\n",
    "    print(\"Hospitals show significant clustering (p < 0.05)\")\n",
    "elif z_score > 1.96:\n",
    "    print(\"Hospitals show significant dispersion (p < 0.05)\")\n",
    "else:\n",
    "    print(\"Hospitals show random spatial pattern\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Moran's I spatial autocorrelation - AI and model implementation measures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 moran's I function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_morans_i(values, coords, k=5):\n",
    "    \"\"\"\n",
    "    Calculate Moran's I using PySAL library\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    values : array-like\n",
    "        The values to test for spatial autocorrelation\n",
    "    coords : array-like\n",
    "        Coordinates as (n, 2) array\n",
    "    k : int\n",
    "        Number of nearest neighbors\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (I, EI, z_score, p_value, n)\n",
    "        I: Moran's I statistic\n",
    "        EI: Expected value of I under null hypothesis\n",
    "        z_score: Standardized z-score\n",
    "        p_value: Two-tailed p-value\n",
    "        n: Number of observations\n",
    "    \"\"\"\n",
    "    # Convert inputs to numpy arrays\n",
    "    values = np.asarray(values).flatten()\n",
    "    coords = np.asarray(coords)\n",
    "    \n",
    "    # Handle missing values\n",
    "    valid_idx = ~np.isnan(values)\n",
    "    if not np.all(valid_idx):\n",
    "        values = values[valid_idx]\n",
    "        coords = coords[valid_idx]\n",
    "    \n",
    "    n = len(values)\n",
    "    if n <= 1 or np.var(values) == 0:\n",
    "        return {'moran_i': np.nan, 'expected_i': np.nan, 'z_score': np.nan, 'p_value': np.nan, 'n': n}\n",
    "    \n",
    "    try:\n",
    "        # Create k-nearest neighbor weights using PySAL\n",
    "        w = KNN.from_array(coords, k=k)\n",
    "        \n",
    "        # Calculate Moran's I using PySAL\n",
    "        moran = Moran(values, w)\n",
    "        \n",
    "        return {\n",
    "            'moran_i': moran.I,\n",
    "            'expected_i': moran.EI,\n",
    "            'z_score': moran.z_norm,\n",
    "            'p_value': moran.p_norm,\n",
    "            'n': n\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {'moran_i': np.nan, 'expected_i': np.nan, 'z_score': np.nan, 'p_value': np.nan, 'n': n}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 US moran's I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Moran's I Spatial Autocorrelation Analysis\n",
    "\n",
    "# Extract coordinates and target variable\n",
    "coords = np.vstack((hospitals_gdf_projected.geometry.x, hospitals_gdf_projected.geometry.y)).T\n",
    "values = hospitals_gdf_projected['ai_base_score'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates \n",
    "coords = np.vstack((geo_hospitals_gdf_projected.geometry.x, geo_hospitals_gdf_projected.geometry.y)).T\n",
    "\n",
    "# Define the variables to analyze \n",
    "variables = [\n",
    "    'ai_base_score', \n",
    "    'ai_base_breadth_score', \n",
    "    'ai_base_dev_score', \n",
    "    'ai_base_eval_score'\n",
    "]\n",
    "\n",
    "# Create a list to store results \n",
    "results_list = []\n",
    "\n",
    "# Calculate Moran's I for each variable \n",
    "for var in variables:\n",
    "    try:\n",
    "        # Get values for this variable\n",
    "        values = geo_hospitals_gdf_projected[var].values\n",
    "        \n",
    "        # Calculate Moran's I\n",
    "        result = calculate_morans_i(values, coords, k=5)\n",
    "        \n",
    "        # Determine pattern \n",
    "        if np.isnan(result['p_value']):\n",
    "            pattern = \"Invalid data\"\n",
    "        elif result['p_value'] < 0.05:\n",
    "            if result['moran_i'] > result['expected_i']:\n",
    "                pattern = \"Significant clustering\"\n",
    "            else:\n",
    "                pattern = \"Significant dispersion\"\n",
    "        else:\n",
    "            pattern = \"Random distribution\"\n",
    "        \n",
    "        # Add results to list \n",
    "        results_list.append({\n",
    "            'Variable': var,\n",
    "            'Moran I': result['moran_i'],\n",
    "            'Expected I': result['expected_i'],\n",
    "            'z-score': result['z_score'],\n",
    "            'p-value': result['p_value'],\n",
    "            'Pattern': pattern,\n",
    "            'n': result['n']\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        results_list.append({\n",
    "            'Variable': var,\n",
    "            'Moran I': np.nan,\n",
    "            'Expected I': np.nan,\n",
    "            'z-score': np.nan,\n",
    "            'p-value': np.nan,\n",
    "            'Pattern': f\"Error: {str(e)}\",\n",
    "            'n': np.nan\n",
    "        })\n",
    "\n",
    "# Create DataFrame \n",
    "moran_results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Format the numeric columns \n",
    "moran_results_df['Moran I'] = moran_results_df['Moran I'].round(4)\n",
    "moran_results_df['Expected I'] = moran_results_df['Expected I'].round(4)\n",
    "moran_results_df['z-score'] = moran_results_df['z-score'].round(4)\n",
    "moran_results_df['p-value'] = moran_results_df['p-value'].round(10)\n",
    "\n",
    "# Display the DataFrame \n",
    "print(\"\\nMoran's I Analysis Results (k=5 neighbors):\")\n",
    "moran_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 moran's I  US Census division "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Moran's I for each division and variable\n",
    "all_results = []\n",
    "\n",
    "for ai_var in ai_variables:\n",
    "    for division in mainland_divisions:\n",
    "        division_data = hospitals_gdf_projected[hospitals_gdf_projected['division'] == division]\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if len(division_data) < 10:\n",
    "            continue\n",
    "            \n",
    "        # Adjust k for smaller divisions\n",
    "        k = min(5, max(2, len(division_data) // 5))\n",
    "        \n",
    "        try:\n",
    "            coords = np.vstack((division_data.geometry.x, division_data.geometry.y)).T\n",
    "            values = division_data[ai_var].values\n",
    "            \n",
    "            result = calculate_morans_i(values, coords, k)\n",
    "            \n",
    "            if not np.isnan(result['moran_i']):\n",
    "                all_results.append({\n",
    "                    'AI_Variable': ai_var,\n",
    "                    'Division': division,\n",
    "                    'Morans_I': result['moran_i'],\n",
    "                    'p_value': result['p_value'],\n",
    "                    'z_score': result['z_score'],\n",
    "                    'expected_I': result['expected_i'],\n",
    "                    'n': result['n']\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "# Process results\n",
    "if all_results:\n",
    "    comprehensive_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Add pattern classification\n",
    "    comprehensive_df['Pattern'] = comprehensive_df.apply(lambda row: \n",
    "        'Clustering' if row['p_value'] < 0.05 and row['Morans_I'] > row['expected_I'] else\n",
    "        'Dispersion' if row['p_value'] < 0.05 and row['Morans_I'] < row['expected_I'] else\n",
    "        'Random', axis=1)\n",
    "    \n",
    "    # Create pivot table for Moran's I values\n",
    "    morans_pivot = comprehensive_df.pivot_table(\n",
    "        index='Division', \n",
    "        columns='AI_Variable', \n",
    "        values='Morans_I', \n",
    "        aggfunc='first'\n",
    "    ).round(4)\n",
    "    \n",
    "    # Create pivot table for significance\n",
    "    pvalue_pivot = comprehensive_df.pivot_table(\n",
    "        index='Division', \n",
    "        columns='AI_Variable', \n",
    "        values='p_value', \n",
    "        aggfunc='first'\n",
    "    ).round(4)\n",
    "    \n",
    "    print(\"Moran's I by Division and AI Variable:\")\n",
    "    print(morans_pivot)\n",
    "    \n",
    "    print(\"\\nP-values:\")\n",
    "    print(pvalue_pivot)\n",
    "    \n",
    "    # Summary statistics\n",
    "    for ai_var in ai_variables:\n",
    "        var_data = comprehensive_df[comprehensive_df['AI_Variable'] == ai_var]\n",
    "        clustering_count = len(var_data[var_data['Pattern'] == 'Clustering'])\n",
    "        total_divisions = len(var_data)\n",
    "        \n",
    "        print(f\"\\n{ai_var}: {clustering_count}/{total_divisions} divisions show significant clustering\")\n",
    "    \n",
    "    return comprehensive_df\n",
    "else:\n",
    "    print(\"No valid results calculated\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 moran's I  US state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of AI variables to analyze\n",
    "ai_variables = [\n",
    "    'ai_base_score', \n",
    "    'ai_base_breadth_score', \n",
    "    'ai_base_dev_score', \n",
    "    'ai_base_eval_score'\n",
    "]\n",
    "\n",
    "# Calculate Moran's I for each state and each AI variable\n",
    "print(\"\\n=== Moran's I by State for All AI Variables ===\")\n",
    "\n",
    "# Store results for all combinations\n",
    "all_state_results = []\n",
    "\n",
    "# Get list of states with hospitals\n",
    "states = hospitals_gdf_projected['mstate_it'].dropna().unique()\n",
    "print(f\"Found {len(states)} states with hospital data\")\n",
    "\n",
    "for ai_var in ai_variables:\n",
    "    print(f\"\\n--- Analyzing {ai_var} ---\")\n",
    "    state_morans = []\n",
    "    \n",
    "    for state in states:\n",
    "        state_data = hospitals_gdf_projected[hospitals_gdf_projected['mstate_it'] == state]\n",
    "        \n",
    "        # Need enough data points to calculate\n",
    "        if len(state_data) < 20:\n",
    "            # print(f\"{state}: Not enough data points for reliable Moran's I calculation ({len(state_data)} hospitals)\")\n",
    "            continue\n",
    "            \n",
    "        # Adjust k for smaller states\n",
    "        k = min(5, max(2, len(state_data) // 5))  # Ensure k is at least 2\n",
    "        \n",
    "        print(f\"Processing {state}: {len(state_data)} hospitals, k={k}\")\n",
    "            \n",
    "        try:\n",
    "            # Extract coordinates and values first\n",
    "            coords = np.vstack((state_data.geometry.x, state_data.geometry.y)).T\n",
    "            values = state_data[ai_var].values\n",
    "            \n",
    "            # Use PySAL function (returns dictionary)\n",
    "            result = calculate_morans_i(values, coords, k)\n",
    "            \n",
    "            # Check if we got valid results\n",
    "            if not np.isnan(result['moran_i']):\n",
    "                # Get state name if available\n",
    "                state_name = state\n",
    "                if 'mstate' in state_data.columns:\n",
    "                    state_names = state_data['mstate'].dropna().unique()\n",
    "                    if len(state_names) > 0:\n",
    "                        state_name = state_names[0]\n",
    "                \n",
    "                # Store results\n",
    "                all_state_results.append({\n",
    "                    'AI_Variable': ai_var,\n",
    "                    'State_Code': state,\n",
    "                    'State': state_name,\n",
    "                    'Morans_I': result['moran_i'],\n",
    "                    'p_value': result['p_value'],\n",
    "                    'z_score': result['z_score'],\n",
    "                    'expected_I': result['expected_i'],\n",
    "                    'n': result['n']\n",
    "                })\n",
    "                \n",
    "                print(f\"  Results: I={result['moran_i']:.4f}, p={result['p_value']:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {state}: Failed to calculate valid Moran's I\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  {state}: Error calculating Moran's I - {str(e)}\")\n",
    "\n",
    "# Check if we have any results\n",
    "if not all_state_results:\n",
    "    print(\"No valid Moran's I results calculated for any state. Check  data and function.\")\n",
    "else:\n",
    "    # Create a comprehensive dataframe\n",
    "    comprehensive_state_df = pd.DataFrame(all_state_results)\n",
    "    \n",
    "    # Format the numeric columns\n",
    "    comprehensive_state_df['Morans_I'] = comprehensive_state_df['Morans_I'].round(4)\n",
    "    comprehensive_state_df['p_value'] = comprehensive_state_df['p_value'].round(4)\n",
    "    comprehensive_state_df['z_score'] = comprehensive_state_df['z_score'].round(4)\n",
    "    comprehensive_state_df['expected_I'] = comprehensive_state_df['expected_I'].round(4)\n",
    "    \n",
    "    # Add significance level column\n",
    "    comprehensive_state_df['Significance'] = comprehensive_state_df['p_value'].apply(lambda x: \n",
    "        '*****' if x < 0.0001 else \n",
    "        '****' if x < 0.001 else \n",
    "        '***' if x < 0.01 else \n",
    "        '**' if x < 0.05 else \n",
    "        '*' if x < 0.1 else \n",
    "        'ns')\n",
    "    \n",
    "    # Add pattern column\n",
    "    comprehensive_state_df['Pattern'] = comprehensive_state_df.apply(lambda row: \n",
    "        'Significant clustering' if row['p_value'] < 0.05 and row['Morans_I'] > row['expected_I'] else\n",
    "        'Significant dispersion' if row['p_value'] < 0.05 and row['Morans_I'] < row['expected_I'] else\n",
    "        'Random distribution', axis=1)\n",
    "    \n",
    "    # Display results by AI variable (abbreviated version)\n",
    "    for ai_var in ai_variables:\n",
    "        var_df = comprehensive_state_df[comprehensive_state_df['AI_Variable'] == ai_var].copy()\n",
    "        \n",
    "        if len(var_df) > 0:\n",
    "            # Sort by Moran's I value\n",
    "            var_df = var_df.sort_values('Morans_I', ascending=False)\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Top 10 States for {ai_var.upper()} (by Moran's I)\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # Show top 10 only for space\n",
    "            top_10 = var_df.head(10)\n",
    "            display_df = top_10[['State_Code', 'State', 'Morans_I', 'z_score', 'p_value', 'Significance', 'Pattern']].copy()\n",
    "            display_df = display_df.rename(columns={\n",
    "                'State_Code': 'Code',\n",
    "                'Morans_I': 'Moran\\'s I',\n",
    "                'z_score': 'Z-Score',\n",
    "                'p_value': 'P-Value'\n",
    "            })\n",
    "            print(display_df.to_string(index=False))\n",
    "            \n",
    "            # Summary for this variable\n",
    "            clustering = len(var_df[var_df['Pattern'] == 'Significant clustering'])\n",
    "            dispersion = len(var_df[var_df['Pattern'] == 'Significant dispersion'])\n",
    "            random = len(var_df[var_df['Pattern'] == 'Random distribution'])\n",
    "            \n",
    "            print(f\"\\nSummary for {ai_var}:\")\n",
    "            print(f\"  Significant clustering: {clustering} states\")\n",
    "            print(f\"  Significant dispersion: {dispersion} states\")\n",
    "            print(f\"  Random distribution: {random} states\")\n",
    "    \n",
    "    # Overall summary across all variables\n",
    "    print(f\"\\n{'='*120}\")\n",
    "    print(\"COMPREHENSIVE STATE COMPARISON ACROSS ALL AI VARIABLES\")\n",
    "    print(f\"{'='*120}\")\n",
    "    \n",
    "    # Create comprehensive pivot tables (SAME AS DIVISION VERSION)\n",
    "    print(\"COMPREHENSIVE COMPARISON TABLE:\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    # Create pivot tables for different metrics\n",
    "    morans_pivot = comprehensive_state_df.pivot_table(\n",
    "        index=['State_Code', 'State'], \n",
    "        columns='AI_Variable', \n",
    "        values='Morans_I', \n",
    "        aggfunc='first'\n",
    "    ).round(4)\n",
    "    \n",
    "    zscore_pivot = comprehensive_state_df.pivot_table(\n",
    "        index=['State_Code', 'State'], \n",
    "        columns='AI_Variable', \n",
    "        values='z_score', \n",
    "        aggfunc='first'\n",
    "    ).round(2)\n",
    "    \n",
    "    pvalue_pivot = comprehensive_state_df.pivot_table(\n",
    "        index=['State_Code', 'State'], \n",
    "        columns='AI_Variable', \n",
    "        values='p_value', \n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    significance_pivot = comprehensive_state_df.pivot_table(\n",
    "        index=['State_Code', 'State'], \n",
    "        columns='AI_Variable', \n",
    "        values='Significance', \n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    # Create combined display tables\n",
    "    print(\"\\n1. Moran's I Values:\")\n",
    "    print(morans_pivot.to_string())\n",
    "    \n",
    "    print(\"\\n2. Z-Scores:\")\n",
    "    print(zscore_pivot.to_string())\n",
    "    \n",
    "    print(\"\\n3. P-Values with Significance Stars:\")\n",
    "    # Combine p-values with significance stars\n",
    "    pvalue_with_stars = pvalue_pivot.copy()\n",
    "    for col in pvalue_with_stars.columns:\n",
    "        for idx in pvalue_with_stars.index:\n",
    "            if pd.notna(pvalue_with_stars.loc[idx, col]):\n",
    "                p_val = pvalue_with_stars.loc[idx, col]\n",
    "                stars = significance_pivot.loc[idx, col]\n",
    "                pvalue_with_stars.loc[idx, col] = f\"{p_val:.4f} {stars}\"\n",
    "    \n",
    "    print(pvalue_with_stars.to_string())\n",
    "    \n",
    "    # Create an ultra-compact summary table\n",
    "    print(\"\\n4. COMPACT SUMMARY (Moran's I [Z-score] Significance):\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    compact_table = pd.DataFrame(index=morans_pivot.index, columns=morans_pivot.columns)\n",
    "    \n",
    "    for state_info in morans_pivot.index:\n",
    "        for ai_var in morans_pivot.columns:\n",
    "            if pd.notna(morans_pivot.loc[state_info, ai_var]):\n",
    "                morans_val = morans_pivot.loc[state_info, ai_var]\n",
    "                z_val = zscore_pivot.loc[state_info, ai_var]\n",
    "                sig = significance_pivot.loc[state_info, ai_var]\n",
    "                # Using 2 decimal places for z-score as requested\n",
    "                compact_table.loc[state_info, ai_var] = f\"{morans_val:.3f} [{z_val:.2f}] {sig}\"\n",
    "            else:\n",
    "                compact_table.loc[state_info, ai_var] = \"No data\"\n",
    "    \n",
    "    print(compact_table.to_string())\n",
    "    \n",
    "    # Legend for significance stars\n",
    "    print(f\"\\nSignificance Legend:\")\n",
    "    print(f\"***** p < 0.0001 (highly significant)\")\n",
    "    print(f\"****  p < 0.001  (very significant)\")\n",
    "    print(f\"***   p < 0.01   (significant)\")\n",
    "    print(f\"**    p < 0.05   (significant)\")\n",
    "    print(f\"*     p < 0.1    (marginally significant)\")\n",
    "    print(f\"ns    p ≥ 0.1    (not significant)\")\n",
    "    \n",
    "    print(f\"\\nTable Format: Moran's I [Z-score] Significance\")\n",
    "    print(f\"Higher Moran's I values indicate stronger spatial clustering\")\n",
    "    \n",
    "    # Overall patterns\n",
    "    total_clustering = len(comprehensive_state_df[comprehensive_state_df['Pattern'] == 'Significant clustering'])\n",
    "    total_dispersion = len(comprehensive_state_df[comprehensive_state_df['Pattern'] == 'Significant dispersion'])\n",
    "    total_random = len(comprehensive_state_df[comprehensive_state_df['Pattern'] == 'Random distribution'])\n",
    "    total_analyses = len(comprehensive_state_df)\n",
    "    \n",
    "    print(f\"\\nOverall Pattern Distribution:\")\n",
    "    print(f\"  Total state analyses: {total_analyses}\")\n",
    "    print(f\"  Significant clustering: {total_clustering} ({total_clustering/total_analyses*100:.1f}%)\")\n",
    "    print(f\"  Significant dispersion: {total_dispersion} ({total_dispersion/total_analyses*100:.1f}%)\")\n",
    "    print(f\"  Random distribution: {total_random} ({total_random/total_analyses*100:.1f}%)\")\n",
    "    \n",
    "    # Find which AI variable shows most clustering at state level\n",
    "    clustering_by_var = comprehensive_state_df[comprehensive_state_df['Pattern'] == 'Significant clustering'].groupby('AI_Variable').size()\n",
    "    if len(clustering_by_var) > 0:\n",
    "        most_clustered_var = clustering_by_var.idxmax()\n",
    "        print(f\"\\nAI variable with most spatial clustering at state level: {most_clustered_var} ({clustering_by_var[most_clustered_var]} states)\")\n",
    "    \n",
    "    # Find states that consistently show clustering\n",
    "    clustering_by_state = comprehensive_state_df[comprehensive_state_df['Pattern'] == 'Significant clustering'].groupby(['State_Code', 'State']).size()\n",
    "    if len(clustering_by_state) > 0:\n",
    "        print(f\"\\nStates showing clustering across multiple AI variables:\")\n",
    "        for (state_code, state_name), count in clustering_by_state.items():\n",
    "            print(f\"  {state_name} ({state_code}): {count} out of {len(ai_variables)} AI variables\")\n",
    "    \n",
    "print(f\"\\nAnalysis complete for {len(ai_variables)} AI variables across states with ≥20 hospitals each.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. DBSCAN - cluster identification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 DBSCAN function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_dbscan_clustering(hospitals_gdf, division, output_col, eps=0.25, min_samples=2, k=5):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering with adaptive feature weights based on local density.\n",
    "    \n",
    "    Parameters:\n",
    "    - hospitals_gdf: GeoDataFrame with hospital data\n",
    "    - division: Census division to analyze\n",
    "    - output_col: Column name for the ML score to use\n",
    "    - eps: DBSCAN epsilon parameter\n",
    "    - min_samples: DBSCAN min_samples parameter\n",
    "    - k: Number of neighbors to consider for density estimation\n",
    "    \"\"\"\n",
    "    # Filter data for this division\n",
    "    division_data = hospitals_gdf[hospitals_gdf['division'] == division].copy()\n",
    "    \n",
    "    # Check if we have the target column\n",
    "    if output_col not in division_data.columns:\n",
    "        print(f\"Error: Column '{output_col}' not found for division '{division}'\")\n",
    "        return pd.DataFrame(), []\n",
    "    \n",
    "    # Handle missing values - drop rows with NaN in the target column or coordinates\n",
    "    division_data = division_data.dropna(subset=[output_col, 'geometry'])\n",
    "    \n",
    "    # Check if we have enough data after dropping NaNs\n",
    "    if len(division_data) < min_samples:\n",
    "        print(f\"Skipping {division}: too few hospitals with valid data ({len(division_data)})\")\n",
    "        return pd.DataFrame(), []\n",
    "    \n",
    "    # Print diagnostic info\n",
    "    print(f\"  Processing {division} with {len(division_data)} hospitals (valid data)\")\n",
    "    \n",
    "    # Calculate local density using k-nearest neighbors\n",
    "    coords = np.column_stack([division_data.geometry.x, division_data.geometry.y])\n",
    "    \n",
    "    # Ensure coordinates don't have NaNs\n",
    "    if np.isnan(coords).any():\n",
    "        print(f\"  Warning: Some coordinates contain NaN values. Removing affected points.\")\n",
    "        valid_idx = ~np.isnan(coords).any(axis=1)\n",
    "        division_data = division_data.iloc[valid_idx].copy()\n",
    "        coords = coords[valid_idx]\n",
    "        \n",
    "        if len(division_data) < min_samples:\n",
    "            print(f\"  Skipping {division}: too few hospitals after removing NaN coordinates\")\n",
    "            return pd.DataFrame(), []\n",
    "    \n",
    "    # Ensure ML scores don't have NaNs\n",
    "    values = division_data[output_col].values\n",
    "    if np.isnan(values).any():\n",
    "        print(f\"  Warning: '{output_col}' contains NaN values. Removing affected points.\")\n",
    "        valid_idx = ~np.isnan(values)\n",
    "        division_data = division_data.iloc[valid_idx].copy()\n",
    "        coords = coords[valid_idx]\n",
    "        values = values[valid_idx]\n",
    "        \n",
    "        if len(division_data) < min_samples:\n",
    "            print(f\"  Skipping {division}: too few hospitals after removing NaN values\")\n",
    "            return pd.DataFrame(), []\n",
    "    \n",
    "    # Adjust k if necessary\n",
    "    actual_k = min(k, len(division_data)-1)\n",
    "    if actual_k < k:\n",
    "        print(f\"  Note: Adjusted k from {k} to {actual_k} due to small sample size\")\n",
    "    \n",
    "    # Fit nearest neighbors\n",
    "    nn = NearestNeighbors(n_neighbors=actual_k+1)  # +1 because point is its own neighbor\n",
    "    nn.fit(coords)\n",
    "    distances, _ = nn.kneighbors(coords)\n",
    "    \n",
    "    # Average distance to k nearest neighbors (excluding self)\n",
    "    avg_distances = distances[:, 1:].mean(axis=1)\n",
    "    \n",
    "    # Convert to density: smaller distances = higher density\n",
    "    density = 1 / (avg_distances + 1e-10)  # Add small constant to avoid division by zero\n",
    "    \n",
    "    # Higher density areas get HIGHER weights\n",
    "    min_density, max_density = np.percentile(density, [5, 95])  # Using percentiles to avoid outliers\n",
    "    normalized_density = (density - min_density) / (max_density - min_density + 1e-10)\n",
    "    normalized_density = np.clip(normalized_density, 0, 1)  # Clip to [0, 1] first\n",
    "    weights = 0.5 + 1.5 * normalized_density  # Scale to [0.5, 2.0] - higher density gets higher weights\n",
    "    \n",
    "    # Store the weights for later analysis\n",
    "    division_data['ml_score_weight'] = weights\n",
    "    \n",
    "    # Create weighted features: higher weight amplifies ML scores in dense areas\n",
    "    X = np.column_stack([\n",
    "        coords,  # x, y coordinates\n",
    "        division_data[output_col].values * weights  # Adaptive weighting - FIXED\n",
    "    ])\n",
    "    \n",
    "    # Final check for NaNs\n",
    "    if np.isnan(X).any():\n",
    "        print(f\"  Error: Feature matrix still contains NaN values after processing\")\n",
    "        print(f\"  NaN counts: {np.isnan(X).sum(axis=0)}\")\n",
    "        return pd.DataFrame(), []\n",
    "    \n",
    "    # Standardize the weighted features\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Run DBSCAN\n",
    "    try:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        cluster_labels = dbscan.fit_predict(X_scaled)\n",
    "        \n",
    "        # Store labels in the dataframe\n",
    "        division_data['cluster'] = cluster_labels\n",
    "        \n",
    "        # Create cluster summary\n",
    "        clusters_summary = []\n",
    "        if len(set(cluster_labels)) > 1:  # If we have actual clusters\n",
    "            for cluster_id in sorted(set(cluster_labels)):\n",
    "                if cluster_id == -1:\n",
    "                    continue  # Skip noise points\n",
    "                    \n",
    "                cluster_hospitals = division_data[division_data['cluster'] == cluster_id]\n",
    "                \n",
    "                # Calculate ML stats\n",
    "                ml_mean = cluster_hospitals[output_col].mean()\n",
    "                ml_std = cluster_hospitals[output_col].std() if len(cluster_hospitals) > 1 else 0\n",
    "                \n",
    "                clusters_summary.append({\n",
    "                    'division': division,\n",
    "                    'cluster_id': f\"{division}-{cluster_id}\",\n",
    "                    'n_hospitals': len(cluster_hospitals),\n",
    "                    'pct_of_division': len(cluster_hospitals) / len(division_data) * 100,\n",
    "                    'ml_mean': ml_mean,\n",
    "                    'ml_std': ml_std,\n",
    "                    'ml_min': cluster_hospitals[output_col].min(),\n",
    "                    'ml_max': cluster_hospitals[output_col].max(),\n",
    "                    'avg_density_weight': cluster_hospitals['ml_score_weight'].mean(),  # Renamed for clarity\n",
    "                    'density_weight_std': cluster_hospitals['ml_score_weight'].std() if len(cluster_hospitals) > 1 else 0\n",
    "                })\n",
    "        \n",
    "        # Count results\n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "        n_noise = list(cluster_labels).count(-1)\n",
    "        \n",
    "        print(f\"  Results: {n_clusters} clusters, {n_noise} noise points ({n_noise/len(cluster_labels)*100:.1f}%)\")\n",
    "        print(f\"  ML score weights - min: {weights.min():.2f}, max: {weights.max():.2f}, avg: {weights.mean():.2f}\")\n",
    "        print(f\"  Weight interpretation: Higher values = denser areas get amplified ML scores\")\n",
    "        \n",
    "        return division_data, clusters_summary\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error during DBSCAN: {str(e)}\")\n",
    "        return pd.DataFrame(), []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.column_stack([hospitals_gdf_projected.geometry.x, hospitals_gdf_projected.geometry.y])\n",
    "ml_scores = hospitals_gdf_projected['ai_base_score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional DBSCAN Clustering Analysis\n",
    "# Applies spatial clustering to hospital data by census division\n",
    "\n",
    "# Initialize storage for results\n",
    "all_results = []\n",
    "all_clusters_summary = []\n",
    "modified_data = pd.DataFrame()\n",
    "\n",
    "# Define analysis regions\n",
    "regions = [\n",
    "    'New England', 'Mid Atlantic', 'South Atlantic', \n",
    "    'East North Central', 'East South Central', 'West North Central',\n",
    "    'West South Central', 'Mountain', 'Pacific'\n",
    "]\n",
    "\n",
    "target_metric = 'ai_base_score'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 DBSCAN ai base score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Clustering Analysis by Census Division\n",
    "# Applies adaptive clustering to each geographic region\n",
    "\n",
    "# Initialize storage\n",
    "all_clusters_summary = []\n",
    "modified_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "target_column = 'ai_base_score'\n",
    "\n",
    "# Apply clustering to each division\n",
    "for division in divisions:\n",
    "    if division not in hospitals_gdf_projected['division'].unique():\n",
    "        continue\n",
    "    \n",
    "    # Run adaptive DBSCAN\n",
    "    division_result, clusters_summary = adaptive_dbscan_clustering(\n",
    "        hospitals_gdf_projected, \n",
    "        division, \n",
    "        output_col=target_column,\n",
    "        eps=0.25, \n",
    "        min_samples=2,\n",
    "        k=5\n",
    "    )\n",
    "    \n",
    "    if not division_result.empty:\n",
    "        modified_data = pd.concat([modified_data, division_result])\n",
    "        all_clusters_summary.extend(clusters_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 DBSCAN secondary model implmentation measures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collection variables\n",
    "all_results = []\n",
    "all_clusters_summary = []\n",
    "modified_hospital_data = pd.DataFrame()  # Empty DataFrame to collect results\n",
    "\n",
    "# Define census divisions\n",
    "divisions = [\n",
    "    'New England', 'Mid Atlantic', 'South Atlantic', \n",
    "    'East North Central', 'East South Central', 'West North Central',\n",
    "    'West South Central', 'Mountain', 'Pacific'\n",
    "]\n",
    "\n",
    "# Define the target column\n",
    "target_column = 'ai_base_score'\n",
    "\n",
    "print(f\"DBSCAN Clustering Analysis for {target_column} by Census Division\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Apply clustering to each division\n",
    "for division in divisions:\n",
    "    print(f\"\\nAnalyzing {division} division...\")\n",
    "    \n",
    "    # Check if this division exists in the data\n",
    "    if division not in hospitals_gdf_projected['division'].unique():\n",
    "        print(f\"  Warning: No hospitals found in '{division}' division\")\n",
    "        continue\n",
    "    \n",
    "    # Apply clustering\n",
    "    division_result, clusters_summary = adaptive_dbscan_clustering(\n",
    "        hospitals_gdf_projected, \n",
    "        division, \n",
    "        output_col=target_column,\n",
    "        eps=0.25, \n",
    "        min_samples=2,\n",
    "        k=5\n",
    "    )\n",
    "    \n",
    "    # Only append if we got results\n",
    "    if not division_result.empty:\n",
    "        modified_hospital_data = pd.concat([modified_hospital_data, division_result])\n",
    "        all_clusters_summary.extend(clusters_summary)\n",
    "\n",
    "# Convert cluster summaries to DataFrame for analysis\n",
    "if all_clusters_summary:\n",
    "    # Create DataFrame from list\n",
    "    clusters_df = pd.DataFrame(all_clusters_summary)\n",
    "    \n",
    "    # Summary statistics by division\n",
    "    division_summary = clusters_df.groupby('division').agg(\n",
    "        Number_of_Clusters=('cluster_id', 'count'),\n",
    "        Total_Hospitals=('n_hospitals', 'sum'),\n",
    "        Avg_Cluster_Size=('n_hospitals', 'mean'),\n",
    "        Avg_Score=('ml_mean', 'mean'),\n",
    "        Min_Score=('ml_min', 'min'),\n",
    "        Max_Score=('ml_max', 'max')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Format the numeric columns\n",
    "    division_summary['Avg_Cluster_Size'] = division_summary['Avg_Cluster_Size'].round(1)\n",
    "    division_summary['Avg_Score'] = division_summary['Avg_Score'].round(2)\n",
    "    division_summary['Min_Score'] = division_summary['Min_Score'].round(2)\n",
    "    division_summary['Max_Score'] = division_summary['Max_Score'].round(2)\n",
    "    \n",
    "    # Overall summary as a single row DataFrame\n",
    "    overall_summary = pd.DataFrame([{\n",
    "        'Division': 'TOTAL',\n",
    "        'Number_of_Clusters': len(clusters_df),\n",
    "        'Total_Hospitals': clusters_df['n_hospitals'].sum(),\n",
    "        'Avg_Cluster_Size': round(clusters_df['n_hospitals'].mean(), 1),\n",
    "        'Avg_Score': round(clusters_df['ml_mean'].mean(), 2),\n",
    "        'Min_Score': round(clusters_df['ml_min'].min(), 2),\n",
    "        'Max_Score': round(clusters_df['ml_max'].max(), 2)\n",
    "    }])\n",
    "    \n",
    "    # Display summary tables\n",
    "    print(\"\\nDivision Summary:\")\n",
    "    display(division_summary)\n",
    "    \n",
    "    print(\"\\nOverall Summary:\")\n",
    "    display(overall_summary)\n",
    "else:\n",
    "    print(\"No clusters were identified across any division\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
