{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2. Spatial Cluster Analysis\n",
    "\n",
    "**Description**  \n",
    "This section conducts comprehensive spatial analysis to identify geographic patterns and clustering in hospital-level AI adoption across the United States. The analysis employs multiple complementary approaches to characterize spatial autocorrelation, identify statistically significant clusters, and examine regional disparities in AI implementation.\n",
    "\n",
    "**Purpose**  \n",
    "To explore patterns in hospital-level AI adoption across geographic regions (state or census division), aiding interpretation of implementation disparities.\n",
    "\n",
    "**Disclaimer**  \n",
    "- This codebase was partially cleaned and annotated using OpenAIâ€™s ChatGPT-4o. Please review and validate before using for critical purposes.  \n",
    "- AHA data is subscription-based and not publicly shareable. All reported results are aggregated at the state or census division level.\n",
    "- All publicly available data should also be independently downlowded from the source \n",
    "\n",
    "**Notebook Workflow**  \n",
    "\n",
    "0. Load necessary libraries, functions, and pre-processed data \n",
    "1. Prepare the data to conduct spatial analysis \n",
    "2. Analaysis \n",
    "3. Neighbor Analysis - Hospital clustering \n",
    "4. Moran's I score autocorrelation \n",
    "5. DBSAN cluster analysis \n",
    "6. Moran's I score on census division and states "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2_0 load necessary libraries, functions and preprocessed data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C2_0_1 load libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import contextily as ctx\n",
    "import scipy.spatial as spatial\n",
    "from scipy.stats import zscore\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C2_0_2 load custom functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom functions to calculate AI scores \n",
    "# 1. Primay variable of interest: ai_base_score \n",
    "# 2. Seconoday variables of interest: ai_base_breadth_score, ai_base_dev_score, ai_base_eval_score \n",
    "# 2.1 ai_base_breadth_score : This score reflects the breadth of the use_cases \n",
    "# 2.2 ai_base_dev_score : This score reflects the degree of model development \n",
    "# 2.3 ai_base_eval_score : This score reflects the degree of model evaluation in bias and accuracy \n",
    "\n",
    "def calculate_base_ai_implementation_row(row):\n",
    "    \"\"\"\n",
    "    Calculate base AI implementation score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: Base AI implementation score\n",
    "    \"\"\"\n",
    "    # Base AI implementation score (continuous)\n",
    "    # Return None if the input value is null\n",
    "    if pd.isna(row['aipred_it']):\n",
    "        return None\n",
    "    elif row['aipred_it'] == 1:  # Machine Learning\n",
    "        return 2\n",
    "    elif row['aipred_it'] == 2:  # Other Non-Machine Learning Predictive Models\n",
    "        return 1\n",
    "    else:  # Neither (3) or Do not know (4)\n",
    "        return 0\n",
    "\n",
    "def calculate_ai_implementation_breadth_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI implementation breadth score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI implementation breadth score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        breadth_score = base_score\n",
    "        # Implementation Breadth Score - count use cases\n",
    "        use_case_cols = ['aitraj_it', 'airfol_it', 'aimhea_it', 'airect_it', \n",
    "                     'aibill_it', 'aische_it', 'aipoth_it', 'aicloth_it']\n",
    "        for col in use_case_cols:\n",
    "            if row[col] is None:\n",
    "                breadth_score += 0\n",
    "            else:\n",
    "                breadth_score += row[col] * 0.25  # 0.25 points per use case\n",
    "        return breadth_score\n",
    "\n",
    "def calculate_ai_development_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI development score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI development score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0 \n",
    "    else:\n",
    "        dev_score = base_score\n",
    "        if 'mlsed_it' in row and pd.notna(row['mlsed_it']):\n",
    "            dev_score += row['mlsed_it'] * 2  # Self-developed\n",
    "        if 'mldev_it' in row and pd.notna(row['mldev_it']):\n",
    "            dev_score += row['mldev_it']  # EHR developer\n",
    "        if 'mlthd_it' in row and pd.notna(row['mlthd_it']):\n",
    "            dev_score += row['mlthd_it']  # Third-party\n",
    "        if 'mlpubd_it' in row and pd.notna(row['mlpubd_it']):\n",
    "            dev_score += row['mlpubd_it'] * 0.5  # Public domain\n",
    "        return dev_score\n",
    "\n",
    "def calculate_ai_evaluation_row(row):\n",
    "    \"\"\"\n",
    "    Calculate AI evaluation score for a single row (hospital).\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        float: AI evaluation score\n",
    "    \"\"\"\n",
    "    # Start with base score\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    if base_score is None:\n",
    "        return None\n",
    "    elif base_score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        eval_score = base_score\n",
    "        # For model accuracy (MLACCU)\n",
    "        if row['mlaccu_it'] is None:\n",
    "            eval_score += 0\n",
    "        elif row['mlaccu_it'] == 1:  # All models\n",
    "            eval_score += 1\n",
    "        elif row['mlaccu_it'] == 2:  # Most models\n",
    "            eval_score += 0.75\n",
    "        elif row['mlaccu_it'] == 3:  # Some models\n",
    "            eval_score += 0.5\n",
    "        elif row['mlaccu_it'] == 4:  # Few models\n",
    "            eval_score += 0.25\n",
    "        # For None (5) or Do not know (6), no points added\n",
    "    \n",
    "    # For model bias (MLBIAS)\n",
    "        if row['mlbias_it'] is None:\n",
    "            eval_score += 0\n",
    "        elif row['mlbias_it'] == 1:  # All models\n",
    "            eval_score += 1\n",
    "        elif row['mlbias_it'] == 2:  # Most models\n",
    "            eval_score += 0.75\n",
    "        elif row['mlbias_it'] == 3:  # Some models\n",
    "            eval_score += 0.5\n",
    "        elif row['mlbias_it'] == 4:  # Few models\n",
    "            eval_score += 0.25\n",
    "        # For None (5) or Do not know (6), no points added\n",
    "    \n",
    "        return eval_score\n",
    "\n",
    "def calculate_all_ai_scores_row(row):\n",
    "    \"\"\"\n",
    "    Calculate all AI/ML implementation scores as continuous measures for a single row.\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series representing a single hospital row\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with all calculated scores\n",
    "    \"\"\"\n",
    "    # Calculate all scores\n",
    "    base_score = calculate_base_ai_implementation_row(row)\n",
    "    breadth_score = calculate_ai_implementation_breadth_row(row)\n",
    "    dev_score = calculate_ai_development_row(row)\n",
    "    eval_score = calculate_ai_evaluation_row(row)\n",
    "    \n",
    "    return {\n",
    "        'ai_base_score': base_score,\n",
    "        'ai_base_breadth_score': breadth_score,\n",
    "        'ai_base_dev_score': dev_score,\n",
    "        'ai_base_eval_score': eval_score\n",
    "    }\n",
    "\n",
    "def apply_ai_scores_to_dataframe(df):\n",
    "    \"\"\"\n",
    "    Apply all AI score calculations row by row to a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: A pandas DataFrame with hospital data\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with added AI score columns\n",
    "    \"\"\"\n",
    "    # Initialize empty columns for scores\n",
    "    df['ai_base_score'] = float('nan')\n",
    "    df['ai_base_breadth_score'] = float('nan')\n",
    "    df['ai_base_dev_score'] = float('nan')\n",
    "    df['ai_base_eval_score'] = float('nan')\n",
    "    \n",
    "    # Apply row by row calculations\n",
    "    for index, row in df.iterrows():\n",
    "        scores = calculate_all_ai_scores_row(row)\n",
    "        for score_name, score_value in scores.items():\n",
    "            df.at[index, score_name] = score_value\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C2_0_3 load custom dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed AHA dataframe from A2 notebook \n",
    "AHA_master = pd.read_csv('./data/AHA_master_external_data.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop hospitals that did not respond to IT supplement \n",
    "AHA_IT = AHA_master[~AHA_master.id_it.isnull()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2_1 Prepare the data to conduct analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AHA_IT2 = apply_ai_scores_to_dataframe(AHA_IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load STATE shape file \n",
    "os.environ['SHAPE_RESTORE_SHX'] = 'YES'\n",
    "states = gpd.read_file('../temp_shp/cb_2018_us_state_500k.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C2_1_2 Clean dataframe \n",
    "1. drop missing or invalid coordinate rows \n",
    "2. create geodataframe \n",
    "3. prepare the dataframe for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing or invalid coordinates\n",
    "AHA_IT = AHA_IT.dropna(subset=['latitude_address', 'longitude_address'])\n",
    "\n",
    "# Filter out invalid coordinates\n",
    "valid_coords = (\n",
    "    (AHA_IT['latitude_address'] != 0) & \n",
    "    (AHA_IT['longitude_address'] != 0) &\n",
    "    (AHA_IT['latitude_address'] >= -90) & \n",
    "    (AHA_IT['latitude_address'] <= 90) &\n",
    "    (AHA_IT['longitude_address'] >= -180) & \n",
    "    (AHA_IT['longitude_address'] <= 180)\n",
    ")\n",
    "AHA_IT = AHA_IT[valid_coords]\n",
    "\n",
    "# Create GeoDataFrame\n",
    "hospitals = gpd.GeoDataFrame(\n",
    "    AHA_IT, \n",
    "    geometry=gpd.points_from_xy(AHA_IT.longitude_address, AHA_IT.latitude_address),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare the data for  analysis\n",
    "# Filter hospitals with valid coordinates and implementation scores\n",
    "valid_hospitals = hospitals.dropna(subset=['longitude_address', 'latitude_address', 'aipred_it'])\n",
    "valid_geo_hospitals = hospitals.dropna(subset=['longitude_address', 'latitude_address'])\n",
    "# Create a GeoDataFrame\n",
    "hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_hospitals.longitude_address, valid_hospitals.latitude_address),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "geo_hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_geo_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_geo_hospitals.longitude_address, valid_geo_hospitals.latitude_address),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a projected CRS for accurate distance calculations\n",
    "hospitals_gdf_projected = hospitals_gdf.to_crs(epsg=3857) # projected coordinate system using flat, 2D plane to represent Earth's surface \n",
    "geo_hospitals_gdf_projected = geo_hospitals_gdf.to_crs(epsg=3857) # projected coordinate system using flat, 2D plane to represent Earth's surface \n",
    "\n",
    "\n",
    "geo_hospitals_gdf_projected['ML_implementation_score'] = geo_hospitals_gdf_projected['aipred_it'].map({\n",
    "    1: 3,  # ML gets highest score\n",
    "    2: 2,  # Non-ML gets middle score\n",
    "    3: 1,  # Neither gets lowest score\n",
    "    4: 1,   # Don't know gets lowest score\n",
    "    None: 0,\n",
    "    0: 0 \n",
    "})\n",
    "geo_hospitals_gdf_projected.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2_2 AI/ML barplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of states to regions\n",
    "# Create a mapping of states to census divisions based on provided image\n",
    "state_to_division = {\n",
    "    # Division 1: New England\n",
    "    'ME': 'New England', 'NH': 'New England', 'VT': 'New England', \n",
    "    'MA': 'New England', 'RI': 'New England', 'CT': 'New England',\n",
    "    \n",
    "    # Division 2: Mid Atlantic\n",
    "    'NY': 'Mid Atlantic', 'NJ': 'Mid Atlantic', 'PA': 'Mid Atlantic',\n",
    "    \n",
    "    # Division 3: South Atlantic\n",
    "    'DE': 'South Atlantic', 'MD': 'South Atlantic', 'DC': 'South Atlantic',\n",
    "    'VA': 'South Atlantic', 'WV': 'South Atlantic', 'NC': 'South Atlantic',\n",
    "    'SC': 'South Atlantic', 'GA': 'South Atlantic', 'FL': 'South Atlantic',\n",
    "    \n",
    "    # Division 4: East North Central\n",
    "    'OH': 'East North Central', 'IN': 'East North Central', 'IL': 'East North Central',\n",
    "    'MI': 'East North Central', 'WI': 'East North Central',\n",
    "    \n",
    "    # Division 5: East South Central\n",
    "    'KY': 'East South Central', 'TN': 'East South Central', \n",
    "    'AL': 'East South Central', 'MS': 'East South Central',\n",
    "    \n",
    "    # Division 6: West North Central\n",
    "    'MN': 'West North Central', 'IA': 'West North Central', 'MO': 'West North Central',\n",
    "    'ND': 'West North Central', 'SD': 'West North Central', 'NE': 'West North Central',\n",
    "    'KS': 'West North Central',\n",
    "    \n",
    "    # Division 7: West South Central\n",
    "    'AR': 'West South Central', 'LA': 'West South Central', \n",
    "    'OK': 'West South Central', 'TX': 'West South Central',\n",
    "    \n",
    "    # Division 8: Mountain\n",
    "    'MT': 'Mountain', 'ID': 'Mountain', 'WY': 'Mountain', 'CO': 'Mountain',\n",
    "    'NM': 'Mountain', 'AZ': 'Mountain', 'UT': 'Mountain', 'NV': 'Mountain',\n",
    "    \n",
    "    # Division 9: Pacific\n",
    "    'WA': 'Pacific', 'OR': 'Pacific', 'CA': 'Pacific', \n",
    "    'AK': 'Pacific', 'HI': 'Pacific',\n",
    "    \n",
    "    # Territories\n",
    "    'PR': 'Territories', 'GU': 'Territories', 'VI': 'Territories', \n",
    "    'AS': 'Territories', 'MP': 'Territories'\n",
    "}\n",
    "\n",
    "# Add census division column to the dataframe\n",
    "hospitals_gdf_projected['division'] = hospitals_gdf_projected['mstate_it'].map(state_to_division)\n",
    "geo_hospitals_gdf_projected['division'] = geo_hospitals_gdf_projected['mstate_it'].map(state_to_division)\n",
    "\n",
    "# Loop through each census division \n",
    "divisions = [\n",
    "    'New England', 'Mid Atlantic', 'South Atlantic', \n",
    "    'East North Central', 'East South Central', 'West North Central',\n",
    "    'West South Central', 'Mountain', 'Pacific'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your existing ai_types\n",
    "ai_types = [3, 2, 1]  # These are your actual AI implementation categories\n",
    "\n",
    "# Map the numeric AI types to labels for better readability\n",
    "ai_type_labels = {\n",
    "    3: 'ML',\n",
    "    2: 'Non-ML Predictive Model',\n",
    "    1: 'Do not know/Neither',\n",
    "    0: 'No Response'  # Assuming 0 or NaN is used for no response\n",
    "}\n",
    "\n",
    "# Create a DataFrame to hold the distribution of AI types by division\n",
    "division_ai_counts = pd.DataFrame(index=divisions, columns=ai_types + [0])  # Include 0 for no response\n",
    "# Fill the DataFrame with counts\n",
    "for division in divisions:\n",
    "    geo_division_data = geo_hospitals_gdf_projected[geo_hospitals_gdf_projected['division'] == division]\n",
    "    \n",
    "    # Count no responses (either 0 or NaN)\n",
    "    no_response_count = len(geo_division_data[geo_division_data['ML_implementation_score'].isna() | \n",
    "                                         (geo_division_data['ML_implementation_score'] == 0)])\n",
    "    division_ai_counts.loc[division, 0] = no_response_count\n",
    "    \n",
    "    # Count other AI types\n",
    "    for ai_type in ai_types:\n",
    "        division_ai_counts.loc[division, ai_type] = len(geo_division_data[geo_division_data['ML_implementation_score'] == ai_type])\n",
    "division_ai_counts\n",
    "\n",
    "# Calculate total hospitals per division\n",
    "division_totals = division_ai_counts.sum(axis=1)\n",
    "\n",
    "# Calculate percentages\n",
    "division_ai_dist = division_ai_counts.div(division_ai_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Calculate grid dimensions\n",
    "n_divisions = len(divisions)\n",
    "n_cols = 3  # Number of columns in the grid\n",
    "n_rows = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Map the numeric AI types to labels for better readability\n",
    "ai_type_labels = {\n",
    "    3: 'AI/ML',\n",
    "    2: 'Non-AI/ML Predictive Model',\n",
    "    1: 'Do not know/Neither',\n",
    "    0: 'No Response'\n",
    "}\n",
    "\n",
    "# Rename columns for better readability\n",
    "division_ai_counts = division_ai_counts.rename(columns=ai_type_labels)\n",
    "\n",
    "# Calculate percentages\n",
    "percentages = division_ai_counts.div(division_ai_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Sort divisions by total\n",
    "total_hospitals = division_ai_counts.sum(axis=1)\n",
    "sorted_divisions = division_ai_counts.loc[total_hospitals.sort_values(ascending=False).index]\n",
    "sorted_percentages = percentages.loc[total_hospitals.sort_values(ascending=False).index]\n",
    "\n",
    "# Define publication-quality colors\n",
    "colors = ['#3366CC', '#66CCEE', '#EEEEFF', '#CCCCCC']\n",
    "\n",
    "# Create figure with a higher DPI for better quality\n",
    "fig, ax = plt.subplots(figsize=(10, 7), dpi=300)\n",
    "\n",
    "# Create stacked bar chart\n",
    "bars = sorted_divisions.plot(kind='bar', stacked=True, color=colors, ax=ax, width=0.7, edgecolor='white', linewidth=0.5)\n",
    "\n",
    "# Add percentage labels on top of each segment\n",
    "for i, division in enumerate(sorted_divisions.index):\n",
    "    cumulative_height = 0\n",
    "    for j, col in enumerate(sorted_divisions.columns):\n",
    "        height = sorted_divisions.loc[division, col]\n",
    "        pct = sorted_percentages.loc[division, col]\n",
    "        \n",
    "        if height > 0:  # Only label non-zero values\n",
    "            # Position at the top of each segment\n",
    "            y_pos = cumulative_height + height\n",
    "            \n",
    "            # For small percentages, only show if they're at least 1%\n",
    "            if pct >= 1:\n",
    "                # Position text at the top of each segment\n",
    "                ax.text(i, cumulative_height + (height * 0.5), f'{pct:.1f}%', \n",
    "                        ha='center', va='center', fontsize=9,\n",
    "                        color='black', fontweight='bold')\n",
    "            \n",
    "        cumulative_height += height\n",
    "\n",
    "ax.set_xlabel('Census Division', fontsize=14, labelpad=10, color='black')\n",
    "ax.set_ylabel('Number of Hospitals', fontsize=14, labelpad=10, color='black')\n",
    "ax.tick_params(axis='both', which='major', labelsize=12, colors='black')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right', color='black')\n",
    "plt.yticks(color='black')\n",
    "\n",
    "# Calculate combined percentage of ML and Non-ML Predictive Model\n",
    "combined_percentage = sorted_percentages['AI/ML'] + sorted_percentages['Non-AI/ML Predictive Model']\n",
    "\n",
    "# Add combined percentage labels above the total count\n",
    "for i, division in enumerate(sorted_divisions.index):\n",
    "    total = total_hospitals[division]\n",
    "    combined_pct = combined_percentage[division]\n",
    "    # Position the combined percentage above the total count\n",
    "    plt.text(i, total + 20, f'Model: {combined_pct:.1f}%', \n",
    "             ha='center', fontsize=8,  # Smaller font size\n",
    "             fontweight='normal',      # Normal weight instead of bold\n",
    "             color='#666666')          # Gray color\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2_3 Hospital nearest neighbor analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Custom nearest neighbor analysis (without relying on pointpats)\n",
    "print(\"Performing Custom Nearest Neighbor Analysis...\")\n",
    "\n",
    "# Extract coordinates\n",
    "coords = np.vstack((geo_hospitals_gdf_projected.geometry.x, geo_hospitals_gdf_projected.geometry.y)).T\n",
    "\n",
    "# Calculate distances between all pairs of points\n",
    "kdtree = spatial.KDTree(coords)\n",
    "distances, indices = kdtree.query(coords, k=2)  # k=2 to get the nearest neighbor (first one is self)\n",
    "mean_observed_nn_distance = np.mean(distances[:, 1])\n",
    "\n",
    "# Calculate the area - use the bounding box as an approximation\n",
    "x_min, y_min, x_max, y_max = geo_hospitals_gdf_projected.total_bounds\n",
    "area = (x_max - x_min) * (y_max - y_min)\n",
    "\n",
    "# Calculate point density\n",
    "n = len(coords)\n",
    "density = n / area\n",
    "\n",
    "# Expected mean distance for random distribution\n",
    "mean_expected_nn_distance = 0.5 / np.sqrt(density)\n",
    "\n",
    "# Calculate nearest neighbor ratio\n",
    "nn_ratio = mean_observed_nn_distance / mean_expected_nn_distance\n",
    "\n",
    "# Standard error\n",
    "se = 0.26136 / np.sqrt(n * density)\n",
    "\n",
    "# Z-score\n",
    "z_score = (mean_observed_nn_distance - mean_expected_nn_distance) / se\n",
    "p_value = 2 * (1 - norm.cdf(abs(z_score)))  # two-tailed test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C2_4 AI/ML Implementation spatial autocorrelation using Moran's I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_morans_i(values, coords, k=5):\n",
    "    \"\"\"Calculate Moran's I spatial autocorrelation using PySAL\"\"\"\n",
    "    # Convert inputs to numpy arrays\n",
    "    values = np.asarray(values).flatten()\n",
    "    coords = np.asarray(coords)\n",
    "    \n",
    "    # Handle missing values\n",
    "    valid_idx = ~np.isnan(values)\n",
    "    if not np.all(valid_idx):\n",
    "        values = values[valid_idx]\n",
    "        coords = coords[valid_idx]\n",
    "    \n",
    "    n = len(values)\n",
    "    if n <= 1 or np.var(values) == 0:\n",
    "        return {'moran_i': np.nan, 'expected_i': np.nan, 'z_score': np.nan, 'p_value': np.nan, 'n': n}\n",
    "    \n",
    "    try:\n",
    "        # Create k-nearest neighbor weights using PySAL\n",
    "        w = KNN.from_array(coords, k=k)\n",
    "        \n",
    "        # Calculate Moran's I using PySAL\n",
    "        moran = Moran(values, w)\n",
    "        \n",
    "        return {\n",
    "            'moran_i': moran.I,\n",
    "            'expected_i': moran.EI,\n",
    "            'z_score': moran.z_norm,\n",
    "            'p_value': moran.p_norm,\n",
    "            'n': n\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {'moran_i': np.nan, 'expected_i': np.nan, 'z_score': np.nan, 'p_value': np.nan, 'n': n}\n",
    "\n",
    "# Extract coordinates (keep this the same)\n",
    "coords = np.vstack((geo_hospitals_gdf_projected.geometry.x, geo_hospitals_gdf_projected.geometry.y)).T\n",
    "\n",
    "# Define the variables to analyze (keep this the same)\n",
    "variables = [\n",
    "    'ai_base_score', \n",
    "    'ai_base_breadth_score', \n",
    "    'ai_base_dev_score', \n",
    "    'ai_base_eval_score'\n",
    "]\n",
    "\n",
    "# Create a list to store results (keep this the same)\n",
    "results_list = []\n",
    "\n",
    "# Calculate Moran's I for each variable (keep this the same)\n",
    "for var in variables:\n",
    "    try:\n",
    "        # Get values for this variable\n",
    "        values = geo_hospitals_gdf_projected[var].values\n",
    "        \n",
    "        # Calculate Moran's I (this function is now using PySAL)\n",
    "        result = calculate_morans_i(values, coords, k=5)\n",
    "        \n",
    "        # Determine pattern (keep this the same)\n",
    "        if np.isnan(result['p_value']):\n",
    "            pattern = \"Invalid data\"\n",
    "        elif result['p_value'] < 0.05:\n",
    "            if result['moran_i'] > result['expected_i']:\n",
    "                pattern = \"Significant clustering\"\n",
    "            else:\n",
    "                pattern = \"Significant dispersion\"\n",
    "        else:\n",
    "            pattern = \"Random distribution\"\n",
    "        \n",
    "        # Add results to list (keep this the same)\n",
    "        results_list.append({\n",
    "            'Variable': var,\n",
    "            'Moran I': result['moran_i'],\n",
    "            'Expected I': result['expected_i'],\n",
    "            'z-score': result['z_score'],\n",
    "            'p-value': result['p_value'],\n",
    "            'Pattern': pattern,\n",
    "            'n': result['n']\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        results_list.append({\n",
    "            'Variable': var,\n",
    "            'Moran I': np.nan,\n",
    "            'Expected I': np.nan,\n",
    "            'z-score': np.nan,\n",
    "            'p-value': np.nan,\n",
    "            'Pattern': f\"Error: {str(e)}\",\n",
    "            'n': np.nan\n",
    "        })\n",
    "\n",
    "# Create DataFrame (keep this the same)\n",
    "moran_results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Format the numeric columns (keep this the same)\n",
    "moran_results_df['Moran I'] = moran_results_df['Moran I'].round(4)\n",
    "moran_results_df['Expected I'] = moran_results_df['Expected I'].round(4)\n",
    "moran_results_df['z-score'] = moran_results_df['z-score'].round(4)\n",
    "moran_results_df['p-value'] = moran_results_df['p-value'].round(10)\n",
    "\n",
    "# Display the DataFrame (keep this the same)\n",
    "print(\"\\nMoran's I Analysis Results (k=5 neighbors):\")\n",
    "moran_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2_5 AI/ML Implementation adaptative clustering analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C2_5_0 DBSCAN parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.column_stack([hospitals_gdf_projected.geometry.x, hospitals_gdf_projected.geometry.y])\n",
    "ml_scores = hospitals_gdf_projected['ai_base_score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional DBSCAN Clustering Analysis\n",
    "# Applies spatial clustering to hospital data by census division\n",
    "\n",
    "# Initialize storage for results\n",
    "all_results = []\n",
    "all_clusters_summary = []\n",
    "modified_data = pd.DataFrame()\n",
    "\n",
    "# Define analysis regions\n",
    "regions = [\n",
    "    'New England', 'Mid Atlantic', 'South Atlantic', \n",
    "    'East North Central', 'East South Central', 'West North Central',\n",
    "    'West South Central', 'Mountain', 'Pacific'\n",
    "]\n",
    "\n",
    "target_metric = 'ai_base_score'\n",
    "\n",
    "# Apply clustering by region\n",
    "for region in regions:\n",
    "    print(f\"Analyzing {region}...\")\n",
    "    \n",
    "    # Skip if region has no data\n",
    "    if region not in hospitals_gdf_projected['division'].unique():\n",
    "        continue\n",
    "    \n",
    "    # Apply adaptive DBSCAN clustering\n",
    "    region_result, cluster_stats = adaptive_dbscan_clustering(\n",
    "        hospitals_gdf_projected, \n",
    "        region, \n",
    "        output_col=target_metric,\n",
    "        eps=0.25,  # Distance threshold\n",
    "        min_samples=2  # Minimum cluster size\n",
    "    )\n",
    "    \n",
    "    # Collect results\n",
    "    if not region_result.empty:\n",
    "        modified_data = pd.concat([modified_data, region_result])\n",
    "        all_clusters_summary.extend(cluster_stats)\n",
    "\n",
    "# Generate summary statistics\n",
    "if all_clusters_summary:\n",
    "    clusters_df = pd.DataFrame(all_clusters_summary)\n",
    "    \n",
    "    # Regional summary\n",
    "    regional_summary = clusters_df.groupby('division').agg(\n",
    "        Number_of_Clusters=('cluster_id', 'count'),\n",
    "        Total_Points=('n_hospitals', 'sum'),\n",
    "        Avg_Cluster_Size=('n_hospitals', 'mean'),\n",
    "        Mean_Score=('ml_mean', 'mean'),\n",
    "        Score_Range=('ml_max', lambda x: f\"{clusters_df.loc[x.index, 'ml_min'].min():.2f}-{x.max():.2f}\")\n",
    "    ).round(2)\n",
    "    \n",
    "    # Overall summary\n",
    "    overall_stats = {\n",
    "        'Total_Clusters': len(clusters_df),\n",
    "        'Total_Points': clusters_df['n_hospitals'].sum(),\n",
    "        'Overall_Mean': clusters_df['ml_mean'].mean()\n",
    "    }\n",
    "    \n",
    "    print(\"Regional Summary:\")\n",
    "    print(regional_summary)\n",
    "    print(f\"\\nOverall: {overall_stats}\")\n",
    "else:\n",
    "    print(\"No clusters identified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C2_5_1 DBSCAN cluster identification for ai_base_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Clustering Analysis by Census Division\n",
    "# Applies adaptive clustering to each geographic region\n",
    "\n",
    "# Initialize storage\n",
    "all_clusters_summary = []\n",
    "modified_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "target_column = 'ai_base_score'\n",
    "\n",
    "# Apply clustering to each division\n",
    "for division in divisions:\n",
    "    if division not in hospitals_gdf_projected['division'].unique():\n",
    "        continue\n",
    "    \n",
    "    # Run adaptive DBSCAN\n",
    "    division_result, clusters_summary = adaptive_dbscan_clustering(\n",
    "        hospitals_gdf_projected, \n",
    "        division, \n",
    "        output_col=target_column,\n",
    "        eps=0.25, \n",
    "        min_samples=2,\n",
    "        k=5\n",
    "    )\n",
    "    \n",
    "    if not division_result.empty:\n",
    "        modified_data = pd.concat([modified_data, division_result])\n",
    "        all_clusters_summary.extend(clusters_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C2_5_2 DBSCAN cluster identification for ai_base_breadth_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collection variables\n",
    "all_results = []\n",
    "all_clusters_summary = []\n",
    "modified_hospital_data = pd.DataFrame()  # Empty DataFrame to collect results\n",
    "\n",
    "\n",
    "# Define the target column\n",
    "target_column = 'ai_base_breadth_score'\n",
    "\n",
    "# Apply clustering to each division\n",
    "for division in divisions:\n",
    "    if division not in data['division'].unique():\n",
    "        continue\n",
    "    \n",
    "    # Run adaptive DBSCAN\n",
    "    division_result, clusters_summary = adaptive_dbscan_clustering(\n",
    "        data, \n",
    "        division, \n",
    "        output_col=target_column,\n",
    "        eps=0.25, \n",
    "        min_samples=2,\n",
    "        k=5\n",
    "    )\n",
    "    \n",
    "    if not division_result.empty:\n",
    "        modified_data = pd.concat([modified_data, division_result])\n",
    "        all_clusters_summary.extend(clusters_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B2_5_3 DBSCAN cluster identification for ai_base_development_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collection variables\n",
    "all_results = []\n",
    "all_clusters_summary = []\n",
    "modified_hospital_data = pd.DataFrame()  # Empty DataFrame to collect results\n",
    "\n",
    "\n",
    "# Define the target column\n",
    "target_column = 'ai_base_dev_score'\n",
    "\n",
    "# Apply clustering to each division\n",
    "for division in divisions:\n",
    "    if division not in hospitals_gdf_projected['division'].unique():\n",
    "        continue\n",
    "    \n",
    "    # Run adaptive DBSCAN\n",
    "    division_result, clusters_summary = adaptive_dbscan_clustering(\n",
    "        hospitals_gdf_projected, \n",
    "        division, \n",
    "        output_col=target_column,\n",
    "        eps=0.25, \n",
    "        min_samples=2,\n",
    "        k=5\n",
    "    )\n",
    "    \n",
    "    if not division_result.empty:\n",
    "        modified_data = pd.concat([modified_data, division_result])\n",
    "        all_clusters_summary.extend(clusters_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B2_5_4 DBSCAN cluster identification for ai_base_evaluation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collection variables\n",
    "all_results = []\n",
    "all_clusters_summary = []\n",
    "modified_hospital_data = pd.DataFrame()  # Empty DataFrame to collect results\n",
    "\n",
    "\n",
    "# Define the target column\n",
    "target_column = 'ai_base_eval_score'\n",
    "\n",
    "# Apply clustering to each division\n",
    "for division in divisions:\n",
    "    if division not in hospitals_gdf_projected['division'].unique():\n",
    "        continue\n",
    "    \n",
    "    # Run adaptive DBSCAN\n",
    "    division_result, clusters_summary = adaptive_dbscan_clustering(\n",
    "        hospitals_gdf_projected, \n",
    "        division, \n",
    "        output_col=target_column,\n",
    "        eps=0.25, \n",
    "        min_samples=2,\n",
    "        k=5\n",
    "    )\n",
    "    \n",
    "    if not division_result.empty:\n",
    "        modified_data = pd.concat([modified_data, division_result])\n",
    "        all_clusters_summary.extend(clusters_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2_6 AI/ML Implementation Moran's I spatial autocorrelation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Moran's I Spatial Autocorrelation Analysis\n",
    "\n",
    "# Extract coordinates and target variable\n",
    "coords = np.vstack((hospitals_gdf_projected.geometry.x, hospitals_gdf_projected.geometry.y)).T\n",
    "values = hospitals_gdf_projected['ai_base_score'].values\n",
    "\n",
    "# Calculate Moran's I\n",
    "result = calculate_morans_i(values, coords, k=5)\n",
    "\n",
    "if not np.isnan(result['moran_i']):\n",
    "    I = result['moran_i']\n",
    "    EI = result['expected_i']\n",
    "    z_score = result['z_score']\n",
    "    p_value = result['p_value']\n",
    "    \n",
    "    # Create results summary\n",
    "    results = {\n",
    "        'Moran_I': round(I, 4),\n",
    "        'Expected_I': round(EI, 4),\n",
    "        'Z_Score': round(z_score, 2),\n",
    "        'P_Value': round(p_value, 6),\n",
    "        'Significant': p_value < 0.05,\n",
    "        'Pattern': 'Clustering' if (p_value < 0.05 and I > EI) else\n",
    "                  'Dispersion' if (p_value < 0.05 and I < EI) else\n",
    "                  'Random'\n",
    "    }\n",
    "    \n",
    "    print(f\"Moran's I: {results['Moran_I']}\")\n",
    "    print(f\"p-value: {results['P_Value']}\")\n",
    "    print(f\"Pattern: {results['Pattern']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare multiple variables\n",
    "ai_variables = ['ai_base_score', 'ai_base_breadth_score', 'ai_base_dev_score', 'ai_base_eval_score']\n",
    "comparison_results = []\n",
    "\n",
    "for var in ai_variables:\n",
    "    var_values = hospitals_gdf_projected[var].values\n",
    "    var_result = calculate_morans_i(var_values, coords, k=5)\n",
    "    \n",
    "    if not np.isnan(var_result['moran_i']):\n",
    "        comparison_results.append({\n",
    "            'Variable': var,\n",
    "            'Moran_I': round(var_result['moran_i'], 4),\n",
    "            'P_Value': round(var_result['p_value'], 6),\n",
    "            'Pattern': 'Clustering' if (var_result['p_value'] < 0.05 and \n",
    "                                     var_result['moran_i'] > var_result['expected_i']) else\n",
    "                      'Dispersion' if (var_result['p_value'] < 0.05 and \n",
    "                                     var_result['moran_i'] < var_result['expected_i']) else\n",
    "                      'Random'\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\nComparison across AI variables:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate Moran's I for each division and variable\n",
    "all_results = []\n",
    "\n",
    "for ai_var in ai_variables:\n",
    "    for division in mainland_divisions:\n",
    "        division_data = hospitals_gdf_projected[hospitals_gdf_projected['division'] == division]\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if len(division_data) < 10:\n",
    "            continue\n",
    "            \n",
    "        # Adjust k for smaller divisions\n",
    "        k = min(5, max(2, len(division_data) // 5))\n",
    "        \n",
    "        try:\n",
    "            coords = np.vstack((division_data.geometry.x, division_data.geometry.y)).T\n",
    "            values = division_data[ai_var].values\n",
    "            \n",
    "            result = calculate_morans_i(values, coords, k)\n",
    "            \n",
    "            if not np.isnan(result['moran_i']):\n",
    "                all_results.append({\n",
    "                    'AI_Variable': ai_var,\n",
    "                    'Division': division,\n",
    "                    'Morans_I': result['moran_i'],\n",
    "                    'p_value': result['p_value'],\n",
    "                    'z_score': result['z_score'],\n",
    "                    'expected_I': result['expected_i'],\n",
    "                    'n': result['n']\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "# Process results\n",
    "if all_results:\n",
    "    comprehensive_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Add pattern classification\n",
    "    comprehensive_df['Pattern'] = comprehensive_df.apply(lambda row: \n",
    "        'Clustering' if row['p_value'] < 0.05 and row['Morans_I'] > row['expected_I'] else\n",
    "        'Dispersion' if row['p_value'] < 0.05 and row['Morans_I'] < row['expected_I'] else\n",
    "        'Random', axis=1)\n",
    "    \n",
    "    # Create pivot table for Moran's I values\n",
    "    morans_pivot = comprehensive_df.pivot_table(\n",
    "        index='Division', \n",
    "        columns='AI_Variable', \n",
    "        values='Morans_I', \n",
    "        aggfunc='first'\n",
    "    ).round(4)\n",
    "    \n",
    "    # Create pivot table for significance\n",
    "    pvalue_pivot = comprehensive_df.pivot_table(\n",
    "        index='Division', \n",
    "        columns='AI_Variable', \n",
    "        values='p_value', \n",
    "        aggfunc='first'\n",
    "    ).round(4)\n",
    "    \n",
    "    print(\"Moran's I by Division and AI Variable:\")\n",
    "    print(morans_pivot)\n",
    "    \n",
    "    print(\"\\nP-values:\")\n",
    "    print(pvalue_pivot)\n",
    "    \n",
    "    # Summary statistics\n",
    "    for ai_var in ai_variables:\n",
    "        var_data = comprehensive_df[comprehensive_df['AI_Variable'] == ai_var]\n",
    "        clustering_count = len(var_data[var_data['Pattern'] == 'Clustering'])\n",
    "        total_divisions = len(var_data)\n",
    "        \n",
    "        print(f\"\\n{ai_var}: {clustering_count}/{total_divisions} divisions show significant clustering\")\n",
    "    \n",
    "    return comprehensive_df\n",
    "else:\n",
    "    print(\"No valid results calculated\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-Level Moran's I Analysis\n",
    "\n",
    "ai_variables = [\n",
    "    'ai_base_score', 'ai_base_breadth_score', \n",
    "    'ai_base_dev_score', 'ai_base_eval_score'\n",
    "]\n",
    "\n",
    "# Get states with sufficient data\n",
    "states = hospitals_gdf_projected['mstate_it'].dropna().unique()\n",
    "all_state_results = []\n",
    "\n",
    "for ai_var in ai_variables:\n",
    "    for state in states:\n",
    "        state_data = hospitals_gdf_projected[hospitals_gdf_projected['mstate_it'] == state]\n",
    "        \n",
    "        # Require minimum 20 hospitals for reliable analysis\n",
    "        if len(state_data) < 20:\n",
    "            continue\n",
    "            \n",
    "        # Adaptive k parameter\n",
    "        k = min(5, max(2, len(state_data) // 5))\n",
    "        \n",
    "        try:\n",
    "            coords = np.vstack((state_data.geometry.x, state_data.geometry.y)).T\n",
    "            values = state_data[ai_var].values\n",
    "            \n",
    "            result = calculate_morans_i(values, coords, k)\n",
    "            \n",
    "            if not np.isnan(result['moran_i']):\n",
    "                # Get state name if available\n",
    "                state_name = state\n",
    "                if 'mstate' in state_data.columns:\n",
    "                    state_names = state_data['mstate'].dropna().unique()\n",
    "                    if len(state_names) > 0:\n",
    "                        state_name = state_names[0]\n",
    "                \n",
    "                all_state_results.append({\n",
    "                    'AI_Variable': ai_var,\n",
    "                    'State_Code': state,\n",
    "                    'State': state_name,\n",
    "                    'Morans_I': result['moran_i'],\n",
    "                    'p_value': result['p_value'],\n",
    "                    'z_score': result['z_score'],\n",
    "                    'expected_I': result['expected_i'],\n",
    "                    'n': result['n']\n",
    "                })\n",
    "                \n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "# Process results\n",
    "if all_state_results:\n",
    "    comprehensive_state_df = pd.DataFrame(all_state_results)\n",
    "    \n",
    "    # Add pattern classification\n",
    "    comprehensive_state_df['Pattern'] = comprehensive_state_df.apply(lambda row: \n",
    "        'Clustering' if row['p_value'] < 0.05 and row['Morans_I'] > row['expected_I'] else\n",
    "        'Dispersion' if row['p_value'] < 0.05 and row['Morans_I'] < row['expected_I'] else\n",
    "        'Random', axis=1)\n",
    "    \n",
    "    # Create pivot table for Moran's I values\n",
    "    morans_pivot = comprehensive_state_df.pivot_table(\n",
    "        index=['State_Code', 'State'], \n",
    "        columns='AI_Variable', \n",
    "        values='Morans_I', \n",
    "        aggfunc='first'\n",
    "    ).round(4)\n",
    "    \n",
    "    \n",
    "    # Overall summary\n",
    "    total_clustering = len(comprehensive_state_df[comprehensive_state_df['Pattern'] == 'Clustering'])\n",
    "    total_analyses = len(comprehensive_state_df)\n",
    "    \n",
    "    return comprehensive_state_df\n",
    "else:\n",
    "    print(\"No valid results calculated\")\n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
