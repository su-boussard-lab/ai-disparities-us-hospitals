{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2. Spatial Cluster Analysis\n",
    "\n",
    "**Description**  \n",
    "This section conducts comprehensive spatial analysis to identify geographic patterns and clustering in hospital-level AI adoption across the United States. The analysis employs multiple complementary approaches to characterize spatial autocorrelation, identify statistically significant clusters, and examine regional disparities in AI implementation.\n",
    "\n",
    "**Purpose**  \n",
    "To explore patterns in hospital-level AI adoption across geographic regions (state or census division), aiding interpretation of implementation disparities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 load necessary libraries, functions and preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import contextily as ctx\n",
    "import scipy.spatial as spatial\n",
    "from scipy.stats import zscore\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from calculate_scores import create_union_aipred_row, apply_ai_scores_to_dataframe\n",
    "\n",
    "# Load data\n",
    "AHA_master = pd.read_csv('../../data/AHA_master_external_data.csv', low_memory=False)\n",
    "\n",
    "# Create aipred_it_union separately (your choice, works perfectly)\n",
    "AHA_master['aipred_it_union'] = AHA_master.apply(create_union_aipred_row, axis=1)\n",
    "\n",
    "# Use the apply function for all other scores\n",
    "AHA_master2 = apply_ai_scores_to_dataframe(AHA_master)\n",
    "AHA_IT = AHA_master2[AHA_master2['id_it'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load STATE shape file \n",
    "os.environ['SHAPE_RESTORE_SHX'] = 'YES'\n",
    "states = gpd.read_file('../../temp_shp/cb_2018_us_state_500k.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Data engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing or invalid coordinates\n",
    "AHA_IT2 = AHA_IT.dropna(subset=['lat_as', 'long_as'])\n",
    "\n",
    "# Filter out invalid coordinates\n",
    "valid_coords = (\n",
    "    (AHA_IT2['lat_as'] != 0) & \n",
    "    (AHA_IT2['long_as'] != 0) &\n",
    "    (AHA_IT2['lat_as'] >= -90) & \n",
    "    (AHA_IT2['lat_as'] <= 90) &\n",
    "    (AHA_IT2['long_as'] >= -180) & \n",
    "    (AHA_IT2['long_as'] <= 180)\n",
    ")\n",
    "AHA_IT2 = AHA_IT2[valid_coords]\n",
    "\n",
    "# Create GeoDataFrame\n",
    "hospitals = gpd.GeoDataFrame(\n",
    "    AHA_IT2, \n",
    "    geometry=gpd.points_from_xy(AHA_IT2.long_as, AHA_IT2.lat_as),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter hospitals with valid coordinates and implementation scores\n",
    "valid_hospitals = hospitals.dropna(subset=['long_as', 'lat_as', 'aipred_it'])\n",
    "valid_geo_hospitals = hospitals.dropna(subset=['long_as', 'lat_as'])\n",
    "# Create a GeoDataFrame\n",
    "hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_hospitals.long_as, valid_hospitals.lat_as),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "geo_hospitals_gdf = gpd.GeoDataFrame(\n",
    "    valid_geo_hospitals, \n",
    "    geometry=gpd.points_from_xy(valid_geo_hospitals.long_as, valid_geo_hospitals.lat_as),\n",
    "    crs=\"EPSG:4326\" #geographic coordinate system using latitude and longitude\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a projected CRS for accurate distance calculations\n",
    "hospitals_gdf_projected = hospitals_gdf.to_crs(epsg=3857) # projected coordinate system using flat, 2D plane to represent Earth's surface \n",
    "geo_hospitals_gdf_projected = geo_hospitals_gdf.to_crs(epsg=3857) # projected coordinate system using flat, 2D plane to represent Earth's surface \n",
    "\n",
    "\n",
    "geo_hospitals_gdf_projected['ML_implementation_score'] = geo_hospitals_gdf_projected['aipred_it_union'].map({\n",
    "    1: 3,  # ML gets highest score\n",
    "    2: 2,  # Non-ML gets middle score\n",
    "    3: 1,  # Neither gets lowest score\n",
    "    4: 1,   # Don't know gets lowest score\n",
    "    None: 0,\n",
    "    0: 0 \n",
    "})\n",
    "geo_hospitals_gdf_projected.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 AI/ML barplot (across census division)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of states to regions\n",
    "# Create a mapping of states to census divisions based on provided image\n",
    "state_to_division = {\n",
    "    # Division 1: New England\n",
    "    'ME': 'New England', 'NH': 'New England', 'VT': 'New England', \n",
    "    'MA': 'New England', 'RI': 'New England', 'CT': 'New England',\n",
    "    \n",
    "    # Division 2: Mid Atlantic\n",
    "    'NY': 'Mid Atlantic', 'NJ': 'Mid Atlantic', 'PA': 'Mid Atlantic',\n",
    "    \n",
    "    # Division 3: South Atlantic\n",
    "    'DE': 'South Atlantic', 'MD': 'South Atlantic', 'DC': 'South Atlantic',\n",
    "    'VA': 'South Atlantic', 'WV': 'South Atlantic', 'NC': 'South Atlantic',\n",
    "    'SC': 'South Atlantic', 'GA': 'South Atlantic', 'FL': 'South Atlantic',\n",
    "    \n",
    "    # Division 4: East North Central\n",
    "    'OH': 'East North Central', 'IN': 'East North Central', 'IL': 'East North Central',\n",
    "    'MI': 'East North Central', 'WI': 'East North Central',\n",
    "    \n",
    "    # Division 5: East South Central\n",
    "    'KY': 'East South Central', 'TN': 'East South Central', \n",
    "    'AL': 'East South Central', 'MS': 'East South Central',\n",
    "    \n",
    "    # Division 6: West North Central\n",
    "    'MN': 'West North Central', 'IA': 'West North Central', 'MO': 'West North Central',\n",
    "    'ND': 'West North Central', 'SD': 'West North Central', 'NE': 'West North Central',\n",
    "    'KS': 'West North Central',\n",
    "    \n",
    "    # Division 7: West South Central\n",
    "    'AR': 'West South Central', 'LA': 'West South Central', \n",
    "    'OK': 'West South Central', 'TX': 'West South Central',\n",
    "    \n",
    "    # Division 8: Mountain\n",
    "    'MT': 'Mountain', 'ID': 'Mountain', 'WY': 'Mountain', 'CO': 'Mountain',\n",
    "    'NM': 'Mountain', 'AZ': 'Mountain', 'UT': 'Mountain', 'NV': 'Mountain',\n",
    "    \n",
    "    # Division 9: Pacific\n",
    "    'WA': 'Pacific', 'OR': 'Pacific', 'CA': 'Pacific', \n",
    "    'AK': 'Pacific', 'HI': 'Pacific',\n",
    "    \n",
    "    # Territories\n",
    "    'PR': 'Territories', 'GU': 'Territories', 'VI': 'Territories', \n",
    "    'AS': 'Territories', 'MP': 'Territories'\n",
    "}\n",
    "\n",
    "# Add census division column to the dataframe\n",
    "hospitals_gdf_projected['division'] = hospitals_gdf_projected['mstate_it'].map(state_to_division)\n",
    "geo_hospitals_gdf_projected['division'] = geo_hospitals_gdf_projected['mstate_it'].map(state_to_division)\n",
    "\n",
    "# Loop through each census division \n",
    "divisions = [\n",
    "    'New England', 'Mid Atlantic', 'South Atlantic', \n",
    "    'East North Central', 'East South Central', 'West North Central',\n",
    "    'West South Central', 'Mountain', 'Pacific'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use  existing ai_types\n",
    "ai_types = [3, 2, 1]  \n",
    "\n",
    "# Map the numeric AI types to labels for better readability\n",
    "ai_type_labels = {\n",
    "    3: 'ML',\n",
    "    2: 'Non-ML Predictive Model',\n",
    "    1: 'Do not know/Neither',\n",
    "    0: 'No Response'  # Assuming 0 or NaN is used for no response\n",
    "}\n",
    "\n",
    "# Create a DataFrame to hold the distribution of AI types by division\n",
    "division_ai_counts = pd.DataFrame(index=divisions, columns=ai_types + [0])  # Include 0 for no response\n",
    "# Fill the DataFrame with counts\n",
    "for division in divisions:\n",
    "    geo_division_data = geo_hospitals_gdf_projected[geo_hospitals_gdf_projected['division'] == division]\n",
    "    \n",
    "    # Count no responses (either 0 or NaN)\n",
    "    no_response_count = len(geo_division_data[geo_division_data['ML_implementation_score'].isna() | \n",
    "                                         (geo_division_data['ML_implementation_score'] == 0)])\n",
    "    division_ai_counts.loc[division, 0] = no_response_count\n",
    "    \n",
    "    # Count other AI types\n",
    "    for ai_type in ai_types:\n",
    "        division_ai_counts.loc[division, ai_type] = len(geo_division_data[geo_division_data['ML_implementation_score'] == ai_type])\n",
    "division_ai_counts\n",
    "\n",
    "# Calculate total hospitals per division\n",
    "division_totals = division_ai_counts.sum(axis=1)\n",
    "\n",
    "# Calculate percentages\n",
    "division_ai_dist = division_ai_counts.div(division_ai_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Calculate grid dimensions\n",
    "n_divisions = len(divisions)\n",
    "n_cols = 3  # Number of columns in the grid\n",
    "n_rows = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Map the numeric AI types to labels for better readability\n",
    "ai_type_labels = {\n",
    "    3: 'AI/ML',\n",
    "    2: 'Non-AI/ML Predictive Model',\n",
    "    1: 'Do not know/Neither',\n",
    "    0: 'No Response'\n",
    "}\n",
    "\n",
    "# Rename columns for better readability\n",
    "division_ai_counts = division_ai_counts.rename(columns=ai_type_labels)\n",
    "\n",
    "# Calculate percentages\n",
    "percentages = division_ai_counts.div(division_ai_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Sort divisions by total\n",
    "total_hospitals = division_ai_counts.sum(axis=1)\n",
    "sorted_divisions = division_ai_counts.loc[total_hospitals.sort_values(ascending=False).index]\n",
    "sorted_percentages = percentages.loc[total_hospitals.sort_values(ascending=False).index]\n",
    "\n",
    "# Define publication-quality colors\n",
    "colors = ['#3366CC', '#66CCEE', '#EEEEFF', '#CCCCCC']\n",
    "\n",
    "# Create figure with a higher DPI for better quality\n",
    "fig, ax = plt.subplots(figsize=(10, 7), dpi=300)\n",
    "\n",
    "# Create stacked bar chart\n",
    "bars = sorted_divisions.plot(kind='bar', stacked=True, color=colors, ax=ax, width=0.7, edgecolor='white', linewidth=0.5)\n",
    "\n",
    "# Add percentage labels on top of each segment\n",
    "for i, division in enumerate(sorted_divisions.index):\n",
    "    cumulative_height = 0\n",
    "    for j, col in enumerate(sorted_divisions.columns):\n",
    "        height = sorted_divisions.loc[division, col]\n",
    "        pct = sorted_percentages.loc[division, col]\n",
    "        \n",
    "        if height > 0:  # Only label non-zero values\n",
    "            # Position at the top of each segment\n",
    "            y_pos = cumulative_height + height\n",
    "            \n",
    "            # For small percentages, only show if they're at least 1%\n",
    "            if pct >= 1:\n",
    "                # Position text at the top of each segment\n",
    "                ax.text(i, cumulative_height + (height * 0.5), f'{pct:.1f}%', \n",
    "                        ha='center', va='center', fontsize=9,\n",
    "                        color='black', fontweight='bold')\n",
    "            \n",
    "        cumulative_height += height\n",
    "\n",
    "ax.set_xlabel('Census Division', fontsize=14, labelpad=10, color='black')\n",
    "ax.set_ylabel('Number of Hospitals', fontsize=14, labelpad=10, color='black')\n",
    "ax.tick_params(axis='both', which='major', labelsize=12, colors='black')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right', color='black')\n",
    "plt.yticks(color='black')\n",
    "\n",
    "# Calculate combined percentage of ML and Non-ML Predictive Model\n",
    "combined_percentage = sorted_percentages['AI/ML'] + sorted_percentages['Non-AI/ML Predictive Model']\n",
    "\n",
    "# Add combined percentage labels above the total count\n",
    "for i, division in enumerate(sorted_divisions.index):\n",
    "    total = total_hospitals[division]\n",
    "    combined_pct = combined_percentage[division]\n",
    "    # Position the combined percentage above the total count\n",
    "    plt.text(i, total + 20, f'Model: {combined_pct:.1f}%', \n",
    "             ha='center', fontsize=8,  # Smaller font size\n",
    "             fontweight='normal',      # Normal weight instead of bold\n",
    "             color='#666666')          # Gray color\n",
    "\n",
    "\n",
    "plt.tight_lat()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Hospital clustering - nearest neighbor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nearest neighbor analysis (without relying on pointpats)\n",
    "print(\"Performing Custom Nearest Neighbor Analysis...\")\n",
    "\n",
    "# Extract coordinates\n",
    "coords = np.vstack((geo_hospitals_gdf_projected.geometry.x, geo_hospitals_gdf_projected.geometry.y)).T\n",
    "\n",
    "# Calculate distances between all pairs of points\n",
    "kdtree = spatial.KDTree(coords)\n",
    "distances, indices = kdtree.query(coords, k=2)  # k=2 to get the nearest neighbor (first one is self)\n",
    "mean_observed_nn_distance = np.mean(distances[:, 1])\n",
    "\n",
    "# Calculate the area - use the bounding box as an approximation\n",
    "x_min, y_min, x_max, y_max = geo_hospitals_gdf_projected.total_bounds\n",
    "area = (x_max - x_min) * (y_max - y_min)\n",
    "\n",
    "# Calculate point density\n",
    "n = len(coords)\n",
    "density = n / area\n",
    "\n",
    "# Expected mean distance for random distribution\n",
    "mean_expected_nn_distance = 0.5 / np.sqrt(density)\n",
    "\n",
    "# Calculate nearest neighbor ratio\n",
    "nn_ratio = mean_observed_nn_distance / mean_expected_nn_distance\n",
    "\n",
    "# Standard error\n",
    "se = 0.26136 / np.sqrt(n * density)\n",
    "\n",
    "# Z-score\n",
    "z_score = (mean_observed_nn_distance - mean_expected_nn_distance) / se\n",
    "\n",
    "# Calculate approximate p-value\n",
    "from scipy.stats import norm\n",
    "p_value = 2 * (1 - norm.cdf(abs(z_score)))  # two-tailed test\n",
    "\n",
    "print(f\"Nearest Neighbor Ratio: {nn_ratio:.3f}\")\n",
    "print(f\"z-score: {z_score:.3f}\")\n",
    "print(f\"p-value: {p_value:.3f}\")\n",
    "\n",
    "if z_score < -1.96:\n",
    "    print(\"Hospitals show significant clustering (p < 0.05)\")\n",
    "elif z_score > 1.96:\n",
    "    print(\"Hospitals show significant dispersion (p < 0.05)\")\n",
    "else:\n",
    "    print(\"Hospitals show random spatial pattern\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Moran's I spatial autocorrelation - AI and model implementation measures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 moran's I function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_morans_i(values, coords, k=6):\n",
    "    \"\"\"Calculate Moran's I\"\"\"\n",
    "    import numpy as np\n",
    "    from libpysal.weights import KNN\n",
    "    from esda.moran import Moran\n",
    "    \n",
    "    # Convert inputs to numpy arrays\n",
    "    values = np.asarray(values).flatten()\n",
    "    coords = np.asarray(coords)\n",
    "    \n",
    "    # Handle missing values\n",
    "    valid_idx = ~np.isnan(values)\n",
    "    if not np.all(valid_idx):\n",
    "        values = values[valid_idx]\n",
    "        coords = coords[valid_idx]\n",
    "    \n",
    "    n = len(values)\n",
    "    if n <= 1 or np.var(values) == 0:\n",
    "        return {'moran_i': np.nan, 'expected_i': np.nan, 'z_score': np.nan, 'p_value': np.nan, 'n': n}\n",
    "    \n",
    "    try:\n",
    "        # Create k-nearest neighbor weights using PySAL\n",
    "        w = KNN.from_array(coords, k=k)\n",
    "        \n",
    "        # Calculate Moran's I using PySAL\n",
    "        moran = Moran(values, w)\n",
    "        \n",
    "        return {\n",
    "            'moran_i': moran.I,\n",
    "            'expected_i': moran.EI,\n",
    "            'z_score': moran.z_norm,\n",
    "            'p_value': moran.p_norm,\n",
    "            'n': n\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"PySAL error: {e}\")\n",
    "        return {'moran_i': np.nan, 'expected_i': np.nan, 'z_score': np.nan, 'p_value': np.nan, 'n': n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 US moran's I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Moran's I Spatial Autocorrelation Analysis\n",
    "\n",
    "# Extract coordinates and target variable\n",
    "coords = np.vstack((hospitals_gdf_projected.geometry.x, hospitals_gdf_projected.geometry.y)).T\n",
    "values = hospitals_gdf_projected['ai_base_score_imputed'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates \n",
    "coords = np.vstack((geo_hospitals_gdf_projected.geometry.x, geo_hospitals_gdf_projected.geometry.y)).T\n",
    "\n",
    "# Define the variables to analyze \n",
    "ai_variables = [\n",
    "    'ai_base_score_imputed', \n",
    "    'ai_base_breadth_score_imputed', \n",
    "    'ai_base_dev_score_imputed', \n",
    "    'ai_base_eval_score_2023_imputed',\n",
    "    'ai_base_eval_score_2024_imputed', \n",
    "    'llm_readiness_score'\n",
    "]\n",
    "\n",
    "# Create a list to store results \n",
    "results_list = []\n",
    "\n",
    "# Calculate Moran's I for each variable \n",
    "for var in variables:\n",
    "    try:\n",
    "        # Get values for this variable\n",
    "        values = geo_hospitals_gdf_projected[var].values\n",
    "        \n",
    "        # Calculate Moran's I\n",
    "        result = calculate_morans_i(values, coords, k=6)\n",
    "        \n",
    "        # Determine pattern \n",
    "        if np.isnan(result['p_value']):\n",
    "            pattern = \"Invalid data\"\n",
    "        elif result['p_value'] < 0.05:\n",
    "            if result['moran_i'] > result['expected_i']:\n",
    "                pattern = \"Significant clustering\"\n",
    "            else:\n",
    "                pattern = \"Significant dispersion\"\n",
    "        else:\n",
    "            pattern = \"Random distribution\"\n",
    "        \n",
    "        # Add results to list \n",
    "        results_list.append({\n",
    "            'Variable': var,\n",
    "            'Moran I': result['moran_i'],\n",
    "            'Expected I': result['expected_i'],\n",
    "            'z-score': result['z_score'],\n",
    "            'p-value': result['p_value'],\n",
    "            'Pattern': pattern,\n",
    "            'n': result['n']\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        results_list.append({\n",
    "            'Variable': var,\n",
    "            'Moran I': np.nan,\n",
    "            'Expected I': np.nan,\n",
    "            'z-score': np.nan,\n",
    "            'p-value': np.nan,\n",
    "            'Pattern': f\"Error: {str(e)}\",\n",
    "            'n': np.nan\n",
    "        })\n",
    "\n",
    "# Create DataFrame \n",
    "moran_results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Format the numeric columns \n",
    "moran_results_df['Moran I'] = moran_results_df['Moran I'].round(4)\n",
    "moran_results_df['Expected I'] = moran_results_df['Expected I'].round(4)\n",
    "moran_results_df['z-score'] = moran_results_df['z-score'].round(4)\n",
    "moran_results_df['p-value'] = moran_results_df['p-value'].round(10)\n",
    "\n",
    "# Display the DataFrame \n",
    "print(\"\\nMoran's I Analysis Results (k=6 neighbors):\")\n",
    "moran_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. DBSCAN - cluster identification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 DBSCAN function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_dbscan_clustering(hospitals_gdf, division, output_col, eps=0.35, min_samples=2, k=6):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering with adaptive feature weights based on local density.\n",
    "    \n",
    "    Parameters:\n",
    "    - hospitals_gdf: GeoDataFrame with hospital data\n",
    "    - division: Census division to analyze\n",
    "    - output_col: Column name for the ML score to use\n",
    "    - eps: DBSCAN epsilon parameter\n",
    "    - min_samples: DBSCAN min_samples parameter\n",
    "    - k: Number of neighbors to consider for density estimation\n",
    "    \"\"\"\n",
    "    # Filter data for this division\n",
    "    division_data = hospitals_gdf[hospitals_gdf['division'] == division].copy()\n",
    "    \n",
    "    # Check if we have the target column\n",
    "    if output_col not in division_data.columns:\n",
    "        print(f\"Error: Column '{output_col}' not found for division '{division}'\")\n",
    "        return pd.DataFrame(), []\n",
    "    \n",
    "    # Handle missing values - drop rows with NaN in the target column or coordinates\n",
    "    division_data = division_data.dropna(subset=[output_col, 'geometry'])\n",
    "    \n",
    "    # Check if we have enough data after dropping NaNs\n",
    "    if len(division_data) < min_samples:\n",
    "        print(f\"Skipping {division}: too few hospitals with valid data ({len(division_data)})\")\n",
    "        return pd.DataFrame(), []\n",
    "    \n",
    "    # Print diagnostic info\n",
    "    print(f\"  Processing {division} with {len(division_data)} hospitals (valid data)\")\n",
    "    \n",
    "    # Calculate local density using k-nearest neighbors\n",
    "    coords = np.column_stack([division_data.geometry.x, division_data.geometry.y])\n",
    "    \n",
    "    # Ensure coordinates don't have NaNs\n",
    "    if np.isnan(coords).any():\n",
    "        print(f\"  Warning: Some coordinates contain NaN values. Removing affected points.\")\n",
    "        valid_idx = ~np.isnan(coords).any(axis=1)\n",
    "        division_data = division_data.iloc[valid_idx].copy()\n",
    "        coords = coords[valid_idx]\n",
    "        \n",
    "        if len(division_data) < min_samples:\n",
    "            print(f\"  Skipping {division}: too few hospitals after removing NaN coordinates\")\n",
    "            return pd.DataFrame(), []\n",
    "    \n",
    "    # Ensure ML scores don't have NaNs\n",
    "    values = division_data[output_col].values\n",
    "    if np.isnan(values).any():\n",
    "        print(f\"  Warning: '{output_col}' contains NaN values. Removing affected points.\")\n",
    "        valid_idx = ~np.isnan(values)\n",
    "        division_data = division_data.iloc[valid_idx].copy()\n",
    "        coords = coords[valid_idx]\n",
    "        values = values[valid_idx]\n",
    "        \n",
    "        if len(division_data) < min_samples:\n",
    "            print(f\"  Skipping {division}: too few hospitals after removing NaN values\")\n",
    "            return pd.DataFrame(), []\n",
    "    \n",
    "    # Adjust k if necessary\n",
    "    actual_k = min(k, len(division_data)-1)\n",
    "    if actual_k < k:\n",
    "        print(f\"  Note: Adjusted k from {k} to {actual_k} due to small sample size\")\n",
    "    \n",
    "    # Fit nearest neighbors\n",
    "    nn = NearestNeighbors(n_neighbors=actual_k+1)  # +1 because point is its own neighbor\n",
    "    nn.fit(coords)\n",
    "    distances, _ = nn.kneighbors(coords)\n",
    "    \n",
    "    # Average distance to k nearest neighbors (excluding self)\n",
    "    avg_distances = distances[:, 1:].mean(axis=1)\n",
    "    \n",
    "    # Convert to density: smaller distances = higher density\n",
    "    density = 1 / (avg_distances + 1e-10)  # Add small constant to avoid division by zero\n",
    "    \n",
    "    # Higher density areas get HIGHER weights\n",
    "    min_density, max_density = np.percentile(density, [5, 95])  # Using percentiles to avoid outliers\n",
    "    normalized_density = (density - min_density) / (max_density - min_density + 1e-10)\n",
    "    normalized_density = np.clip(normalized_density, 0, 1)  # Clip to [0, 1] first\n",
    "    weights = 0.5 + 1.5 * normalized_density  # Scale to [0.5, 2.0] - higher density gets higher weights\n",
    "    \n",
    "    # Store the weights for later analysis\n",
    "    division_data['ml_score_weight'] = weights\n",
    "    \n",
    "    # Create weighted features: higher weight amplifies ML scores in dense areas\n",
    "    X = np.column_stack([\n",
    "        coords,  # x, y coordinates\n",
    "        division_data[output_col].values * weights  # Adaptive weighting - FIXED\n",
    "    ])\n",
    "    \n",
    "    # Final check for NaNs\n",
    "    if np.isnan(X).any():\n",
    "        print(f\"  Error: Feature matrix still contains NaN values after processing\")\n",
    "        print(f\"  NaN counts: {np.isnan(X).sum(axis=0)}\")\n",
    "        return pd.DataFrame(), []\n",
    "    \n",
    "    # Standardize the weighted features\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Run DBSCAN\n",
    "    try:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        cluster_labels = dbscan.fit_predict(X_scaled)\n",
    "        \n",
    "        # Store labels in the dataframe\n",
    "        division_data['cluster'] = cluster_labels\n",
    "        \n",
    "        # Create cluster summary\n",
    "        clusters_summary = []\n",
    "        if len(set(cluster_labels)) > 1:  # If we have actual clusters\n",
    "            for cluster_id in sorted(set(cluster_labels)):\n",
    "                if cluster_id == -1:\n",
    "                    continue  # Skip noise points\n",
    "                    \n",
    "                cluster_hospitals = division_data[division_data['cluster'] == cluster_id]\n",
    "                \n",
    "                # Calculate ML stats\n",
    "                ml_mean = cluster_hospitals[output_col].mean()\n",
    "                ml_std = cluster_hospitals[output_col].std() if len(cluster_hospitals) > 1 else 0\n",
    "                \n",
    "                clusters_summary.append({\n",
    "                    'division': division,\n",
    "                    'cluster_id': f\"{division}-{cluster_id}\",\n",
    "                    'n_hospitals': len(cluster_hospitals),\n",
    "                    'pct_of_division': len(cluster_hospitals) / len(division_data) * 100,\n",
    "                    'ml_mean': ml_mean,\n",
    "                    'ml_std': ml_std,\n",
    "                    'ml_min': cluster_hospitals[output_col].min(),\n",
    "                    'ml_max': cluster_hospitals[output_col].max(),\n",
    "                    'avg_density_weight': cluster_hospitals['ml_score_weight'].mean(),  # Renamed for clarity\n",
    "                    'density_weight_std': cluster_hospitals['ml_score_weight'].std() if len(cluster_hospitals) > 1 else 0\n",
    "                })\n",
    "        \n",
    "        # Count results\n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "        n_noise = list(cluster_labels).count(-1)\n",
    "        \n",
    "        print(f\"  Results: {n_clusters} clusters, {n_noise} noise points ({n_noise/len(cluster_labels)*100:.1f}%)\")\n",
    "        print(f\"  ML score weights - min: {weights.min():.2f}, max: {weights.max():.2f}, avg: {weights.mean():.2f}\")\n",
    "        print(f\"  Weight interpretation: Higher values = denser areas get amplified ML scores\")\n",
    "        \n",
    "        return division_data, clusters_summary\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error during DBSCAN: {str(e)}\")\n",
    "        return pd.DataFrame(), []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.column_stack([hospitals_gdf_projected.geometry.x, hospitals_gdf_projected.geometry.y])\n",
    "ml_scores = hospitals_gdf_projected['ai_base_score_imputed'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional DBSCAN Clustering Analysis\n",
    "# Applies spatial clustering to hospital data by census division\n",
    "\n",
    "# Initialize storage for results\n",
    "all_results = []\n",
    "all_clusters_summary = []\n",
    "modified_data = pd.DataFrame()\n",
    "\n",
    "# Define analysis regions\n",
    "regions = [\n",
    "    'New England', 'Mid Atlantic', 'South Atlantic', \n",
    "    'East North Central', 'East South Central', 'West North Central',\n",
    "    'West South Central', 'Mountain', 'Pacific'\n",
    "]\n",
    "\n",
    "target_metric = 'ai_base_score_imputed'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 DBSCAN ai base score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Clustering Analysis by Census Division\n",
    "# Applies adaptive clustering to each geographic region\n",
    "\n",
    "# Initialize storage\n",
    "all_clusters_summary = []\n",
    "modified_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "target_column = 'ai_base_score_imputed'\n",
    "\n",
    "# Apply clustering to each division\n",
    "for division in divisions:\n",
    "    if division not in hospitals_gdf_projected['division'].unique():\n",
    "        continue\n",
    "    \n",
    "    # Run adaptive DBSCAN\n",
    "    division_result, clusters_summary = adaptive_dbscan_clustering(\n",
    "        hospitals_gdf_projected, \n",
    "        division, \n",
    "        output_col=target_column,\n",
    "        eps=0.35, \n",
    "        min_samples=2,\n",
    "        k=6\n",
    "    )\n",
    "    \n",
    "    if not division_result.empty:\n",
    "        modified_data = pd.concat([modified_data, division_result])\n",
    "        all_clusters_summary.extend(clusters_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 DBSCAN secondary model implmentation measures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collection variables\n",
    "all_results = []\n",
    "all_clusters_summary = []\n",
    "modified_hospital_data = pd.DataFrame()  # Empty DataFrame to collect results\n",
    "\n",
    "# Define census divisions\n",
    "divisions = [\n",
    "    'New England', 'Mid Atlantic', 'South Atlantic', \n",
    "    'East North Central', 'East South Central', 'West North Central',\n",
    "    'West South Central', 'Mountain', 'Pacific'\n",
    "]\n",
    "\n",
    "# Define the target column\n",
    "target_column = 'ai_base_score_imputed'\n",
    "\n",
    "print(f\"DBSCAN Clustering Analysis for {target_column} by Census Division\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Apply clustering to each division\n",
    "for division in divisions:\n",
    "    print(f\"\\nAnalyzing {division} division...\")\n",
    "    \n",
    "    # Check if this division exists in the data\n",
    "    if division not in hospitals_gdf_projected['division'].unique():\n",
    "        print(f\"  Warning: No hospitals found in '{division}' division\")\n",
    "        continue\n",
    "    \n",
    "    # Apply clustering\n",
    "    division_result, clusters_summary = adaptive_dbscan_clustering(\n",
    "        hospitals_gdf_projected, \n",
    "        division, \n",
    "        output_col=target_column,\n",
    "        eps=0.35, \n",
    "        min_samples=2,\n",
    "        k=6\n",
    "    )\n",
    "    \n",
    "    # Only append if we got results\n",
    "    if not division_result.empty:\n",
    "        modified_hospital_data = pd.concat([modified_hospital_data, division_result])\n",
    "        all_clusters_summary.extend(clusters_summary)\n",
    "\n",
    "# Convert cluster summaries to DataFrame for analysis\n",
    "if all_clusters_summary:\n",
    "    # Create DataFrame from list\n",
    "    clusters_df = pd.DataFrame(all_clusters_summary)\n",
    "    \n",
    "    # Summary statistics by division\n",
    "    division_summary = clusters_df.groupby('division').agg(\n",
    "        Number_of_Clusters=('cluster_id', 'count'),\n",
    "        Total_Hospitals=('n_hospitals', 'sum'),\n",
    "        Avg_Cluster_Size=('n_hospitals', 'mean'),\n",
    "        Avg_Score=('ml_mean', 'mean'),\n",
    "        Min_Score=('ml_min', 'min'),\n",
    "        Max_Score=('ml_max', 'max')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Format the numeric columns\n",
    "    division_summary['Avg_Cluster_Size'] = division_summary['Avg_Cluster_Size'].round(1)\n",
    "    division_summary['Avg_Score'] = division_summary['Avg_Score'].round(2)\n",
    "    division_summary['Min_Score'] = division_summary['Min_Score'].round(2)\n",
    "    division_summary['Max_Score'] = division_summary['Max_Score'].round(2)\n",
    "    \n",
    "    # Overall summary as a single row DataFrame\n",
    "    overall_summary = pd.DataFrame([{\n",
    "        'Division': 'TOTAL',\n",
    "        'Number_of_Clusters': len(clusters_df),\n",
    "        'Total_Hospitals': clusters_df['n_hospitals'].sum(),\n",
    "        'Avg_Cluster_Size': round(clusters_df['n_hospitals'].mean(), 1),\n",
    "        'Avg_Score': round(clusters_df['ml_mean'].mean(), 2),\n",
    "        'Min_Score': round(clusters_df['ml_min'].min(), 2),\n",
    "        'Max_Score': round(clusters_df['ml_max'].max(), 2)\n",
    "    }])\n",
    "    \n",
    "    # Display summary tables\n",
    "    print(\"\\nDivision Summary:\")\n",
    "    display(division_summary)\n",
    "    \n",
    "    print(\"\\nOverall Summary:\")\n",
    "    display(overall_summary)\n",
    "else:\n",
    "    print(\"No clusters were identified across any division\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
